"""
Day 1: æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€å®ç°

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†æ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶
2. æ©ç æ³¨æ„åŠ›
3. æ•°å­¦æ¨å¯¼éªŒè¯
4. å¯è§†åŒ–å·¥å…·
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import math
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def plot_attention_weights(attention_weights, tokens=None, title="Attention Weights"):
    """
    å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    
    Args:
        attention_weights: æ³¨æ„åŠ›æƒé‡çŸ©é˜µ [seq_len, seq_len]
        tokens: åºåˆ—ä¸­çš„tokenåˆ—è¡¨
        title: å›¾è¡¨æ ‡é¢˜
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if torch.is_tensor(attention_weights):
        weights = attention_weights.detach().cpu().numpy()
    else:
        weights = attention_weights
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    im = ax.imshow(weights, cmap='Blues', aspect='auto')
    
    # è®¾ç½®æ ‡ç­¾
    if tokens is not None:
        ax.set_xticks(range(len(tokens)))
        ax.set_yticks(range(len(tokens)))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        ax.set_yticklabels(tokens)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i in range(weights.shape[0]):
        for j in range(weights.shape[1]):
            text = ax.text(j, i, f'{weights[i, j]:.3f}',
                         ha="center", va="center", color="black", fontsize=8)
    
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel('Key Position')
    ax.set_ylabel('Query Position')
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=ax, shrink=0.8)
    plt.tight_layout()
    return fig

class BasicAttention(nn.Module):
    """
    åŸºç¡€æ³¨æ„åŠ›æœºåˆ¶å®ç°
    
    å®ç°å…¬å¼: Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
    """
    
    def __init__(self, d_k, temperature=None):
        """
        Args:
            d_k: Keyå‘é‡çš„ç»´åº¦
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä¸ºâˆšd_k
        """
        super().__init__()
        self.d_k = d_k
        self.temperature = temperature if temperature is not None else math.sqrt(d_k)
        
    def forward(self, query, key, value, mask=None, return_attention=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query: [batch_size, seq_len_q, d_k]
            key: [batch_size, seq_len_k, d_k]  
            value: [batch_size, seq_len_v, d_v]
            mask: [batch_size, seq_len_q, seq_len_k] å¯é€‰çš„æ©ç 
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            output: [batch_size, seq_len_q, d_v]
            attention_weights: [batch_size, seq_len_q, seq_len_k] (å¯é€‰)
        """
        batch_size, seq_len_q, d_k = query.shape
        seq_len_k = key.shape[1]
        
        # æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° QK^T
        # [batch_size, seq_len_q, d_k] Ã— [batch_size, d_k, seq_len_k] 
        # -> [batch_size, seq_len_q, seq_len_k]
        scores = torch.matmul(query, key.transpose(-2, -1))
        
        # æ­¥éª¤2: ç¼©æ”¾
        scaled_scores = scores / self.temperature
        
        # æ­¥éª¤3: åº”ç”¨æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
        if mask is not None:
            # å°†æ©ç ä¸º0çš„ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·ï¼Œsoftmaxåä¼šå˜æˆ0
            scaled_scores = scaled_scores.masked_fill(mask == 0, -1e9)
        
        # æ­¥éª¤4: å½’ä¸€åŒ–ï¼ˆsoftmaxï¼‰
        attention_weights = F.softmax(scaled_scores, dim=-1)
        
        # æ­¥éª¤5: åŠ æƒæ±‚å’Œ
        # [batch_size, seq_len_q, seq_len_k] Ã— [batch_size, seq_len_k, d_v]
        # -> [batch_size, seq_len_q, d_v]
        output = torch.matmul(attention_weights, value)
        
        if return_attention:
            return output, attention_weights
        return output

def create_padding_mask(seq, pad_token_id=0):
    """åˆ›å»ºå¡«å……æ©ç """
    return (seq != pad_token_id).unsqueeze(1).unsqueeze(1)

def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.unsqueeze(0).unsqueeze(0)

def demonstrate_attention_math():
    """
    æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦è®¡ç®—è¿‡ç¨‹
    """
    print("=== æ³¨æ„åŠ›æœºåˆ¶æ•°å­¦æ¨å¯¼æ¼”ç¤º ===\n")
    
    # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    batch_size, seq_len, d_k, d_v = 1, 4, 8, 6
    
    # éšæœºåˆå§‹åŒ–Q, K, V
    Q = torch.randn(batch_size, seq_len, d_k)
    K = torch.randn(batch_size, seq_len, d_k) 
    V = torch.randn(batch_size, seq_len, d_v)
    
    print(f"è¾“å…¥ç»´åº¦:")
    print(f"Q: {Q.shape} (Query)")
    print(f"K: {K.shape} (Key)")
    print(f"V: {V.shape} (Value)")
    print()
    
    # æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    print("æ­¥éª¤1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° S = QK^T")
    scores = torch.matmul(Q, K.transpose(-2, -1))
    print(f"åˆ†æ•°çŸ©é˜µ S çš„å½¢çŠ¶: {scores.shape}")
    print(f"åˆ†æ•°çŸ©é˜µ S:\n{scores[0].detach().numpy()}")
    print()
    
    # æ­¥éª¤2: ç¼©æ”¾
    print("æ­¥éª¤2: ç¼©æ”¾ S' = S / âˆšd_k")
    scaled_scores = scores / math.sqrt(d_k)
    print(f"ç¼©æ”¾å› å­: âˆš{d_k} = {math.sqrt(d_k):.3f}")
    print(f"ç¼©æ”¾ååˆ†æ•°:\n{scaled_scores[0].detach().numpy()}")
    print()
    
    # æ­¥éª¤3: Softmaxå½’ä¸€åŒ–
    print("æ­¥éª¤3: Softmaxå½’ä¸€åŒ–")
    attention_weights = F.softmax(scaled_scores, dim=-1)
    print(f"æ³¨æ„åŠ›æƒé‡çŸ©é˜µ:\n{attention_weights[0].detach().numpy()}")
    
    # éªŒè¯æ¯è¡Œå’Œä¸º1
    row_sums = attention_weights.sum(dim=-1)
    print(f"æ¯è¡Œå’Œï¼ˆåº”è¯¥éƒ½æ¥è¿‘1.0ï¼‰: {row_sums[0].detach().numpy()}")
    print()
    
    # æ­¥éª¤4: åŠ æƒæ±‚å’Œ
    print("æ­¥éª¤4: åŠ æƒæ±‚å’Œ Output = Attention_weights Ã— V")
    output = torch.matmul(attention_weights, V)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºçŸ©é˜µ:\n{output[0].detach().numpy()}")
    print()
    
    # éªŒè¯æ‰‹åŠ¨è®¡ç®—
    print("=== æ‰‹åŠ¨éªŒè¯ç¬¬ä¸€ä¸ªä½ç½®çš„è®¡ç®— ===")
    manual_output_0 = torch.zeros(d_v)
    for i in range(seq_len):
        weight = attention_weights[0, 0, i]
        value_vec = V[0, i, :]
        manual_output_0 += weight * value_vec
        print(f"ä½ç½®{i}: æƒé‡={weight:.4f}, è´¡çŒ®={weight * value_vec}")
    
    print(f"\næ‰‹åŠ¨è®¡ç®—ç»“æœ: {manual_output_0.detach().numpy()}")
    print(f"è‡ªåŠ¨è®¡ç®—ç»“æœ: {output[0, 0, :].detach().numpy()}")
    print(f"å·®å¼‚: {torch.abs(manual_output_0 - output[0, 0, :]).max().item():.8f}")
    
    return Q, K, V, attention_weights, output

def analyze_attention_properties():
    """
    åˆ†ææ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦æ€§è´¨
    """
    print("\n=== æ³¨æ„åŠ›æœºåˆ¶æ•°å­¦æ€§è´¨åˆ†æ ===\n")
    
    torch.manual_seed(42)
    
    # 1. ç¼©æ”¾çš„é‡è¦æ€§
    print("1. ç¼©æ”¾æ“ä½œçš„é‡è¦æ€§")
    d_k_values = [4, 16, 64, 256]
    
    for d_k in d_k_values:
        Q = torch.randn(1, 4, d_k)
        K = torch.randn(1, 4, d_k)
        
        # æœªç¼©æ”¾çš„åˆ†æ•°
        scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))
        
        # ç¼©æ”¾çš„åˆ†æ•°
        scores_scaled = scores_unscaled / math.sqrt(d_k)
        
        # è®¡ç®—æ–¹å·®
        var_unscaled = scores_unscaled.var().item()
        var_scaled = scores_scaled.var().item()
        
        print(f"d_k={d_k:3d}: æœªç¼©æ”¾æ–¹å·®={var_unscaled:6.2f}, ç¼©æ”¾åæ–¹å·®={var_scaled:6.2f}")
    
    print()
    
    # 2. Softmaxé¥±å’Œé—®é¢˜
    print("2. Softmaxé¥±å’Œé—®é¢˜æ¼”ç¤º")
    x = torch.tensor([1.0, 2.0, 3.0])
    
    for scale in [1, 5, 10, 20]:
        scaled_x = x * scale
        softmax_result = F.softmax(scaled_x, dim=0)
        entropy = -(softmax_result * torch.log(softmax_result + 1e-8)).sum()
        
        print(f"ç¼©æ”¾{scale:2d}å€: {softmax_result.numpy()} (ç†µ: {entropy:.3f})")
    
    print()

def visualize_attention_example():
    """
    å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶ç¤ºä¾‹
    """
    print("=== æ³¨æ„åŠ›æƒé‡å¯è§†åŒ– ===\n")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼šå¥å­ä¸­çš„è¯æ±‡æ³¨æ„åŠ›
    tokens = ["The", "cat", "sat", "on", "the", "mat"]
    seq_len = len(tokens)
    d_model = 8
    
    # åˆ›å»ºè¯åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    torch.manual_seed(42)
    embeddings = torch.randn(seq_len, d_model)
    
    # è‡ªæ³¨æ„åŠ›ï¼šQ = K = V = embeddings
    attention = BasicAttention(d_k=d_model)
    output, weights = attention(
        embeddings.unsqueeze(0), 
        embeddings.unsqueeze(0), 
        embeddings.unsqueeze(0), 
        return_attention=True
    )
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    attention_matrix = weights[0].detach().numpy()
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = plot_attention_weights(attention_matrix, tokens, "Self-Attention Example")
    
    # ä¿å­˜å›¾ç‰‡
    os.makedirs("outputs", exist_ok=True)
    fig.savefig("outputs/attention_weights_example.png", 
                dpi=300, bbox_inches='tight')
    plt.close(fig)
    
    print("æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å·²ä¿å­˜åˆ°: outputs/attention_weights_example.png")
    
    # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    print("\næ³¨æ„åŠ›æ¨¡å¼åˆ†æ:")
    for i, token in enumerate(tokens):
        top_attention = np.argsort(attention_matrix[i])[::-1][:3]
        print(f"'{token}' æœ€å…³æ³¨çš„è¯æ±‡:")
        for j in top_attention:
            print(f"  -> '{tokens[j]}' (æƒé‡: {attention_matrix[i, j]:.3f})")
        print()
    
    return attention_matrix, tokens

def test_masked_attention():
    """
    æµ‹è¯•æ©ç æ³¨æ„åŠ›
    """
    print("=== æ©ç æ³¨æ„åŠ›æµ‹è¯• ===\n")
    
    torch.manual_seed(42)
    
    # åˆ›å»ºåºåˆ—ï¼ˆåŒ…å«å¡«å……ï¼‰
    seq_len = 5
    d_model = 4
    
    # æ¨¡æ‹Ÿä¸€ä¸ªæ‰¹æ¬¡ï¼Œå…¶ä¸­ç¬¬äºŒä¸ªåºåˆ—è¾ƒçŸ­
    sequences = torch.tensor([
        [1, 2, 3, 4, 5],  # å®Œæ•´åºåˆ—
        [1, 2, 3, 0, 0],  # å¡«å……åºåˆ—ï¼ˆ0è¡¨ç¤ºå¡«å……ï¼‰
    ])
    
    # åˆ›å»ºåµŒå…¥
    embeddings = torch.randn(2, seq_len, d_model)
    
    # åˆ›å»ºå¡«å……æ©ç 
    padding_mask = create_padding_mask(sequences, pad_token_id=0)
    print(f"å¡«å……æ©ç å½¢çŠ¶: {padding_mask.shape}")
    print(f"å¡«å……æ©ç :\n{padding_mask[0, 0].numpy()}")
    print(f"å¡«å……æ©ç :\n{padding_mask[1, 0].numpy()}")
    print()
    
    # æµ‹è¯•æ— æ©ç æ³¨æ„åŠ›
    attention = BasicAttention(d_k=d_model)
    output_no_mask, weights_no_mask = attention(
        embeddings, embeddings, embeddings, return_attention=True
    )
    
    # æµ‹è¯•æœ‰æ©ç æ³¨æ„åŠ›
    output_with_mask, weights_with_mask = attention(
        embeddings, embeddings, embeddings, mask=padding_mask, return_attention=True
    )
    
    print("æ— æ©ç æ³¨æ„åŠ›æƒé‡ï¼ˆç¬¬äºŒä¸ªåºåˆ—ï¼‰:")
    print(weights_no_mask[1, 0].detach().numpy())
    print("\næœ‰æ©ç æ³¨æ„åŠ›æƒé‡ï¼ˆç¬¬äºŒä¸ªåºåˆ—ï¼‰:")
    print(weights_with_mask[1, 0].detach().numpy())
    print()
    
    # éªŒè¯æ©ç æ•ˆæœ
    print("æ©ç æ•ˆæœéªŒè¯:")
    print(f"å¡«å……ä½ç½®æƒé‡å’Œï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰: {weights_with_mask[1, 0, 3:].sum():.6f}")
    print(f"éå¡«å……ä½ç½®æƒé‡å’Œï¼ˆåº”è¯¥æ¥è¿‘1ï¼‰: {weights_with_mask[1, 0, :3].sum():.6f}")

def test_causal_attention():
    """
    æµ‹è¯•å› æœæ³¨æ„åŠ›ï¼ˆç”¨äºè¯­è¨€æ¨¡å‹ï¼‰
    """
    print("\n=== å› æœæ³¨æ„åŠ›æµ‹è¯• ===\n")
    
    torch.manual_seed(42)
    
    seq_len = 4
    d_model = 6
    
    # åˆ›å»ºè¾“å…¥åºåˆ—
    embeddings = torch.randn(1, seq_len, d_model)
    
    # åˆ›å»ºå› æœæ©ç 
    causal_mask = create_causal_mask(seq_len)
    print(f"å› æœæ©ç :\n{causal_mask[0, 0].numpy()}")
    print()
    
    # æµ‹è¯•å› æœæ³¨æ„åŠ›
    attention = BasicAttention(d_k=d_model)
    output, weights = attention(
        embeddings, embeddings, embeddings, 
        mask=causal_mask, return_attention=True
    )
    
    print("å› æœæ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print(f"æƒé‡å¼ é‡å½¢çŠ¶: {weights.shape}")
    attention_matrix = weights[0, 0].detach().numpy()  # ç§»é™¤batchå’Œheadç»´åº¦ï¼Œå¾—åˆ° [4, 4]
    print(attention_matrix)
    print()
    
    # éªŒè¯å› æœæ€§è´¨
    print("å› æœæ€§è´¨éªŒè¯:")
    for i in range(seq_len):
        if i + 1 < seq_len:
            future_weights = attention_matrix[i, i+1:]
            print(f"ä½ç½®{i}å¯¹æœªæ¥ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å’Œ: {future_weights.sum():.8f}")
        else:
            print(f"ä½ç½®{i}å¯¹æœªæ¥ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å’Œ: 0.00000000 (æœ€åä½ç½®)")

def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤ºå’Œæµ‹è¯•
    """
    print("ğŸš€ Day 1: æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€å®ç°ä¸éªŒè¯\n")
    print("=" * 60)
    
    # 1. æ•°å­¦æ¨å¯¼æ¼”ç¤º
    Q, K, V, attention_weights, output = demonstrate_attention_math()
    
    # 2. æ•°å­¦æ€§è´¨åˆ†æ
    analyze_attention_properties()
    
    # 3. å¯è§†åŒ–ç¤ºä¾‹
    attention_matrix, tokens = visualize_attention_example()
    
    # 4. æ©ç æ³¨æ„åŠ›æµ‹è¯•
    test_masked_attention()
    
    # 5. å› æœæ³¨æ„åŠ›æµ‹è¯•
    test_causal_attention()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- outputs/attention_weights_example.png")
    print("\nğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("- ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹æ®Šæ€§è´¨")
    print("- å­¦ä¹ ä½ç½®ç¼–ç çš„å¿…è¦æ€§")
    print("- æ¢ç´¢å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶")

if __name__ == "__main__":
    main() 