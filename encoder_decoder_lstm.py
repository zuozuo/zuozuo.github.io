import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
# æ ¹æ®CSDNåšå®¢ https://blog.csdn.net/weixin_46474921/article/details/123783987 çš„è§£å†³æ–¹æ¡ˆ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOSç³»ç»Ÿæ¨èå­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¸­æ–‡å­—ä½“ä¸‹åæ ‡è½´è´Ÿæ•°çš„è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

class Vocabulary:
    """è¯æ±‡è¡¨ç±»ï¼Œç”¨äºæ–‡æœ¬å’Œæ•°å­—ä¹‹é—´çš„è½¬æ¢"""
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.vocab_size = 4
    
    def add_word(self, word):
        if word not in self.word2idx:
            self.word2idx[word] = self.vocab_size
            self.idx2word[self.vocab_size] = word
            self.vocab_size += 1
    
    def add_sentence(self, sentence):
        for word in sentence.split():
            self.add_word(word)
    
    def sentence_to_indices(self, sentence):
        return [self.word2idx.get(word, self.word2idx['<UNK>']) 
                for word in sentence.split()]
    
    def indices_to_sentence(self, indices):
        return ' '.join([self.idx2word[idx] for idx in indices 
                        if idx not in [0, 1, 2]])  # æ’é™¤ç‰¹æ®Šæ ‡è®°

class LSTMEncoder(nn.Module):
    """LSTMç¼–ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        # LSTMå±‚
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, bidirectional=False)
        
    def forward(self, input_seq, input_lengths):
        # input_seq: [batch_size, seq_len]
        batch_size = input_seq.size(0)
        
        # è¯åµŒå…¥
        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]
        
        # æ‰“åŒ…åºåˆ—ä»¥å¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, input_lengths, batch_first=True, enforce_sorted=False)
        
        # LSTMå‰å‘ä¼ æ’­
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # è§£åŒ…åºåˆ—
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        # è¿”å›æœ€åçš„éšçŠ¶æ€ä½œä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤º
        # hidden: [num_layers, batch_size, hidden_size]
        # æˆ‘ä»¬å–æœ€åä¸€å±‚çš„éšçŠ¶æ€
        context_vector = hidden[-1]  # [batch_size, hidden_size]
        
        return context_vector, (hidden, cell)

class LSTMDecoder(nn.Module):
    """LSTMè§£ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMDecoder, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, encoder_hidden, target_seq=None, max_length=20):
        batch_size = encoder_hidden[0].size(1)
        
        if target_seq is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ç›®æ ‡åºåˆ—
            embedded = self.embedding(target_seq)
            output, _ = self.lstm(embedded, encoder_hidden)
            output = self.output_projection(output)
            return output
        else:
            # æ¨ç†æ¨¡å¼ï¼šé€æ­¥ç”Ÿæˆ
            outputs = []
            hidden = encoder_hidden
            input_token = torch.ones(batch_size, 1, dtype=torch.long) * 1  # <SOS>
            
            for _ in range(max_length):
                embedded = self.embedding(input_token)
                output, hidden = self.lstm(embedded, hidden)
                output = self.output_projection(output)
                
                # å–æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
                input_token = output.argmax(dim=-1)
                outputs.append(output)
                
                # å¦‚æœç”Ÿæˆäº†<EOS>ï¼Œæå‰åœæ­¢
                if (input_token == 2).all():
                    break
            
            return torch.cat(outputs, dim=1)

class Seq2SeqModel(nn.Module):
    """åºåˆ—åˆ°åºåˆ—æ¨¡å‹"""
    def __init__(self, encoder, decoder):
        super(Seq2SeqModel, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, input_seq, input_lengths, target_seq=None, max_length=20):
        # ç¼–ç é˜¶æ®µï¼šå°†è¾“å…¥åºåˆ—å‹ç¼©æˆå›ºå®šé•¿åº¦å‘é‡
        context_vector, encoder_hidden = self.encoder(input_seq, input_lengths)
        
        print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")
        print(f"ä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context_vector.shape}")
        print(f"ç¼–ç å™¨éšçŠ¶æ€å½¢çŠ¶: {encoder_hidden[0].shape}")
        
        # è§£ç é˜¶æ®µï¼šåŸºäºä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—
        output = self.decoder(encoder_hidden, target_seq, max_length)
        
        return output, context_vector

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    # ä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ç¤ºä¾‹
    data_pairs = [
        ("æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "I love natural language processing"),
        ("ä»Šå¤© å¤©æ°” å¾ˆ å¥½", "Today weather is good"),
        ("æœºå™¨ å­¦ä¹  å¾ˆ æœ‰è¶£", "Machine learning is interesting"),
        ("æ·±åº¦ å­¦ä¹  å¾ˆ å¼ºå¤§", "Deep learning is powerful"),
        ("äººå·¥ æ™ºèƒ½ æ”¹å˜ ä¸–ç•Œ", "AI changes the world"),
    ]
    return data_pairs

class TranslationDataset(Dataset):
    """ç¿»è¯‘æ•°æ®é›†"""
    def __init__(self, data_pairs, src_vocab, tgt_vocab):
        self.data_pairs = data_pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        src_sentence, tgt_sentence = self.data_pairs[idx]
        
        src_indices = self.src_vocab.sentence_to_indices(src_sentence)
        tgt_indices = [1] + self.tgt_vocab.sentence_to_indices(tgt_sentence) + [2]  # æ·»åŠ <SOS>å’Œ<EOS>
        
        return torch.tensor(src_indices), torch.tensor(tgt_indices)

def collate_fn(batch):
    """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
    src_batch, tgt_batch = zip(*batch)
    
    # è®¡ç®—åºåˆ—é•¿åº¦
    src_lengths = [len(seq) for seq in src_batch]
    tgt_lengths = [len(seq) for seq in tgt_batch]
    
    # å¡«å……åºåˆ—
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    
    return src_batch, torch.tensor(src_lengths), tgt_batch, torch.tensor(tgt_lengths)

def main():
    # 1. å‡†å¤‡æ•°æ®
    print("=" * 50)
    print("å‡†å¤‡æ•°æ®...")
    data_pairs = create_sample_data()
    
    # æ„å»ºè¯æ±‡è¡¨
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    
    for src, tgt in data_pairs:
        src_vocab.add_sentence(src)
        tgt_vocab.add_sentence(tgt)
    
    print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {src_vocab.vocab_size}")
    print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {tgt_vocab.vocab_size}")
    
    # 2. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = TranslationDataset(data_pairs, src_vocab, tgt_vocab)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    
    # 3. å®šä¹‰æ¨¡å‹å‚æ•°
    print("\n" + "=" * 50)
    print("åˆå§‹åŒ–æ¨¡å‹...")
    embed_size = 64
    hidden_size = 128
    num_layers = 1
    
    # 4. åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
    encoder = LSTMEncoder(src_vocab.vocab_size, embed_size, hidden_size, num_layers)
    decoder = LSTMDecoder(tgt_vocab.vocab_size, embed_size, hidden_size, num_layers)
    model = Seq2SeqModel(encoder, decoder)
    
    # 5. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 6. è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    for epoch in range(50):
        total_loss = 0
        for batch_idx, (src_batch, src_lengths, tgt_batch, tgt_lengths) in enumerate(dataloader):
            optimizer.zero_grad()
            
            # å‡†å¤‡ç›®æ ‡åºåˆ—ï¼ˆç”¨äºteacher forcingï¼‰
            decoder_input = tgt_batch[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtoken
            decoder_target = tgt_batch[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªtoken(<SOS>)
            
            # å‰å‘ä¼ æ’­
            output, context_vector = model(src_batch, src_lengths, decoder_input)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output.reshape(-1, output.size(-1)), 
                           decoder_target.reshape(-1))
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/50], Loss: {total_loss/len(dataloader):.4f}')
    
    # 7. æµ‹è¯•æ¨¡å‹
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ¨¡å‹...")
    model.eval()
    
    test_sentences = ["æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "ä»Šå¤© å¤©æ°” å¾ˆ å¥½"]
    
    with torch.no_grad():
        for test_sentence in test_sentences:
            print(f"\nè¾“å…¥: {test_sentence}")
            
            # å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•
            src_indices = src_vocab.sentence_to_indices(test_sentence)
            src_tensor = torch.tensor(src_indices).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            src_length = torch.tensor([len(src_indices)])
            
            # ç”Ÿæˆç¿»è¯‘
            output, context_vector = model(src_tensor, src_length, max_length=10)
            
            # å°†è¾“å‡ºè½¬æ¢ä¸ºå•è¯
            predicted_indices = output.argmax(dim=-1).squeeze(0).tolist()
            predicted_sentence = tgt_vocab.indices_to_sentence(predicted_indices)
            
            print(f"è¾“å‡º: {predicted_sentence}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦: {context_vector.shape}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡å€¼: {context_vector.squeeze().numpy()[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼

    # 8. Embeddingè´¨é‡åˆ†æä¸å¯è§†åŒ–
    print("\n" + "=" * 50)
    print("Embeddingè´¨é‡åˆ†æä¸å¯è§†åŒ–...")
    
    # ä»æ¨¡å‹ä¸­è·å–æºè¯­è¨€çš„embeddingçŸ©é˜µå’Œè¯æ±‡è¡¨
    src_embedding_matrix = model.encoder.embedding.weight.data.cpu()
    
    # å…ˆè¿›è¡Œè´¨é‡åˆ†æ
    analysis_results = analyze_embedding_quality(src_embedding_matrix, src_vocab, top_k=8)
    
    # å†è¿›è¡Œå¯è§†åŒ–
    print(f"\n{'='*60}")
    print("ğŸ¨ ç”Ÿæˆt-SNEå¯è§†åŒ–å›¾...")
    print(f"{'='*60}")
    visualize_embeddings(src_embedding_matrix, src_vocab, method='tsne', title="Source Language Embedding Visualization (t-SNE)")

def analyze_embedding_quality(embedding_matrix, vocab, top_k=5):
    """åˆ†æembeddingè´¨é‡å’Œèšç±»æ•ˆæœ"""
    print(f"\n{'='*60}")
    print("ğŸ“Š Embeddingè´¨é‡åˆ†æ")
    print(f"{'='*60}")
    
    # è®¡ç®—æ‰€æœ‰è¯æ±‡çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    
    # è¿‡æ»¤æ‰ç‰¹æ®Šæ ‡è®°ï¼Œåªåˆ†æå®é™…è¯æ±‡
    real_words = []
    real_indices = []
    real_embeddings = []
    
    for idx, word in vocab.idx2word.items():
        if idx < embedding_matrix.shape[0] and word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:
            real_words.append(word)
            real_indices.append(idx)
            real_embeddings.append(embedding_matrix[idx].numpy())
    
    if len(real_embeddings) < 2:
        print("âš ï¸  å®é™…è¯æ±‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
        return
    
    real_embeddings = np.array(real_embeddings)
    similarity_matrix = cosine_similarity(real_embeddings)
    
    print(f"\nğŸ” è¯æ±‡ç›¸ä¼¼åº¦åˆ†æ (å…±{len(real_words)}ä¸ªè¯æ±‡)")
    print("-" * 50)
    
    # 1. æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è¯å¯¹
    most_similar_pairs = []
    for i in range(len(real_words)):
        for j in range(i+1, len(real_words)):
            similarity = similarity_matrix[i][j]
            most_similar_pairs.append((real_words[i], real_words[j], similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    most_similar_pairs.sort(key=lambda x: x[2], reverse=True)
    
    print(f"\nğŸ“ˆ æœ€ç›¸ä¼¼çš„{min(top_k, len(most_similar_pairs))}å¯¹è¯æ±‡:")
    for i, (word1, word2, sim) in enumerate(most_similar_pairs[:top_k]):
        print(f"   {i+1}. '{word1}' â†” '{word2}': {sim:.4f}")
    
    # 2. åˆ†æç‰¹å®šä¸»é¢˜è¯æ±‡çš„èšé›†åº¦
    print(f"\nğŸ¯ ä¸»é¢˜è¯æ±‡èšé›†åˆ†æ:")
    print("-" * 30)
    
    # å®šä¹‰ä¸»é¢˜è¯æ±‡ç»„
    theme_groups = {
        "æ—¶é—´å¤©æ°”": ["ä»Šå¤©", "å¤©æ°”", "å¥½"],
        "AIæŠ€æœ¯": ["äººå·¥", "æ™ºèƒ½", "æœºå™¨", "å­¦ä¹ "],
        "NLP": ["è‡ªç„¶", "è¯­è¨€", "å¤„ç†"],
        "æƒ…æ„Ÿ": ["çˆ±", "æœ‰è¶£", "å¼ºå¤§"]
    }
    
    for theme_name, words in theme_groups.items():
        # æ‰¾å‡ºè¯¥ä¸»é¢˜ä¸­å­˜åœ¨çš„è¯æ±‡
        existing_words = [w for w in words if w in real_words]
        if len(existing_words) >= 2:
            # è®¡ç®—ç»„å†…å¹³å‡ç›¸ä¼¼åº¦
            indices = [real_words.index(w) for w in existing_words]
            group_similarities = []
            for i in range(len(indices)):
                for j in range(i+1, len(indices)):
                    group_similarities.append(similarity_matrix[indices[i]][indices[j]])
            
            avg_similarity = np.mean(group_similarities)
            print(f"   {theme_name}: {existing_words} â†’ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    # 3. æ£€æŸ¥å…±ç°è¯æ±‡çš„ç›¸ä¼¼åº¦
    print(f"\nğŸ”— è®­ç»ƒæ•°æ®å…±ç°è¯æ±‡ç›¸ä¼¼åº¦:")
    print("-" * 35)
    
    cooccurrence_pairs = [
        ("æˆ‘", "çˆ±"), ("ä»Šå¤©", "å¤©æ°”"), ("å¤©æ°”", "å¥½"),
        ("æœºå™¨", "å­¦ä¹ "), ("æ·±åº¦", "å­¦ä¹ "), ("äººå·¥", "æ™ºèƒ½")
    ]
    
    for word1, word2 in cooccurrence_pairs:
        if word1 in real_words and word2 in real_words:
            idx1, idx2 = real_words.index(word1), real_words.index(word2)
            similarity = similarity_matrix[idx1][idx2]
            print(f"   '{word1}' â†” '{word2}': {similarity:.4f}")
    
    # 4. ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“Š Embeddingç»Ÿè®¡ç‰¹æ€§:")
    print("-" * 25)
    
    # å‘é‡èŒƒæ•°åˆ†æ
    norms = np.linalg.norm(real_embeddings, axis=1)
    print(f"   å‘é‡èŒƒæ•° - å‡å€¼: {np.mean(norms):.4f}, æ ‡å‡†å·®: {np.std(norms):.4f}")
    
    # æ•´ä½“ç›¸ä¼¼åº¦åˆ†å¸ƒ
    upper_triangle = similarity_matrix[np.triu_indices(len(real_words), k=1)]
    print(f"   ç›¸ä¼¼åº¦åˆ†å¸ƒ - å‡å€¼: {np.mean(upper_triangle):.4f}, æ ‡å‡†å·®: {np.std(upper_triangle):.4f}")
    print(f"   ç›¸ä¼¼åº¦èŒƒå›´: [{np.min(upper_triangle):.4f}, {np.max(upper_triangle):.4f}]")
    
    # 5. å¼‚å¸¸æ£€æµ‹
    print(f"\nâš ï¸  å¼‚å¸¸å‘é‡æ£€æµ‹:")
    print("-" * 20)
    
    mean_norm = np.mean(norms)
    std_norm = np.std(norms)
    outlier_threshold = 2.0  # 2å€æ ‡å‡†å·®
    
    outliers = []
    for i, (word, norm) in enumerate(zip(real_words, norms)):
        if abs(norm - mean_norm) > outlier_threshold * std_norm:
            outliers.append((word, norm))
    
    if outliers:
        print(f"   å‘ç°{len(outliers)}ä¸ªå¼‚å¸¸å‘é‡:")
        for word, norm in outliers:
            print(f"     '{word}': èŒƒæ•° = {norm:.4f}")
    else:
        print("   âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸å‘é‡")

    return {
        'similarity_matrix': similarity_matrix,
        'most_similar_pairs': most_similar_pairs[:top_k],
        'real_words': real_words,
        'statistics': {
            'mean_norm': np.mean(norms),
            'mean_similarity': np.mean(upper_triangle),
            'std_similarity': np.std(upper_triangle)
        }
    }

def visualize_embeddings(embedding_matrix, vocab, method='tsne', title='Embedding Visualization', num_words_to_annotate=20):
    """ä½¿ç”¨t-SNEæˆ–PCAå¯è§†åŒ–embedding"""
    # ç¡®ä¿è¯æ±‡è¡¨ä¸­çš„è¯å°‘äºæˆ–ç­‰äºå®é™…embeddingçŸ©é˜µä¸­çš„è¡Œæ•°
    # é€šå¸¸ï¼Œvocab.vocab_size ä¼šæ˜¯ embedding_matrix.shape[0]
    # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬å–ä¸¤è€…ä¸­è¾ƒå°çš„å€¼ï¼Œå¹¶æ’é™¤ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æœå®ƒä»¬å½±å“å¯è§†åŒ–ï¼‰
    
    # è·å–æ‰€æœ‰è¯ï¼ˆæ’é™¤ç‰¹æ®Šæ ‡è®°å¦‚<PAD>, <SOS>, <EOS>, <UNK>ï¼Œå¦‚æœå®ƒä»¬åœ¨è¯æ±‡è¡¨ç´¢å¼•çš„å¼€å¤´ï¼‰
    # æˆ‘ä»¬å‡è®¾ç‰¹æ®Šæ ‡è®°çš„ç´¢å¼•è¾ƒå°ï¼Œå¦‚æœ visualize_embeddings åªå…³æ³¨éç‰¹æ®Šè¯æ±‡
    
    # è¿‡æ»¤æ‰æƒé‡å…¨ä¸ºé›¶çš„å‘é‡ (é€šå¸¸æ˜¯ padding_idx)
    # åŒæ—¶æ”¶é›†æœ‰æ•ˆçš„è¯å’Œå®ƒä»¬çš„ç´¢å¼•
    valid_indices = []
    valid_words = []
    
    # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥ä¿å­˜æ‰€æœ‰è¯çš„å‘é‡
    all_vectors_list = []
    
    # è·å–æ‰€æœ‰è¯çš„åˆ—è¡¨ï¼ŒæŒ‰ç´¢å¼•é¡ºåº
    # vocab.idx2word æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦æŒ‰ç´¢å¼•æ’åºçš„è¯
    # æˆ‘ä»¬åªå–è¯æ±‡è¡¨ä¸­å®é™…å­˜åœ¨çš„è¯æ±‡ï¼Œç›´åˆ° embedding_matrix.shape[0]
    # é€šå¸¸ vocab.vocab_size åº”è¯¥ç­‰äº embedding_matrix.shape[0]
    
    words_to_process_indices = sorted([idx for idx in vocab.idx2word.keys() if idx < embedding_matrix.shape[0]])

    for idx in words_to_process_indices:
        word = vocab.idx2word[idx]
        vector = embedding_matrix[idx]
        # æ’é™¤<PAD>ç­‰ç‰¹æ®Štokençš„å¯è§†åŒ–, é€šå¸¸ padding_idx ä¸º 0
        if word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] or torch.any(vector != 0):
             # åªæœ‰å½“è¯ä¸æ˜¯ç‰¹æ®Šè¯ï¼Œæˆ–è€…å‘é‡ä¸å…¨ä¸º0æ—¶æ‰æ·»åŠ 
            # å®é™…ä¸Šï¼Œå¯¹äºéç‰¹æ®Šè¯ï¼Œå‘é‡ä¸åº”å…¨ä¸º0ï¼Œé™¤éembed_sizeå¾ˆå°æˆ–ç‰¹æ®Šæƒ…å†µ
            # è€Œå¯¹äº<PAD>ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ’é™¤å®ƒï¼Œé™¤éç‰¹åˆ«æƒ³è§‚å¯Ÿå®ƒ
            if not (word == '<PAD>' and torch.all(vector == 0)):
                 all_vectors_list.append(vector.numpy()) # TSNE/PCAéœ€è¦numpyæ•°ç»„
                 valid_words.append(word)
                 valid_indices.append(idx) # è™½ç„¶æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥å¤‡å°†æ¥ä¹‹éœ€

    if not all_vectors_list:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è¯å‘é‡å¯ä¾›å¯è§†åŒ–ã€‚")
        return

    embeddings_to_visualize = np.array(all_vectors_list)

    if embeddings_to_visualize.shape[0] < 2:
        print(f"æœ‰æ•ˆçš„è¯å‘é‡æ•°é‡ ({embeddings_to_visualize.shape[0]}) ä¸è¶³ä»¥è¿›è¡Œé™ç»´å¯è§†åŒ–ã€‚")
        return

    # é™ç»´åˆ°2D
    # t-SNEå¯¹äºå°‘äº perplexity+1 ä¸ªæ ·æœ¬ä¼šå‡ºé—®é¢˜ï¼Œé€šå¸¸perplexityåœ¨5-50ä¹‹é—´
    # PCAæ²¡æœ‰è¿™ä¸ªé™åˆ¶
    n_samples = embeddings_to_visualize.shape[0]
    
    if method == 'tsne':
        # å¯¹äºéå¸¸å°çš„æ ·æœ¬é‡ï¼ŒTSNEå¯èƒ½å¤±è´¥æˆ–äº§ç”Ÿæ— æ„ä¹‰çš„ç»“æœ
        # è°ƒæ•´ perplexity, n_iter, learning_rate
        perplexity_value = min(30.0, float(n_samples - 1)) # Perplexity must be less than n_samples
        if perplexity_value <= 0: # å¦‚æœåªæœ‰ä¸€ä¸ªç‚¹æˆ–æ²¡æœ‰ç‚¹
             print(f"æ ·æœ¬æ•°é‡ ({n_samples}) è¿‡å°‘ï¼Œæ— æ³•ä½¿ç”¨t-SNEã€‚")
             if n_samples > 1 and embeddings_to_visualize.ndim == 2 and embeddings_to_visualize.shape[1] >=2:
                 print("å°è¯•ä½¿ç”¨PCAæ›¿ä»£...")
                 method = 'pca' # å°è¯•PCA
             else:
                 return # ç¡®å®æ— æ³•å¯è§†åŒ–
        
        if method == 'tsne': # å†æ¬¡æ£€æŸ¥ï¼Œå› ä¸ºå¯èƒ½åœ¨ä¸Šé¢è¢«æ”¹ä¸ºpca
            try:
                reducer = TSNE(n_components=2, random_state=42, perplexity=perplexity_value, 
                               max_iter=300, learning_rate=200) # ä¿®å¤sklearnå‚æ•°ï¼šn_iteræ”¹ä¸ºmax_iter
                embeddings_2d = reducer.fit_transform(embeddings_to_visualize)
            except Exception as e:
                print(f"t-SNEæ‰§è¡Œå¤±è´¥: {e}ã€‚å°è¯•ä½¿ç”¨PCAã€‚")
                if n_samples > 1 and embeddings_to_visualize.ndim == 2 and embeddings_to_visualize.shape[1] >=2: # ç¡®ä¿PCAå¯ä»¥è¿è¡Œ
                    method = 'pca'
                else:
                    return


    if method == 'pca': # å¦‚æœåŸå§‹æ–¹æ³•æ˜¯PCAï¼Œæˆ–è€…t-SNEå¤±è´¥åè½¬ä¸ºPCA
        if n_samples < 2 or embeddings_to_visualize.shape[1] < 2: # PCAè‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬å’Œ2ä¸ªç‰¹å¾
            print("æ ·æœ¬æˆ–ç‰¹å¾æ•°é‡ä¸è¶³ä»¥è¿›è¡ŒPCAå¯è§†åŒ–ã€‚")
            return
        try:
            reducer = PCA(n_components=2)
            embeddings_2d = reducer.fit_transform(embeddings_to_visualize)
        except Exception as e:
            print(f"PCAæ‰§è¡Œå¤±è´¥: {e}")
            return


    if embeddings_2d is None or embeddings_2d.shape[0] == 0:
        print("é™ç»´å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆ2DåµŒå…¥ã€‚")
        return

    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(12, 10)) # å¢å¤§å›¾åƒå°ºå¯¸ä»¥å®¹çº³æ›´å¤šæ ‡ç­¾
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=30) # å¢å¤§ç‚¹çš„å¤§å°
    
    # æ·»åŠ è¯æ±‡æ ‡ç­¾ (åªæ³¨é‡Šä¸€éƒ¨åˆ†è¯ä»¥é¿å…æ‹¥æŒ¤)
    words_to_annotate_actual = min(num_words_to_annotate, len(valid_words))
    
    # ä¸ºäº†æ›´å¥½çš„å¯è¯»æ€§ï¼Œé€‰æ‹©ä¸€äº›è¯è¿›è¡Œæ ‡æ³¨ï¼Œä¾‹å¦‚å‡åŒ€é—´éš”çš„æˆ–è€…éšæœºé€‰æ‹©çš„
    # è¿™é‡Œæˆ‘ä»¬ç®€å•é€‰æ‹©å‰ num_words_to_annotate_actual ä¸ªè¯
    indices_to_annotate = np.random.choice(len(valid_words), size=words_to_annotate_actual, replace=False)


    for i in indices_to_annotate:
        plt.annotate(valid_words[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)
    
    plt.title(title, fontsize=14)
    plt.xlabel('Dimension 1', fontsize=12)
    plt.ylabel('Dimension 2', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout() # è°ƒæ•´å¸ƒå±€ä»¥é˜²æ­¢æ ‡ç­¾æº¢å‡º
    # ä¿å­˜å›¾åƒè€Œä¸æ˜¯æ˜¾ç¤ºï¼Œä»¥ä¾¿åœ¨æ— GUIç¯å¢ƒè¿è¡Œ
    try:
        plt.savefig("embedding_visualization.png")
        print(f"Embeddingå¯è§†åŒ–å›¾åƒå·²ä¿å­˜åˆ° embedding_visualization.png")
    except Exception as e:
        print(f"ä¿å­˜å›¾åƒå¤±è´¥: {e}")
    # plt.show() # åœ¨è„šæœ¬ä¸­é€šå¸¸ä¸ç›´æ¥è°ƒç”¨show()ï¼Œé™¤éæ˜¯äº¤äº’å¼è¿è¡Œ

if __name__ == "__main__":
    main()
