# Transformer解码器理论详解

## 1. 解码器架构概述

Transformer解码器是序列到序列模型的关键组件，负责根据编码器的输出和已生成的序列来预测下一个token。与编码器不同，解码器具有以下特殊设计：

### 1.1 解码器层结构
每个解码器层包含三个子层：
1. **掩码多头自注意力** (Masked Multi-Head Self-Attention)
2. **编码器-解码器注意力** (Encoder-Decoder Attention)
3. **前馈神经网络** (Feed-Forward Network)

每个子层都有残差连接和层归一化。

### 1.2 自回归生成
解码器采用自回归方式生成序列：
- 在时间步t，只能看到位置1到t-1的信息
- 通过掩码机制防止"未来信息泄露"
- 逐步生成完整的输出序列

## 2. 掩码注意力机制

### 2.1 因果掩码的必要性
在训练阶段，我们有完整的目标序列，但必须模拟推理时的条件：
- 生成第i个token时，不能看到第i+1个及之后的token
- 这确保了模型学会真正的序列建模能力

### 2.2 掩码矩阵构造
对于长度为n的序列，掩码矩阵M是一个下三角矩阵：
```
M = [[1, 0, 0, 0],
     [1, 1, 0, 0],
     [1, 1, 1, 0],
     [1, 1, 1, 1]]
```

### 2.3 掩码应用方式
在计算注意力分数后，将掩码为0的位置设为负无穷：
```
scores = scores.masked_fill(mask == 0, -1e9)
```
经过softmax后，这些位置的权重接近0。

## 3. 编码器-解码器注意力

### 3.1 交叉注意力机制
这是解码器独有的注意力机制：
- **Query (Q)**: 来自解码器的前一层输出
- **Key (K)**: 来自编码器的最终输出
- **Value (V)**: 来自编码器的最终输出

### 3.2 信息流向
```
编码器输出 → Key, Value
解码器状态 → Query
交叉注意力 → 融合编码器信息到解码器
```

### 3.3 注意力模式
- 解码器的每个位置都可以关注编码器的所有位置
- 这实现了源序列到目标序列的对齐
- 不需要掩码，因为编码器信息对解码器完全可见

## 4. 解码器层的完整流程

### 4.1 单层解码器的计算步骤
```python
def decoder_layer(x, encoder_output, src_mask, tgt_mask):
    # 1. 掩码自注意力
    attn1 = masked_self_attention(x, x, x, tgt_mask)
    x = layer_norm(x + attn1)
    
    # 2. 编码器-解码器注意力
    attn2 = cross_attention(x, encoder_output, encoder_output, src_mask)
    x = layer_norm(x + attn2)
    
    # 3. 前馈网络
    ffn_out = feed_forward(x)
    x = layer_norm(x + ffn_out)
    
    return x
```

### 4.2 多层解码器堆叠
- 通常使用6层解码器
- 每层的输出作为下一层的输入
- 最后一层的输出用于预测下一个token

## 5. 训练与推理的差异

### 5.1 训练阶段（Teacher Forcing）
- 使用完整的目标序列作为输入
- 通过掩码模拟自回归条件
- 并行计算所有位置的损失
- 训练效率高

### 5.2 推理阶段（自回归生成）
- 逐步生成，每次只预测一个token
- 将预测结果添加到输入序列
- 重复直到生成结束符或达到最大长度
- 推理速度相对较慢

## 6. 关键设计考虑

### 6.1 位置编码
- 解码器同样需要位置信息
- 通常与编码器使用相同的位置编码方案
- 在输入嵌入后立即添加

### 6.2 输出投影
- 解码器最后一层输出需要投影到词汇表大小
- 通常使用线性层：`Linear(d_model, vocab_size)`
- 应用softmax得到概率分布

### 6.3 参数共享
- 输入嵌入和输出投影可以共享权重
- 这减少了参数数量并提高了性能
- 需要对共享权重进行适当的缩放

## 7. 常见问题与解决方案

### 7.1 梯度消失/爆炸
- 使用残差连接缓解梯度消失
- 使用层归一化稳定训练
- 适当的权重初始化

### 7.2 训练不稳定
- 使用学习率预热
- 梯度裁剪防止梯度爆炸
- 适当的dropout率

### 7.3 生成质量问题
- 调整温度参数控制随机性
- 使用beam search改善生成质量
- 考虑长度惩罚机制

## 8. 数学公式总结

### 8.1 掩码自注意力
```
MaskedSelfAttn(Q,K,V) = softmax(mask(QK^T/√d_k))V
```

### 8.2 交叉注意力
```
CrossAttn(Q_dec,K_enc,V_enc) = softmax(Q_dec K_enc^T/√d_k)V_enc
```

### 8.3 解码器层
```
DecoderLayer(x,enc) = LayerNorm(FFN(LayerNorm(CrossAttn(LayerNorm(MaskedSelfAttn(x)+x),enc)+x))+x)
```

这些理论基础将指导我们在实现阶段的具体编码工作。 