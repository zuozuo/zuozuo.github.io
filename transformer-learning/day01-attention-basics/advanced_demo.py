"""
é«˜çº§æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º
éªŒè¯æˆ‘ä»¬ä»theory.mdä¸­å­¦åˆ°çš„æ‰€æœ‰æ¦‚å¿µ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

# æ·»åŠ utilsè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.visualization import (plot_attention_weights, plot_interactive_attention, 
                               plot_attention_flow, plot_attention_statistics)
from implementation import BasicAttention, create_causal_mask, create_padding_mask

def verify_attention_formula():
    """
    éªŒè¯æ³¨æ„åŠ›å…¬å¼çš„æ¯ä¸€æ­¥è®¡ç®—
    å¯¹åº”theory.mdä¸­çš„å…¬å¼æ¨å¯¼
    """
    print("ğŸ” éªŒè¯æ³¨æ„åŠ›å…¬å¼ï¼šAttention(Q,K,V) = softmax(QK^T/âˆšd_k)V")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    # åˆ›å»ºç®€å•çš„ç¤ºä¾‹
    d_k, d_v = 4, 3
    seq_len = 3
    
    Q = torch.tensor([[1.0, 0.0, 0.0, 0.0],
                      [0.0, 1.0, 0.0, 0.0], 
                      [0.0, 0.0, 1.0, 0.0]], dtype=torch.float32)
    
    K = torch.tensor([[1.0, 0.0, 0.0, 0.0],
                      [0.5, 0.5, 0.0, 0.0],
                      [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)
    
    V = torch.tensor([[2.0, 0.0, 1.0],
                      [0.0, 3.0, 0.0],
                      [1.0, 1.0, 2.0]], dtype=torch.float32)
    
    print(f"Query Q:\n{Q}")
    print(f"Key K:\n{K}")
    print(f"Value V:\n{V}")
    print()
    
    # æ­¥éª¤1: è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    scores = torch.matmul(Q, K.T)
    print(f"1. ç›¸ä¼¼åº¦åˆ†æ•° QK^T:\n{scores}")
    print()
    
    # æ­¥éª¤2: ç¼©æ”¾
    scaled_scores = scores / (d_k ** 0.5)
    print(f"2. ç¼©æ”¾ååˆ†æ•° QK^T/âˆšd_k (âˆš{d_k}={d_k**0.5}):\n{scaled_scores}")
    print()
    
    # æ­¥éª¤3: Softmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scaled_scores, dim=-1)
    print(f"3. æ³¨æ„åŠ›æƒé‡ softmax(...):\n{attention_weights}")
    print(f"   éªŒè¯è¡Œå’Œä¸º1: {attention_weights.sum(dim=-1)}")
    print()
    
    # æ­¥éª¤4: åŠ æƒèšåˆ
    output = torch.matmul(attention_weights, V)
    print(f"4. æœ€ç»ˆè¾“å‡º Attention_weights Ã— V:\n{output}")
    print()
    
    # æ‰‹åŠ¨éªŒè¯ç¬¬ä¸€è¡Œ
    print("æ‰‹åŠ¨éªŒè¯ç¬¬ä¸€è¡Œè®¡ç®—:")
    manual_first = (attention_weights[0, 0] * V[0] + 
                   attention_weights[0, 1] * V[1] + 
                   attention_weights[0, 2] * V[2])
    print(f"æ‰‹åŠ¨è®¡ç®—: {manual_first}")
    print(f"è‡ªåŠ¨è®¡ç®—: {output[0]}")
    print(f"å·®å¼‚: {torch.abs(manual_first - output[0]).max()}")
    
    return Q, K, V, attention_weights, output

def demonstrate_scaling_importance():
    """
    æ¼”ç¤ºç¼©æ”¾æ“ä½œçš„é‡è¦æ€§
    éªŒè¯theory.mdä¸­å…³äºâˆšd_kç¼©æ”¾çš„åˆ†æ
    """
    print("\nğŸ¯ ç¼©æ”¾æ“ä½œé‡è¦æ€§æ¼”ç¤º")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    # æµ‹è¯•ä¸åŒçš„d_kå€¼
    d_k_values = [4, 16, 64, 256]
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    for idx, d_k in enumerate(d_k_values):
        # ç”Ÿæˆéšæœºçš„Q, K
        Q = torch.randn(1, 5, d_k)
        K = torch.randn(1, 5, d_k)
        
        # è®¡ç®—æœªç¼©æ”¾å’Œç¼©æ”¾çš„åˆ†æ•°
        scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))[0]
        scores_scaled = scores_unscaled / (d_k ** 0.5)
        
        # è®¡ç®—softmax
        weights_unscaled = F.softmax(scores_unscaled, dim=-1)
        weights_scaled = F.softmax(scores_scaled, dim=-1)
        
        # è®¡ç®—ç†µï¼ˆè¡¡é‡åˆ†å¸ƒçš„é›†ä¸­ç¨‹åº¦ï¼‰
        entropy_unscaled = -(weights_unscaled * torch.log(weights_unscaled + 1e-8)).sum(dim=-1).mean()
        entropy_scaled = -(weights_scaled * torch.log(weights_scaled + 1e-8)).sum(dim=-1).mean()
        
        # å¯è§†åŒ–
        ax = axes[idx]
        x = range(5)
        width = 0.35
        
        ax.bar([i - width/2 for i in x], weights_unscaled[0].detach().numpy(), 
               width, label=f'æœªç¼©æ”¾ (ç†µ:{entropy_unscaled:.3f})', alpha=0.7)
        ax.bar([i + width/2 for i in x], weights_scaled[0].detach().numpy(), 
               width, label=f'ç¼©æ”¾å (ç†µ:{entropy_scaled:.3f})', alpha=0.7)
        
        ax.set_title(f'd_k = {d_k}')
        ax.set_xlabel('Position')
        ax.set_ylabel('Attention Weight')
        ax.legend()
        ax.set_xticks(x)
        
        print(f"d_k={d_k}: æœªç¼©æ”¾ç†µ={entropy_unscaled:.3f}, ç¼©æ”¾åç†µ={entropy_scaled:.3f}")
    
    plt.tight_layout()
    plt.savefig('outputs/scaling_importance.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("ç¼©æ”¾é‡è¦æ€§å›¾å·²ä¿å­˜: outputs/scaling_importance.png")

def demonstrate_masking_effects():
    """
    æ¼”ç¤ºä¸åŒç±»å‹æ©ç çš„æ•ˆæœ
    """
    print("\nğŸ­ æ©ç æ•ˆæœæ¼”ç¤º")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    # åˆ›å»ºç¤ºä¾‹åºåˆ—
    seq_len = 6
    d_model = 8
    embeddings = torch.randn(1, seq_len, d_model)
    
    attention = BasicAttention(d_k=d_model)
    
    # 1. æ— æ©ç 
    _, weights_no_mask = attention(embeddings, embeddings, embeddings, return_attention=True)
    
    # 2. å¡«å……æ©ç ï¼ˆæ¨¡æ‹Ÿæœ€åä¸¤ä¸ªä½ç½®æ˜¯å¡«å……ï¼‰
    padding_mask = torch.ones(1, 1, seq_len, seq_len)
    padding_mask[:, :, :, -2:] = 0  # æœ€åä¸¤ä¸ªä½ç½®è¢«æ©ç 
    _, weights_padding = attention(embeddings, embeddings, embeddings, 
                                 mask=padding_mask, return_attention=True)
    
    # 3. å› æœæ©ç 
    causal_mask = create_causal_mask(seq_len)
    _, weights_causal = attention(embeddings, embeddings, embeddings, 
                                mask=causal_mask, return_attention=True)
    
    # å¯è§†åŒ–å¯¹æ¯”
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    masks_info = [
        (weights_no_mask[0].detach().numpy(), "æ— æ©ç ", "åŸå§‹æ³¨æ„åŠ›æƒé‡"),
        (weights_padding[0].detach().numpy(), "å¡«å……æ©ç ", "å¿½ç•¥å¡«å……ä½ç½®"),
        (weights_causal[0].detach().numpy(), "å› æœæ©ç ", "åªçœ‹å†å²ä¿¡æ¯")
    ]
    
    for i, (weights, title, desc) in enumerate(masks_info):
        # ç¡®ä¿weightsæ˜¯2Dçš„
        if len(weights.shape) == 3:
            weights = weights[0]
        im = axes[i].imshow(weights, cmap='Blues', vmin=0, vmax=1)
        axes[i].set_title(f"{title}\n({desc})")
        axes[i].set_xlabel('Key Position')
        axes[i].set_ylabel('Query Position')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for j in range(weights.shape[0]):
            for k in range(weights.shape[1]):
                axes[i].text(k, j, f'{weights[j, k]:.2f}', 
                           ha='center', va='center', 
                           color='white' if weights[j, k] > 0.5 else 'black',
                           fontsize=8)
    
    plt.tight_layout()
    plt.savefig('outputs/masking_effects.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("æ©ç æ•ˆæœå›¾å·²ä¿å­˜: outputs/masking_effects.png")
    
    # éªŒè¯æ©ç å±æ€§
    print("\næ©ç å±æ€§éªŒè¯:")
    print(f"æ— æ©ç ï¼šæ¯è¡Œæƒé‡å’Œ = {weights_no_mask[0].sum(dim=-1)}")
    print(f"å¡«å……æ©ç ï¼šæ¯è¡Œæƒé‡å’Œ = {weights_padding[0].sum(dim=-1)}")
    print(f"å› æœæ©ç ï¼šæ¯è¡Œæƒé‡å’Œ = {weights_causal[0].sum(dim=-1)}")

def demonstrate_permutation_invariance():
    """
    æ¼”ç¤ºæ’åˆ—ä¸å˜æ€§
    éªŒè¯theory.mdä¸­å…³äºæ’åˆ—ä¸å˜æ€§çš„åˆ†æ
    """
    print("\nğŸ”„ æ’åˆ—ä¸å˜æ€§æ¼”ç¤º")
    print("=" * 60)
    
    torch.manual_seed(42)
    
    # åˆ›å»ºåŸå§‹åºåˆ—
    seq_len = 4
    d_model = 6
    embeddings = torch.randn(1, seq_len, d_model)
    
    # åˆ›å»ºæ’åˆ—
    permutation = torch.tensor([2, 0, 3, 1])  # é‡æ–°æ’åˆ—é¡ºåº
    embeddings_permuted = embeddings[:, permutation, :]
    
    attention = BasicAttention(d_k=d_model)
    
    # è®¡ç®—åŸå§‹æ³¨æ„åŠ›
    output_orig, weights_orig = attention(embeddings, embeddings, embeddings, return_attention=True)
    
    # è®¡ç®—æ’åˆ—åçš„æ³¨æ„åŠ›
    output_perm, weights_perm = attention(embeddings_permuted, embeddings_permuted, embeddings_permuted, return_attention=True)
    
    # å°†æ’åˆ—åçš„è¾“å‡ºæŒ‰åŸé¡ºåºæ’å›
    inverse_permutation = torch.argsort(permutation)
    output_perm_restored = output_perm[:, inverse_permutation, :]
    
    print("åŸå§‹åºåˆ—å½¢çŠ¶:", embeddings.shape)
    print("æ’åˆ—ååºåˆ—å½¢çŠ¶:", embeddings_permuted.shape)
    print("æ’åˆ—ç´¢å¼•:", permutation.tolist())
    print()
    
    print("éªŒè¯æ’åˆ—ä¸å˜æ€§:")
    print(f"åŸå§‹è¾“å‡ºå‡å€¼: {output_orig.mean():.6f}")
    print(f"æ’åˆ—åæ¢å¤è¾“å‡ºå‡å€¼: {output_perm_restored.mean():.6f}")
    print(f"å·®å¼‚: {torch.abs(output_orig - output_perm_restored).max():.8f}")
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # åŸå§‹æ³¨æ„åŠ›æƒé‡
    im1 = axes[0].imshow(weights_orig[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)
    axes[0].set_title('åŸå§‹æ³¨æ„åŠ›æƒé‡')
    axes[0].set_xlabel('Key Position')
    axes[0].set_ylabel('Query Position')
    
    # æ’åˆ—åçš„æ³¨æ„åŠ›æƒé‡
    im2 = axes[1].imshow(weights_perm[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)
    axes[1].set_title('æ’åˆ—åæ³¨æ„åŠ›æƒé‡')
    axes[1].set_xlabel('Key Position')
    axes[1].set_ylabel('Query Position')
    
    # æƒé‡å·®å¼‚
    diff = np.abs(weights_orig[0].detach().numpy() - weights_perm[0][inverse_permutation][:, inverse_permutation].detach().numpy())
    im3 = axes[2].imshow(diff, cmap='Reds')
    axes[2].set_title('æƒé‡å·®å¼‚ï¼ˆåº”è¯¥å¾ˆå°ï¼‰')
    axes[2].set_xlabel('Key Position')
    axes[2].set_ylabel('Query Position')
    
    # æ·»åŠ colorbar
    for i, im in enumerate([im1, im2, im3]):
        plt.colorbar(im, ax=axes[i], shrink=0.8)
    
    plt.tight_layout()
    plt.savefig('outputs/permutation_invariance.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("æ’åˆ—ä¸å˜æ€§å›¾å·²ä¿å­˜: outputs/permutation_invariance.png")

def analyze_attention_patterns():
    """
    åˆ†ææ³¨æ„åŠ›æ¨¡å¼ï¼ŒéªŒè¯ä¸åŒç›¸ä¼¼åº¦çš„å½±å“
    """
    print("\nğŸ“Š æ³¨æ„åŠ›æ¨¡å¼åˆ†æ")
    print("=" * 60)
    
    # åˆ›å»ºå…·æœ‰ä¸åŒç›¸ä¼¼åº¦æ¨¡å¼çš„åºåˆ—
    scenarios = {
        "è‡ªæˆ‘å…³æ³¨": torch.eye(4) * 2,  # æ¯ä¸ªä½ç½®åªå…³æ³¨è‡ªå·±
        "å±€éƒ¨å…³æ³¨": torch.tensor([  # ç›¸é‚»ä½ç½®ç›¸ä¼¼
            [2.0, 1.0, 0.0, 0.0],
            [1.0, 2.0, 1.0, 0.0], 
            [0.0, 1.0, 2.0, 1.0],
            [0.0, 0.0, 1.0, 2.0]
        ]),
        "å…¨å±€å…³æ³¨": torch.ones(4, 4) * 0.5 + torch.eye(4) * 0.5,  # å‡åŒ€åˆ†å¸ƒä½†è‡ªå·±ç¨å¼º
        "å±‚æ¬¡å…³æ³¨": torch.tensor([  # ç¬¬ä¸€ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰ï¼Œå…¶ä»–é€’å‡
            [2.0, 1.5, 1.0, 0.5],
            [0.5, 2.0, 1.0, 0.5],
            [0.5, 0.5, 2.0, 1.0], 
            [0.5, 0.5, 0.5, 2.0]
        ])
    }
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    for idx, (name, similarity_matrix) in enumerate(scenarios.items()):
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        scaled_scores = similarity_matrix / (4 ** 0.5)
        attention_weights = F.softmax(scaled_scores, dim=-1)
        
        # è®¡ç®—æ³¨æ„åŠ›ç†µ
        entropy = -(attention_weights * torch.log(attention_weights + 1e-8)).sum(dim=-1)
        
        # ä¸Šæ’ï¼šæ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
        im1 = axes[0, idx].imshow(attention_weights.numpy(), cmap='Blues', vmin=0, vmax=1)
        axes[0, idx].set_title(f'{name}\næ³¨æ„åŠ›æƒé‡')
        
        # ä¸‹æ’ï¼šæ³¨æ„åŠ›ç†µ
        bars = axes[1, idx].bar(range(4), entropy.numpy())
        axes[1, idx].set_title(f'æ³¨æ„åŠ›ç†µ\n(å¹³å‡: {entropy.mean():.3f})')
        axes[1, idx].set_ylabel('ç†µå€¼')
        axes[1, idx].set_xlabel('Queryä½ç½®')
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for i, bar in enumerate(bars):
            height = bar.get_height()
            axes[1, idx].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                            f'{entropy[i]:.3f}', ha='center', va='bottom', fontsize=8)
        
        print(f"{name}: å¹³å‡ç†µ={entropy.mean():.3f}, æƒé‡æœ€å¤§å€¼={attention_weights.max():.3f}")
    
    plt.tight_layout()
    plt.savefig('outputs/attention_patterns.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("æ³¨æ„åŠ›æ¨¡å¼åˆ†æå›¾å·²ä¿å­˜: outputs/attention_patterns.png")

def create_comprehensive_demo():
    """
    åˆ›å»ºä¸€ä¸ªç»¼åˆæ¼”ç¤ºï¼Œå±•ç¤ºæ‰€æœ‰æ¦‚å¿µ
    """
    print("\nğŸš€ ç»¼åˆæ¼”ç¤ºï¼šæœºå™¨ç¿»è¯‘åœºæ™¯")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè‹±è¯‘ä¸­çš„åœºæ™¯
    english_tokens = ["I", "love", "deep", "learning"]
    chinese_tokens = ["æˆ‘", "å–œæ¬¢", "æ·±åº¦", "å­¦ä¹ "]
    
    torch.manual_seed(42)
    
    # åˆ›å»ºè¯åµŒå…¥ï¼ˆç®€åŒ–ï¼‰
    d_model = 8
    english_embeddings = torch.randn(len(english_tokens), d_model)
    chinese_embeddings = torch.randn(len(chinese_tokens), d_model) 
    
    # äº¤å‰æ³¨æ„åŠ›ï¼šä¸­æ–‡æŸ¥è¯¢è‹±æ–‡
    attention = BasicAttention(d_k=d_model)
    
    # Query: ä¸­æ–‡, Key&Value: è‹±æ–‡
    output, weights = attention(
        chinese_embeddings.unsqueeze(0),  # Query
        english_embeddings.unsqueeze(0),  # Key  
        english_embeddings.unsqueeze(0),  # Value
        return_attention=True
    )
    
    attention_matrix = weights[0].detach().numpy()
    
    # åˆ›å»ºå¤šç§å¯è§†åŒ–
    fig = plt.figure(figsize=(20, 12))
    
    # 1. åŸºç¡€çƒ­åŠ›å›¾
    ax1 = plt.subplot(2, 3, 1)
    im1 = ax1.imshow(attention_matrix, cmap='Blues', vmin=0, vmax=1)
    ax1.set_xticks(range(len(english_tokens)))
    ax1.set_yticks(range(len(chinese_tokens)))
    ax1.set_xticklabels(english_tokens)
    ax1.set_yticklabels(chinese_tokens)
    ax1.set_title('äº¤å‰æ³¨æ„åŠ›ï¼šä¸­æ–‡â†’è‹±æ–‡')
    ax1.set_xlabel('è‹±æ–‡è¯æ±‡ (Key)')
    ax1.set_ylabel('ä¸­æ–‡è¯æ±‡ (Query)')
    plt.colorbar(im1, ax=ax1, shrink=0.8)
    
    # 2. æ³¨æ„åŠ›åˆ†å¸ƒå›¾
    ax2 = plt.subplot(2, 3, 2)
    for i, chinese_word in enumerate(chinese_tokens):
        ax2.plot(attention_matrix[i], 'o-', label=chinese_word, linewidth=2, markersize=8)
    ax2.set_xticks(range(len(english_tokens)))
    ax2.set_xticklabels(english_tokens)
    ax2.set_ylabel('æ³¨æ„åŠ›æƒé‡')
    ax2.set_xlabel('è‹±æ–‡è¯æ±‡')
    ax2.set_title('æ¯ä¸ªä¸­æ–‡è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ³¨æ„åŠ›å¼ºåº¦ç»Ÿè®¡
    ax3 = plt.subplot(2, 3, 3)
    total_attention = attention_matrix.sum(axis=0)
    bars = ax3.bar(english_tokens, total_attention, alpha=0.7, color='skyblue')
    ax3.set_title('è‹±æ–‡è¯æ±‡å—å…³æ³¨ç¨‹åº¦')
    ax3.set_ylabel('æ€»æ³¨æ„åŠ›æƒé‡')
    ax3.tick_params(axis='x', rotation=45)
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{total_attention[i]:.3f}', ha='center', va='bottom')
    
    # 4. æ³¨æ„åŠ›ç†µåˆ†æ
    ax4 = plt.subplot(2, 3, 4)
    entropy = -(attention_matrix * np.log(attention_matrix + 1e-8)).sum(axis=1)
    bars = ax4.bar(chinese_tokens, entropy, alpha=0.7, color='lightcoral')
    ax4.set_title('ä¸­æ–‡è¯æ±‡æ³¨æ„åŠ›ç†µ')
    ax4.set_ylabel('ç†µå€¼ï¼ˆé›†ä¸­ç¨‹åº¦ï¼‰')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{entropy[i]:.3f}', ha='center', va='bottom')
    
    # 5. æƒé‡åˆ†å¸ƒç›´æ–¹å›¾
    ax5 = plt.subplot(2, 3, 5)
    weights_flat = attention_matrix.flatten()
    ax5.hist(weights_flat, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
    ax5.axvline(weights_flat.mean(), color='red', linestyle='--', 
                label=f'å‡å€¼: {weights_flat.mean():.3f}')
    ax5.set_title('æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ')
    ax5.set_xlabel('æƒé‡å€¼')
    ax5.set_ylabel('é¢‘æ¬¡')
    ax5.legend()
    
    # 6. æœ€å¼ºæ³¨æ„åŠ›è¿æ¥
    ax6 = plt.subplot(2, 3, 6)
    # æ‰¾å‡ºæœ€å¼ºçš„æ³¨æ„åŠ›è¿æ¥
    top_connections = []
    for i in range(len(chinese_tokens)):
        for j in range(len(english_tokens)):
            top_connections.append((attention_matrix[i, j], chinese_tokens[i], english_tokens[j]))
    
    top_connections.sort(reverse=True)
    top_5 = top_connections[:5]
    
    y_pos = range(len(top_5))
    weights = [conn[0] for conn in top_5]
    labels = [f"{conn[1]} â†’ {conn[2]}" for conn in top_5]
    
    bars = ax6.barh(y_pos, weights, alpha=0.7, color='gold')
    ax6.set_yticks(y_pos)
    ax6.set_yticklabels(labels)
    ax6.set_xlabel('æ³¨æ„åŠ›æƒé‡')
    ax6.set_title('Top 5 æ³¨æ„åŠ›è¿æ¥')
    
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax6.text(width + 0.01, bar.get_y() + bar.get_height()/2.,
                f'{weights[i]:.3f}', ha='left', va='center')
    
    plt.tight_layout()
    plt.savefig('outputs/comprehensive_demo.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("ç»¼åˆæ¼”ç¤ºå›¾å·²ä¿å­˜: outputs/comprehensive_demo.png")
    
    # è¾“å‡ºåˆ†æç»“æœ
    print("\nåˆ†æç»“æœ:")
    for i, chinese_word in enumerate(chinese_tokens):
        max_attention_idx = np.argmax(attention_matrix[i])
        max_attention_word = english_tokens[max_attention_idx]
        max_attention_value = attention_matrix[i, max_attention_idx]
        print(f"'{chinese_word}' æœ€å…³æ³¨ '{max_attention_word}' (æƒé‡: {max_attention_value:.3f})")

def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º
    """
    print("ğŸ¯ é«˜çº§æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤ºä¸éªŒè¯")
    print("åŸºäº theory.md ä¸­çš„ç†è®ºçŸ¥è¯†")
    print("=" * 60)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs('outputs', exist_ok=True)
    
    # 1. éªŒè¯æ³¨æ„åŠ›å…¬å¼
    verify_attention_formula()
    
    # 2. æ¼”ç¤ºç¼©æ”¾é‡è¦æ€§
    demonstrate_scaling_importance()
    
    # 3. æ¼”ç¤ºæ©ç æ•ˆæœ
    demonstrate_masking_effects()
    
    # 4. æ¼”ç¤ºæ’åˆ—ä¸å˜æ€§
    demonstrate_permutation_invariance()
    
    # 5. åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    analyze_attention_patterns()
    
    # 6. ç»¼åˆæ¼”ç¤º
    create_comprehensive_demo()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“Š ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶:")
    print("- outputs/scaling_importance.png")
    print("- outputs/masking_effects.png") 
    print("- outputs/permutation_invariance.png")
    print("- outputs/attention_patterns.png")
    print("- outputs/comprehensive_demo.png")
    print("\nğŸ‰ ç†è®ºéªŒè¯å®Œæˆï¼æ‰€æœ‰æ¦‚å¿µéƒ½å¾—åˆ°äº†å®é™…éªŒè¯ã€‚")

if __name__ == "__main__":
    main() 