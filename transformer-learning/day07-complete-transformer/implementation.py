"""
Day 7: å®Œæ•´Transformeræ¶æ„å®ç°

æœ¬æ–‡ä»¶å®ç°äº†å®Œæ•´çš„Transformeræ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
1. ç¼–ç å™¨-è§£ç å™¨æ¶æ„
2. è¾“å…¥è¾“å‡ºå¤„ç†
3. æ©ç æœºåˆ¶ç»Ÿä¸€ç®¡ç†
4. è®­ç»ƒå’Œæ¨ç†æ¨¡å¼æ”¯æŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import sys
import os
from typing import Optional, Tuple, Dict, Any
from dataclasses import dataclass

# ä¸ºäº†é¿å…å¾ªç¯å¯¼å…¥ï¼Œæˆ‘ä»¬å°†ç›´æ¥åœ¨è¿™é‡Œå®ç°æ‰€éœ€çš„åŸºç¡€ç»„ä»¶
# è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼ŒåŒ…å«äº†å®Œæ•´Transformeræ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # åˆå¹¶å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        # è¾“å‡ºæŠ•å½±
        output = self.w_o(attention_output)
        
        return output, attention_weights
    
    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                                   mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, V)
        return output, attention_weights


class FeedForwardNetwork(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear2(self.dropout(F.relu(self.linear1(x))))


class LayerNormalization(nn.Module):
    """å±‚å½’ä¸€åŒ–"""
    
    def __init__(self, d_model: int, eps: float = 1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model: int, max_seq_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]


class TransformerEncoderLayer(nn.Module):
    """Transformerç¼–ç å™¨å±‚"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1,
                 layer_norm_eps: float = 1e-6):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)
        self.norm1 = LayerNormalization(d_model, layer_norm_eps)
        self.norm2 = LayerNormalization(d_model, layer_norm_eps)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x


class TransformerEncoder(nn.Module):
    """Transformerç¼–ç å™¨"""
    
    def __init__(self, encoder_layer: TransformerEncoderLayer, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, mask)
        return x


class TransformerDecoderLayer(nn.Module):
    """Transformerè§£ç å™¨å±‚"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1,
                 layer_norm_eps: float = 1e-6):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)
        self.norm1 = LayerNormalization(d_model, layer_norm_eps)
        self.norm2 = LayerNormalization(d_model, layer_norm_eps)
        self.norm3 = LayerNormalization(d_model, layer_norm_eps)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # æ©ç è‡ªæ³¨æ„åŠ›
        attn1_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn1_output))
        
        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        attn2_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(attn2_output))
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ffn_output))
        
        return x


class TransformerDecoder(nn.Module):
    """Transformerè§£ç å™¨"""
    
    def __init__(self, decoder_layer: TransformerDecoderLayer, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])
        
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return x


@dataclass
class TransformerConfig:
    """Transformeræ¨¡å‹é…ç½®"""
    # åŸºç¡€é…ç½®
    d_model: int = 512          # æ¨¡å‹ç»´åº¦
    n_heads: int = 8            # æ³¨æ„åŠ›å¤´æ•°
    n_encoder_layers: int = 6   # ç¼–ç å™¨å±‚æ•°
    n_decoder_layers: int = 6   # è§£ç å™¨å±‚æ•°
    d_ff: int = 2048           # å‰é¦ˆç½‘ç»œç»´åº¦
    dropout: float = 0.1        # Dropoutç‡
    
    # è¯æ±‡è¡¨é…ç½®
    src_vocab_size: int = 10000 # æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
    tgt_vocab_size: int = 10000 # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
    max_seq_len: int = 5000     # æœ€å¤§åºåˆ—é•¿åº¦
    
    # ç‰¹æ®Šç¬¦å·
    pad_idx: int = 0            # å¡«å……ç¬¦ç´¢å¼•
    bos_idx: int = 1            # å¼€å§‹ç¬¦ç´¢å¼•
    eos_idx: int = 2            # ç»“æŸç¬¦ç´¢å¼•
    
    # è®­ç»ƒé…ç½®
    share_embeddings: bool = True    # æ˜¯å¦å…±äº«åµŒå…¥æƒé‡
    tie_weights: bool = True         # æ˜¯å¦ç»‘å®šè¾“å…¥è¾“å‡ºæƒé‡
    layer_norm_eps: float = 1e-6     # å±‚å½’ä¸€åŒ–epsilon
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        assert self.d_model % self.n_heads == 0, "d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"
        assert self.d_ff > self.d_model, "å‰é¦ˆç½‘ç»œç»´åº¦åº”å¤§äºæ¨¡å‹ç»´åº¦"


class TokenEmbedding(nn.Module):
    """è¯åµŒå…¥å±‚"""
    
    def __init__(self, vocab_size: int, d_model: int, pad_idx: int = 0):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)
        self.scale = math.sqrt(d_model)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len]
        Returns:
            [batch_size, seq_len, d_model]
        """
        return self.embedding(x) * self.scale


class TransformerOutputLayer(nn.Module):
    """Transformerè¾“å‡ºå±‚"""
    
    def __init__(self, d_model: int, vocab_size: int, tie_weights: bool = False, 
                 embedding_layer: Optional[nn.Module] = None):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        if tie_weights and embedding_layer is not None:
            # æƒé‡ç»‘å®šï¼šè¾“å‡ºå±‚æƒé‡ = åµŒå…¥å±‚æƒé‡çš„è½¬ç½®
            self.projection = nn.Linear(d_model, vocab_size, bias=False)
            self.projection.weight = embedding_layer.embedding.weight
        else:
            self.projection = nn.Linear(d_model, vocab_size)
            
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            [batch_size, seq_len, vocab_size]
        """
        return self.projection(x)


class MaskGenerator:
    """æ©ç ç”Ÿæˆå™¨ - ç»Ÿä¸€ç®¡ç†å„ç§æ©ç """
    
    @staticmethod
    def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:
        """
        åˆ›å»ºå¡«å……æ©ç 
        Args:
            seq: [batch_size, seq_len]
            pad_idx: å¡«å……ç¬¦ç´¢å¼•
        Returns:
            [batch_size, 1, 1, seq_len] - ç”¨äºæ³¨æ„åŠ›è®¡ç®—
        """
        mask = (seq != pad_idx).unsqueeze(1).unsqueeze(2)
        return mask
    
    @staticmethod
    def create_causal_mask(size: int, device: torch.device) -> torch.Tensor:
        """
        åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        Args:
            size: åºåˆ—é•¿åº¦
            device: è®¾å¤‡
        Returns:
            [1, 1, size, size]
        """
        mask = torch.tril(torch.ones(size, size, device=device))
        return mask.unsqueeze(0).unsqueeze(0)
    
    @staticmethod
    def create_decoder_mask(tgt: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:
        """
        åˆ›å»ºè§£ç å™¨æ©ç ï¼ˆå¡«å……æ©ç  + å› æœæ©ç ï¼‰
        Args:
            tgt: [batch_size, seq_len]
            pad_idx: å¡«å……ç¬¦ç´¢å¼•
        Returns:
            [batch_size, 1, seq_len, seq_len]
        """
        batch_size, seq_len = tgt.shape
        device = tgt.device
        
        # å¡«å……æ©ç 
        padding_mask = MaskGenerator.create_padding_mask(tgt, pad_idx)
        # [batch_size, 1, 1, seq_len]
        
        # å› æœæ©ç 
        causal_mask = MaskGenerator.create_causal_mask(seq_len, device)
        # [1, 1, seq_len, seq_len]
        
        # æ‰©å±•å¡«å……æ©ç ä»¥åŒ¹é…å› æœæ©ç çš„ç»´åº¦
        padding_mask = padding_mask.expand(batch_size, 1, seq_len, seq_len)
        
        # ç»„åˆæ©ç ï¼šä¸¤ä¸ªæ©ç éƒ½ä¸ºTrueæ—¶æ‰ä¸ºTrue
        # è½¬æ¢ä¸ºå¸ƒå°”ç±»å‹è¿›è¡Œä½è¿ç®—
        combined_mask = padding_mask.bool() & causal_mask.bool()
        # [batch_size, 1, seq_len, seq_len]
        
        return combined_mask.float()  # è½¬å›æµ®ç‚¹æ•°ç”¨äºæ³¨æ„åŠ›è®¡ç®—


class Transformer(nn.Module):
    """å®Œæ•´çš„Transformeræ¨¡å‹"""
    
    def __init__(self, config: TransformerConfig):
        super().__init__()
        self.config = config
        
        # è¾“å…¥åµŒå…¥å±‚
        self.src_embedding = TokenEmbedding(
            config.src_vocab_size, config.d_model, config.pad_idx
        )
        
        if config.share_embeddings and config.src_vocab_size == config.tgt_vocab_size:
            # å…±äº«æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€åµŒå…¥
            self.tgt_embedding = self.src_embedding
        else:
            self.tgt_embedding = TokenEmbedding(
                config.tgt_vocab_size, config.d_model, config.pad_idx
            )
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_len)
        
        # ç¼–ç å™¨
        encoder_layer = TransformerEncoderLayer(
            d_model=config.d_model,
            n_heads=config.n_heads,
            d_ff=config.d_ff,
            dropout=config.dropout,
            layer_norm_eps=config.layer_norm_eps
        )
        self.encoder = TransformerEncoder(encoder_layer, config.n_encoder_layers)
        
        # è§£ç å™¨
        decoder_layer = TransformerDecoderLayer(
            d_model=config.d_model,
            n_heads=config.n_heads,
            d_ff=config.d_ff,
            dropout=config.dropout,
            layer_norm_eps=config.layer_norm_eps
        )
        self.decoder = TransformerDecoder(decoder_layer, config.n_decoder_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = TransformerOutputLayer(
            config.d_model, 
            config.tgt_vocab_size,
            config.tie_weights,
            self.tgt_embedding if config.tie_weights else None
        )
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=self.config.d_model ** -0.5)
                if module.padding_idx is not None:
                    module.weight.data[module.padding_idx].zero_()
    
    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ç¼–ç å™¨å‰å‘ä¼ æ’­
        Args:
            src: [batch_size, src_len]
            src_mask: [batch_size, 1, 1, src_len]
        Returns:
            [batch_size, src_len, d_model]
        """
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        src_emb = self.src_embedding(src)
        src_emb = self.pos_encoding(src_emb)
        src_emb = self.dropout(src_emb)
        
        # ç¼–ç å™¨
        encoder_output = self.encoder(src_emb, src_mask)
        
        return encoder_output
    
    def decode(self, tgt: torch.Tensor, encoder_output: torch.Tensor,
               src_mask: Optional[torch.Tensor] = None,
               tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è§£ç å™¨å‰å‘ä¼ æ’­
        Args:
            tgt: [batch_size, tgt_len]
            encoder_output: [batch_size, src_len, d_model]
            src_mask: [batch_size, 1, 1, src_len]
            tgt_mask: [batch_size, 1, tgt_len, tgt_len]
        Returns:
            [batch_size, tgt_len, d_model]
        """
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        tgt_emb = self.tgt_embedding(tgt)
        tgt_emb = self.pos_encoding(tgt_emb)
        tgt_emb = self.dropout(tgt_emb)
        
        # è§£ç å™¨
        decoder_output = self.decoder(tgt_emb, encoder_output, src_mask, tgt_mask)
        
        return decoder_output
    
    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å®Œæ•´å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
        Args:
            src: [batch_size, src_len]
            tgt: [batch_size, tgt_len]
            src_mask: [batch_size, 1, 1, src_len]
            tgt_mask: [batch_size, 1, tgt_len, tgt_len]
        Returns:
            [batch_size, tgt_len, vocab_size]
        """
        # ç¼–ç 
        encoder_output = self.encode(src, src_mask)
        
        # è§£ç 
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_layer(decoder_output)
        
        return logits
    
    def create_masks(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºæ‰€æœ‰å¿…è¦çš„æ©ç 
        Args:
            src: [batch_size, src_len]
            tgt: [batch_size, tgt_len]
        Returns:
            src_mask: [batch_size, 1, 1, src_len]
            tgt_mask: [batch_size, 1, tgt_len, tgt_len]
        """
        src_mask = MaskGenerator.create_padding_mask(src, self.config.pad_idx)
        tgt_mask = MaskGenerator.create_decoder_mask(tgt, self.config.pad_idx)
        
        return src_mask, tgt_mask
    
    def generate(self, src: torch.Tensor, max_length: int = 100, 
                 temperature: float = 1.0, top_k: Optional[int] = None,
                 top_p: Optional[float] = None) -> torch.Tensor:
        """
        è‡ªå›å½’ç”Ÿæˆï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        Args:
            src: [batch_size, src_len]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_k: Top-Ké‡‡æ ·
            top_p: Top-Pé‡‡æ ·
        Returns:
            [batch_size, generated_len]
        """
        self.eval()
        batch_size = src.size(0)
        device = src.device
        
        # ç¼–ç æºåºåˆ—
        src_mask = MaskGenerator.create_padding_mask(src, self.config.pad_idx)
        encoder_output = self.encode(src, src_mask)
        
        # åˆå§‹åŒ–ç›®æ ‡åºåˆ—ï¼ˆä»¥BOSå¼€å§‹ï¼‰
        tgt = torch.full((batch_size, 1), self.config.bos_idx, device=device)
        
        # é€æ­¥ç”Ÿæˆ
        for _ in range(max_length - 1):
            # åˆ›å»ºç›®æ ‡æ©ç 
            tgt_mask = MaskGenerator.create_decoder_mask(tgt, self.config.pad_idx)
            
            # è§£ç 
            decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)
            
            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
            logits = self.output_layer(decoder_output[:, -1:, :])  # [batch_size, 1, vocab_size]
            logits = logits.squeeze(1)  # [batch_size, vocab_size]
            
            # æ¸©åº¦è°ƒèŠ‚
            if temperature != 1.0:
                logits = logits / temperature
            
            # é‡‡æ ·ç­–ç•¥
            if top_k is not None:
                # Top-Ké‡‡æ ·
                top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)
                logits = torch.full_like(logits, float('-inf'))
                logits.scatter_(-1, top_k_indices, top_k_logits)
            
            if top_p is not None:
                # Top-Pé‡‡æ ·
                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                # å°†è¶…è¿‡é˜ˆå€¼çš„logitsè®¾ä¸º-inf
                indices_to_remove = sorted_indices_to_remove.scatter(
                    -1, sorted_indices, sorted_indices_to_remove
                )
                logits[indices_to_remove] = float('-inf')
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªè¯
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]
            
            # æ·»åŠ åˆ°åºåˆ—
            tgt = torch.cat([tgt, next_token], dim=1)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½å·²ç»“æŸ
            if (next_token == self.config.eos_idx).all():
                break
        
        return tgt
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
            'config': self.config,
            'encoder_layers': self.config.n_encoder_layers,
            'decoder_layers': self.config.n_decoder_layers,
            'attention_heads': self.config.n_heads,
            'model_dimension': self.config.d_model,
            'feedforward_dimension': self.config.d_ff,
        }


def create_transformer_model(config_name: str = 'base') -> Transformer:
    """åˆ›å»ºé¢„å®šä¹‰é…ç½®çš„Transformeræ¨¡å‹"""
    
    configs = {
        'tiny': TransformerConfig(
            d_model=128,
            n_heads=4,
            n_encoder_layers=2,
            n_decoder_layers=2,
            d_ff=512,
            src_vocab_size=1000,
            tgt_vocab_size=1000,
            max_seq_len=128
        ),
        'small': TransformerConfig(
            d_model=256,
            n_heads=8,
            n_encoder_layers=4,
            n_decoder_layers=4,
            d_ff=1024,
            src_vocab_size=5000,
            tgt_vocab_size=5000,
            max_seq_len=512
        ),
        'base': TransformerConfig(
            d_model=512,
            n_heads=8,
            n_encoder_layers=6,
            n_decoder_layers=6,
            d_ff=2048,
            src_vocab_size=10000,
            tgt_vocab_size=10000,
            max_seq_len=1024
        ),
        'large': TransformerConfig(
            d_model=1024,
            n_heads=16,
            n_encoder_layers=6,
            n_decoder_layers=6,
            d_ff=4096,
            src_vocab_size=32000,
            tgt_vocab_size=32000,
            max_seq_len=2048
        )
    }
    
    if config_name not in configs:
        raise ValueError(f"æœªçŸ¥é…ç½®: {config_name}. å¯ç”¨é…ç½®: {list(configs.keys())}")
    
    config = configs[config_name]
    return Transformer(config)


# æµ‹è¯•å‡½æ•°
def test_transformer_basic():
    """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    print("=== TransformeråŸºç¡€åŠŸèƒ½æµ‹è¯• ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_transformer_model('tiny')
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # æ¨¡å‹ä¿¡æ¯
    info = model.get_model_info()
    print(f"å‚æ•°æ•°é‡: {info['total_parameters']:,}")
    print(f"æ¨¡å‹å¤§å°: {info['model_size_mb']:.2f} MB")
    
    # æµ‹è¯•æ•°æ®
    batch_size, src_len, tgt_len = 2, 10, 8
    src = torch.randint(1, 100, (batch_size, src_len))
    tgt = torch.randint(1, 100, (batch_size, tgt_len))
    
    print(f"è¾“å…¥å½¢çŠ¶: src={src.shape}, tgt={tgt.shape}")
    
    # åˆ›å»ºæ©ç 
    src_mask, tgt_mask = model.create_masks(src, tgt)
    print(f"æ©ç å½¢çŠ¶: src_mask={src_mask.shape}, tgt_mask={tgt_mask.shape}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        logits = model(src, tgt, src_mask, tgt_mask)
        print(f"è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{logits.min():.3f}, {logits.max():.3f}]")
    
    print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡\n")


def test_transformer_generation():
    """ç”ŸæˆåŠŸèƒ½æµ‹è¯•"""
    print("=== Transformerç”ŸæˆåŠŸèƒ½æµ‹è¯• ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_transformer_model('tiny')
    
    # æµ‹è¯•æ•°æ®
    batch_size, src_len = 2, 10
    src = torch.randint(1, 100, (batch_size, src_len))
    
    print(f"æºåºåˆ—å½¢çŠ¶: {src.shape}")
    
    # ç”Ÿæˆæµ‹è¯•
    model.eval()
    with torch.no_grad():
        # è´ªå©ªç”Ÿæˆ
        generated = model.generate(src, max_length=15, temperature=1.0)
        print(f"ç”Ÿæˆåºåˆ—å½¢çŠ¶: {generated.shape}")
        print(f"ç”Ÿæˆåºåˆ—: {generated[0].tolist()}")
        
        # æ¸©åº¦é‡‡æ ·
        generated_temp = model.generate(src, max_length=15, temperature=0.8)
        print(f"æ¸©åº¦é‡‡æ ·ç»“æœ: {generated_temp[0].tolist()}")
        
        # Top-Ké‡‡æ ·
        generated_topk = model.generate(src, max_length=15, temperature=1.0, top_k=10)
        print(f"Top-Ké‡‡æ ·ç»“æœ: {generated_topk[0].tolist()}")
    
    print("âœ… ç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡\n")


def test_mask_functionality():
    """æ©ç åŠŸèƒ½æµ‹è¯•"""
    print("=== æ©ç åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•å¡«å……æ©ç 
    seq = torch.tensor([[1, 2, 3, 0, 0], [1, 2, 0, 0, 0]])
    padding_mask = MaskGenerator.create_padding_mask(seq, pad_idx=0)
    print(f"å¡«å……æ©ç å½¢çŠ¶: {padding_mask.shape}")
    print(f"å¡«å……æ©ç :\n{padding_mask.squeeze()}")
    
    # æµ‹è¯•å› æœæ©ç 
    causal_mask = MaskGenerator.create_causal_mask(5, torch.device('cpu'))
    print(f"å› æœæ©ç å½¢çŠ¶: {causal_mask.shape}")
    print(f"å› æœæ©ç :\n{causal_mask.squeeze()}")
    
    # æµ‹è¯•è§£ç å™¨æ©ç 
    tgt = torch.tensor([[1, 2, 3, 0, 0], [1, 2, 0, 0, 0]])
    decoder_mask = MaskGenerator.create_decoder_mask(tgt, pad_idx=0)
    print(f"è§£ç å™¨æ©ç å½¢çŠ¶: {decoder_mask.shape}")
    print(f"è§£ç å™¨æ©ç  (ç¬¬ä¸€ä¸ªæ ·æœ¬):\n{decoder_mask[0, 0]}")
    
    print("âœ… æ©ç åŠŸèƒ½æµ‹è¯•é€šè¿‡\n")


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_transformer_basic()
    test_transformer_generation()
    test_mask_functionality()
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®Œæ•´Transformerå®ç°æˆåŠŸï¼") 