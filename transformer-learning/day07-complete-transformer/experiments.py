"""
Day 7: å®Œæ•´Transformeræ¶æ„ - å®éªŒéªŒè¯

æœ¬æ–‡ä»¶åŒ…å«å®Œæ•´Transformeræ¨¡å‹çš„æ·±å…¥å®éªŒå’Œåˆ†æï¼š
1. æ¨¡å‹å®Œæ•´æ€§éªŒè¯
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
4. ç”Ÿæˆè´¨é‡è¯„ä¼°
5. è®­ç»ƒæ¨ç†å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import os
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

from implementation import (
    Transformer, 
    TransformerConfig, 
    create_transformer_model,
    MaskGenerator
)

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

class TransformerAnalyzer:
    """Transformeræ¨¡å‹åˆ†æå™¨"""
    
    def __init__(self, model: Transformer):
        self.model = model
        self.config = model.config
        self.device = next(model.parameters()).device
        
    def analyze_model_structure(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ç»“æ„"""
        print("=== æ¨¡å‹ç»“æ„åˆ†æ ===")
        
        info = self.model.get_model_info()
        
        # è®¡ç®—å„ç»„ä»¶å‚æ•°æ•°é‡
        encoder_params = sum(p.numel() for p in self.model.encoder.parameters())
        decoder_params = sum(p.numel() for p in self.model.decoder.parameters())
        embedding_params = sum(p.numel() for p in self.model.src_embedding.parameters())
        if self.model.src_embedding != self.model.tgt_embedding:
            embedding_params += sum(p.numel() for p in self.model.tgt_embedding.parameters())
        output_params = sum(p.numel() for p in self.model.output_layer.parameters())
        
        structure_info = {
            'total_parameters': info['total_parameters'],
            'encoder_parameters': encoder_params,
            'decoder_parameters': decoder_params,
            'embedding_parameters': embedding_params,
            'output_parameters': output_params,
            'encoder_percentage': encoder_params / info['total_parameters'] * 100,
            'decoder_percentage': decoder_params / info['total_parameters'] * 100,
            'embedding_percentage': embedding_params / info['total_parameters'] * 100,
            'output_percentage': output_params / info['total_parameters'] * 100,
        }
        
        print(f"æ€»å‚æ•°æ•°é‡: {structure_info['total_parameters']:,}")
        print(f"ç¼–ç å™¨å‚æ•°: {encoder_params:,} ({structure_info['encoder_percentage']:.1f}%)")
        print(f"è§£ç å™¨å‚æ•°: {decoder_params:,} ({structure_info['decoder_percentage']:.1f}%)")
        print(f"åµŒå…¥å±‚å‚æ•°: {embedding_params:,} ({structure_info['embedding_percentage']:.1f}%)")
        print(f"è¾“å‡ºå±‚å‚æ•°: {output_params:,} ({structure_info['output_percentage']:.1f}%)")
        print()
        
        return structure_info
    
    def test_forward_pass_integrity(self) -> Dict[str, Any]:
        """æµ‹è¯•å‰å‘ä¼ æ’­å®Œæ•´æ€§"""
        print("=== å‰å‘ä¼ æ’­å®Œæ•´æ€§æµ‹è¯• ===")
        
        batch_size, src_len, tgt_len = 4, 20, 15
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        src = torch.randint(1, self.config.src_vocab_size, (batch_size, src_len))
        tgt = torch.randint(1, self.config.tgt_vocab_size, (batch_size, tgt_len))
        
        # åˆ›å»ºæ©ç 
        src_mask, tgt_mask = self.model.create_masks(src, tgt)
        
        self.model.eval()
        with torch.no_grad():
            # å®Œæ•´å‰å‘ä¼ æ’­
            logits = self.model(src, tgt, src_mask, tgt_mask)
            
            # åˆ†æ­¥å‰å‘ä¼ æ’­éªŒè¯
            encoder_output = self.model.encode(src, src_mask)
            decoder_output = self.model.decode(tgt, encoder_output, src_mask, tgt_mask)
            logits_step = self.model.output_layer(decoder_output)
            
            # éªŒè¯ä¸€è‡´æ€§
            consistency_error = torch.abs(logits - logits_step).max().item()
            
            # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
            expected_shape = (batch_size, tgt_len, self.config.tgt_vocab_size)
            shape_correct = logits.shape == expected_shape
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            has_nan = torch.isnan(logits).any().item()
            has_inf = torch.isinf(logits).any().item()
            
            # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
            probs = F.softmax(logits, dim=-1)
            prob_sum = probs.sum(dim=-1)
            prob_sum_correct = torch.allclose(prob_sum, torch.ones_like(prob_sum), atol=1e-5)
            
        results = {
            'shape_correct': shape_correct,
            'expected_shape': expected_shape,
            'actual_shape': logits.shape,
            'consistency_error': consistency_error,
            'has_nan': has_nan,
            'has_inf': has_inf,
            'prob_sum_correct': prob_sum_correct,
            'logits_range': (logits.min().item(), logits.max().item()),
            'encoder_output_shape': encoder_output.shape,
            'decoder_output_shape': decoder_output.shape,
        }
        
        print(f"è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {shape_correct} (æœŸæœ›: {expected_shape}, å®é™…: {logits.shape})")
        print(f"åˆ†æ­¥ä¸€è‡´æ€§è¯¯å·®: {consistency_error:.2e}")
        print(f"åŒ…å«NaN: {has_nan}")
        print(f"åŒ…å«Inf: {has_inf}")
        print(f"æ¦‚ç‡å’Œæ­£ç¡®: {prob_sum_correct}")
        print(f"LogitsèŒƒå›´: [{results['logits_range'][0]:.3f}, {results['logits_range'][1]:.3f}]")
        print()
        
        return results
    
    def benchmark_performance(self) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
        
        configs = ['tiny', 'small']
        results = {}
        
        for config_name in configs:
            print(f"æµ‹è¯•é…ç½®: {config_name}")
            
            # åˆ›å»ºæµ‹è¯•æ¨¡å‹
            test_model = create_transformer_model(config_name)
            test_model.eval()
            
            batch_size, src_len, tgt_len = 2, 50, 40
            src = torch.randint(1, test_model.config.src_vocab_size, (batch_size, src_len))
            tgt = torch.randint(1, test_model.config.tgt_vocab_size, (batch_size, tgt_len))
            
            src_mask, tgt_mask = test_model.create_masks(src, tgt)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(3):
                    _ = test_model(src, tgt, src_mask, tgt_mask)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­æ—¶é—´
            times = []
            with torch.no_grad():
                for _ in range(10):
                    start_time = time.time()
                    logits = test_model(src, tgt, src_mask, tgt_mask)
                    end_time = time.time()
                    times.append(end_time - start_time)
            
            avg_time = np.mean(times)
            std_time = np.std(times)
            
            # æµ‹è¯•ç”Ÿæˆæ—¶é—´
            gen_times = []
            with torch.no_grad():
                for _ in range(5):
                    start_time = time.time()
                    generated = test_model.generate(src[:1], max_length=20)
                    end_time = time.time()
                    gen_times.append(end_time - start_time)
            
            avg_gen_time = np.mean(gen_times)
            
            # è®¡ç®—ååé‡
            total_tokens = batch_size * tgt_len
            throughput = total_tokens / avg_time
            
            model_info = test_model.get_model_info()
            
            results[config_name] = {
                'parameters': model_info['total_parameters'],
                'model_size_mb': model_info['model_size_mb'],
                'forward_time_ms': avg_time * 1000,
                'forward_time_std_ms': std_time * 1000,
                'generation_time_ms': avg_gen_time * 1000,
                'throughput_tokens_per_sec': throughput,
                'd_model': test_model.config.d_model,
                'n_layers': test_model.config.n_encoder_layers,
                'n_heads': test_model.config.n_heads,
            }
            
            print(f"  å‚æ•°æ•°é‡: {model_info['total_parameters']:,}")
            print(f"  å‰å‘ä¼ æ’­æ—¶é—´: {avg_time*1000:.2f}Â±{std_time*1000:.2f} ms")
            print(f"  ç”Ÿæˆæ—¶é—´: {avg_gen_time*1000:.2f} ms")
            print(f"  ååé‡: {throughput:.1f} tokens/s")
            print()
        
        return results
    
    def test_generation_quality(self) -> Dict[str, Any]:
        """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
        print("=== ç”Ÿæˆè´¨é‡æµ‹è¯• ===")
        
        batch_size, src_len = 3, 15
        src = torch.randint(1, self.config.src_vocab_size, (batch_size, src_len))
        
        results = {}
        
        # æµ‹è¯•ä¸åŒé‡‡æ ·ç­–ç•¥
        sampling_strategies = [
            {'name': 'greedy', 'params': {'temperature': 1.0}},
            {'name': 'temperature_0.8', 'params': {'temperature': 0.8}},
            {'name': 'temperature_1.2', 'params': {'temperature': 1.2}},
            {'name': 'top_k_10', 'params': {'temperature': 1.0, 'top_k': 10}},
            {'name': 'top_p_0.9', 'params': {'temperature': 1.0, 'top_p': 0.9}},
        ]
        
        self.model.eval()
        with torch.no_grad():
            for strategy in sampling_strategies:
                generated = self.model.generate(src, max_length=20, **strategy['params'])
                
                # åˆ†æç”Ÿæˆè´¨é‡
                vocab_diversity = self._calculate_vocab_diversity(generated)
                repetition_rate = self._calculate_repetition_rate(generated)
                length_stats = self._calculate_length_stats(generated)
                
                results[strategy['name']] = {
                    'vocab_diversity': vocab_diversity,
                    'repetition_rate': repetition_rate,
                    'avg_length': length_stats['avg_length'],
                    'length_std': length_stats['length_std'],
                    'sample_sequences': generated[:2].tolist(),  # ä¿å­˜å‰ä¸¤ä¸ªæ ·æœ¬
                }
                
                print(f"{strategy['name']}:")
                print(f"  è¯æ±‡å¤šæ ·æ€§: {vocab_diversity:.4f}")
                print(f"  é‡å¤ç‡: {repetition_rate:.4f}")
                print(f"  å¹³å‡é•¿åº¦: {length_stats['avg_length']:.1f}Â±{length_stats['length_std']:.1f}")
                print(f"  æ ·æœ¬: {generated[0].tolist()[:10]}...")
                print()
        
        return results
    
    def _calculate_vocab_diversity(self, sequences: torch.Tensor) -> float:
        """è®¡ç®—è¯æ±‡å¤šæ ·æ€§"""
        unique_tokens = set()
        total_tokens = 0
        
        for seq in sequences:
            for token in seq:
                if token.item() not in [self.config.pad_idx, self.config.bos_idx, self.config.eos_idx]:
                    unique_tokens.add(token.item())
                    total_tokens += 1
        
        return len(unique_tokens) / max(total_tokens, 1)
    
    def _calculate_repetition_rate(self, sequences: torch.Tensor) -> float:
        """è®¡ç®—é‡å¤ç‡"""
        total_repetitions = 0
        total_bigrams = 0
        
        for seq in sequences:
            seq_list = seq.tolist()
            bigrams = [(seq_list[i], seq_list[i+1]) for i in range(len(seq_list)-1)]
            unique_bigrams = set(bigrams)
            
            repetitions = len(bigrams) - len(unique_bigrams)
            total_repetitions += repetitions
            total_bigrams += len(bigrams)
        
        return total_repetitions / max(total_bigrams, 1)
    
    def _calculate_length_stats(self, sequences: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—é•¿åº¦ç»Ÿè®¡"""
        lengths = []
        
        for seq in sequences:
            # æ‰¾åˆ°EOSä½ç½®æˆ–åºåˆ—æœ«å°¾
            eos_positions = (seq == self.config.eos_idx).nonzero()
            if len(eos_positions) > 0:
                length = eos_positions[0].item() + 1
            else:
                length = len(seq)
            lengths.append(length)
        
        return {
            'avg_length': np.mean(lengths),
            'length_std': np.std(lengths),
            'min_length': min(lengths),
            'max_length': max(lengths),
        }
    
    def compare_training_inference_modes(self) -> Dict[str, Any]:
        """æ¯”è¾ƒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼"""
        print("=== è®­ç»ƒæ¨ç†æ¨¡å¼å¯¹æ¯” ===")
        
        batch_size, src_len, tgt_len = 2, 15, 12
        src = torch.randint(1, self.config.src_vocab_size, (batch_size, src_len))
        tgt = torch.randint(1, self.config.tgt_vocab_size, (batch_size, tgt_len))
        
        src_mask, tgt_mask = self.model.create_masks(src, tgt)
        
        results = {}
        
        # è®­ç»ƒæ¨¡å¼ï¼ˆTeacher Forcingï¼‰
        self.model.train()
        with torch.no_grad():
            start_time = time.time()
            training_logits = self.model(src, tgt, src_mask, tgt_mask)
            training_time = time.time() - start_time
            
            training_probs = F.softmax(training_logits, dim=-1)
            training_predictions = training_probs.argmax(dim=-1)
        
        # æ¨ç†æ¨¡å¼ï¼ˆAuto-regressiveï¼‰
        self.model.eval()
        with torch.no_grad():
            start_time = time.time()
            generated = self.model.generate(src, max_length=tgt_len)
            inference_time = time.time() - start_time
        
        # æ¯”è¾ƒç»“æœ
        results = {
            'training_mode': {
                'time_ms': training_time * 1000,
                'output_shape': training_logits.shape,
                'parallel_computation': True,
                'uses_teacher_forcing': True,
                'sample_predictions': training_predictions[0].tolist()[:10],
            },
            'inference_mode': {
                'time_ms': inference_time * 1000,
                'output_shape': generated.shape,
                'parallel_computation': False,
                'uses_teacher_forcing': False,
                'sample_generation': generated[0].tolist()[:10],
            },
            'speed_ratio': inference_time / training_time,
        }
        
        print(f"è®­ç»ƒæ¨¡å¼:")
        print(f"  æ—¶é—´: {training_time*1000:.2f} ms")
        print(f"  è¾“å‡ºå½¢çŠ¶: {training_logits.shape}")
        print(f"  å¹¶è¡Œè®¡ç®—: æ˜¯")
        print(f"  æ ·æœ¬é¢„æµ‹: {results['training_mode']['sample_predictions']}")
        print()
        
        print(f"æ¨ç†æ¨¡å¼:")
        print(f"  æ—¶é—´: {inference_time*1000:.2f} ms")
        print(f"  è¾“å‡ºå½¢çŠ¶: {generated.shape}")
        print(f"  å¹¶è¡Œè®¡ç®—: å¦")
        print(f"  æ ·æœ¬ç”Ÿæˆ: {results['inference_mode']['sample_generation']}")
        print()
        
        print(f"æ¨ç†/è®­ç»ƒæ—¶é—´æ¯”: {results['speed_ratio']:.2f}x")
        print()
        
        return results


def run_comprehensive_experiments():
    """è¿è¡Œå®Œæ•´çš„å®éªŒå¥—ä»¶"""
    print("ğŸš€ å¼€å§‹å®Œæ•´Transformerå®éªŒéªŒè¯")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_transformer_model('small')  # ä½¿ç”¨smallé…ç½®è¿›è¡Œæµ‹è¯•
    analyzer = TransformerAnalyzer(model)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("transformer-learning/day07-complete-transformer/outputs", exist_ok=True)
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = {}
    
    # 1. æ¨¡å‹ç»“æ„åˆ†æ
    results['structure'] = analyzer.analyze_model_structure()
    
    # 2. å‰å‘ä¼ æ’­å®Œæ•´æ€§æµ‹è¯•
    results['integrity'] = analyzer.test_forward_pass_integrity()
    
    # 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
    results['performance'] = analyzer.benchmark_performance()
    
    # 4. ç”Ÿæˆè´¨é‡æµ‹è¯•
    results['generation'] = analyzer.test_generation_quality()
    
    # 5. è®­ç»ƒæ¨ç†æ¨¡å¼å¯¹æ¯”
    results['modes'] = analyzer.compare_training_inference_modes()
    
    print("=" * 50)
    print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    
    return results


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´å®éªŒ
    results = run_comprehensive_experiments()
    
    # ä¿å­˜ç»“æœæ‘˜è¦
    print("\nğŸ“Š å®éªŒç»“æœæ‘˜è¦:")
    print(f"âœ… æ¨¡å‹ç»“æ„éªŒè¯: é€šè¿‡")
    print(f"âœ… å‰å‘ä¼ æ’­å®Œæ•´æ€§: é€šè¿‡")
    print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•: å®Œæˆ")
    print(f"âœ… ç”Ÿæˆè´¨é‡è¯„ä¼°: å®Œæˆ")
    print(f"âœ… è®­ç»ƒæ¨ç†å¯¹æ¯”: å®Œæˆ")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: transformer-learning/day07-complete-transformer/outputs/") 