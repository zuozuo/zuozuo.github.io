# 多头注意力机制：理论深度解析

## 1. 引言：为什么需要多头注意力？

在Day 02中，我们学习了自注意力机制，它能够让模型关注序列中的不同位置。但是，单一的注意力头存在一些局限性：

### 1.1 单头注意力的局限性
- **表示能力有限**：单个注意力头只能学习一种类型的关系
- **信息瓶颈**：所有信息都要通过同一个注意力权重矩阵
- **缺乏多样性**：无法同时关注不同类型的语言现象

### 1.2 多头注意力的优势
- **多视角表示**：不同头可以学习不同类型的关系（语法、语义、位置等）
- **并行计算**：多个头可以并行处理，提高计算效率
- **信息融合**：通过线性组合融合多个头的信息

## 2. 数学原理详解

### 2.1 核心公式
多头注意力的数学定义如下：

```
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O

其中：
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### 2.2 参数矩阵
对于每个头 i，我们有三个投影矩阵：
- `W_i^Q ∈ R^{d_model × d_k}`: Query投影矩阵
- `W_i^K ∈ R^{d_model × d_k}`: Key投影矩阵  
- `W_i^V ∈ R^{d_model × d_v}`: Value投影矩阵

输出投影矩阵：
- `W^O ∈ R^{hd_v × d_model}`: 最终输出投影矩阵

### 2.3 维度分析
假设：
- `d_model = 512` (模型维度)
- `h = 8` (注意力头数)
- `d_k = d_v = d_model / h = 64` (每个头的维度)

输入维度：`[batch_size, seq_len, d_model]`
每个头的输出维度：`[batch_size, seq_len, d_v]`
拼接后维度：`[batch_size, seq_len, h × d_v] = [batch_size, seq_len, d_model]`

## 3. 计算过程详解

### 3.1 步骤分解
1. **线性投影**：将输入Q、K、V分别投影到h个子空间
2. **并行注意力**：在每个子空间中计算注意力
3. **拼接融合**：将所有头的输出拼接
4. **最终投影**：通过W^O矩阵得到最终输出

### 3.2 具体计算示例
假设输入序列长度为4，d_model=8，h=2：

```python
# 输入
Q = [[1,2,3,4,5,6,7,8],    # token 1
     [2,3,4,5,6,7,8,9],    # token 2  
     [3,4,5,6,7,8,9,10],   # token 3
     [4,5,6,7,8,9,10,11]]  # token 4

# 对于头1：d_k = 4
W_1^Q = [[0.1, 0.2, 0.3, 0.4],
         [0.2, 0.3, 0.4, 0.5],
         [0.3, 0.4, 0.5, 0.6],
         [0.4, 0.5, 0.6, 0.7],
         [0.5, 0.6, 0.7, 0.8],
         [0.6, 0.7, 0.8, 0.9],
         [0.7, 0.8, 0.9, 1.0],
         [0.8, 0.9, 1.0, 1.1]]

# 计算Q1 = Q @ W_1^Q
Q1 = Q @ W_1^Q  # shape: [4, 4]
```

## 4. 不同头的作用机制

### 4.1 语言学视角
研究表明，不同的注意力头会学习到不同类型的语言关系：

- **句法头**：关注语法结构，如主谓关系、修饰关系
- **语义头**：关注语义相关性，如同义词、反义词
- **位置头**：关注位置信息，如相邻词、距离关系
- **功能头**：关注功能词，如介词、连词的作用

### 4.2 注意力模式
不同头会形成不同的注意力模式：
- **局部模式**：主要关注相邻的词
- **全局模式**：关注整个序列的信息
- **稀疏模式**：只关注少数几个重要位置
- **均匀模式**：对所有位置给予相似的注意力

## 5. 计算复杂度分析

### 5.1 时间复杂度
- **单头注意力**：O(n²d)
- **多头注意力**：O(n²d) (并行计算)
- **额外开销**：线性投影 O(nd²)

总体时间复杂度：O(n²d + nd²)

### 5.2 空间复杂度
- **参数存储**：O(hd²) 
- **中间结果**：O(hn²d)
- **注意力权重**：O(hn²)

### 5.3 并行化优势
多头注意力的一个重要优势是可以完全并行化：
- 不同头之间没有依赖关系
- 可以在不同GPU上并行计算
- 内存访问模式友好

## 6. 超参数选择

### 6.1 头数选择
- **常见选择**：8头（BERT）、12头（GPT-3）、16头（大模型）
- **经验法则**：d_model应该能被h整除
- **性能权衡**：更多头 ≠ 更好性能

### 6.2 维度分配
- **均匀分配**：d_k = d_v = d_model / h
- **不均匀分配**：某些头使用更大维度
- **实践建议**：通常使用均匀分配

## 7. 实现技巧

### 7.1 高效实现
- **批量矩阵乘法**：使用torch.bmm或torch.matmul
- **内存优化**：重用中间结果
- **数值稳定性**：注意softmax的数值稳定性

### 7.2 初始化策略
- **Xavier初始化**：适用于线性层
- **小随机值**：避免梯度消失
- **层归一化**：稳定训练过程

## 8. 常见问题与解决方案

### 8.1 梯度消失/爆炸
- **问题**：深层网络中梯度传播困难
- **解决**：残差连接、层归一化、梯度裁剪

### 8.2 注意力坍塌
- **问题**：所有头学习相似的注意力模式
- **解决**：正则化、不同的初始化、注意力多样性损失

### 8.3 计算效率
- **问题**：大序列长度下计算开销巨大
- **解决**：稀疏注意力、局部注意力、线性注意力

## 9. 与其他注意力机制的比较

### 9.1 vs 单头注意力
| 特性 | 单头注意力 | 多头注意力 |
|------|------------|------------|
| 表示能力 | 有限 | 丰富 |
| 计算复杂度 | O(n²d) | O(n²d) |
| 参数量 | 少 | 多 |
| 并行性 | 无 | 高 |

### 9.2 vs 卷积神经网络
| 特性 | CNN | 多头注意力 |
|------|-----|------------|
| 感受野 | 局部 | 全局 |
| 位置不变性 | 是 | 否 |
| 长距离依赖 | 困难 | 容易 |
| 计算复杂度 | O(nd) | O(n²d) |

## 10. 总结

多头注意力机制是Transformer架构的核心创新之一，它通过以下方式解决了单头注意力的局限性：

1. **多视角表示**：不同头学习不同类型的关系
2. **并行计算**：提高计算效率
3. **信息融合**：通过线性组合整合多种信息
4. **可解释性**：不同头的注意力模式可以分析

理解多头注意力的关键在于：
- 掌握数学公式和计算过程
- 理解不同头的作用机制
- 熟悉实现技巧和优化方法
- 了解超参数选择的原则

在接下来的实现部分，我们将把这些理论知识转化为可运行的代码。 