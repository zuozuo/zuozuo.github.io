# 多头注意力机制：理论深度解析

## 1. 引言：为什么需要多头注意力？

在Day 02中，我们学习了自注意力机制，它能够让模型关注序列中的不同位置。但是，单一的注意力头存在一些局限性：

### 1.1 单头注意力的局限性
- **表示能力有限**：单个注意力头只能学习一种类型的关系
- **信息瓶颈**：所有信息都要通过同一个注意力权重矩阵
- **缺乏多样性**：无法同时关注不同类型的语言现象

### 1.2 多头注意力的优势
- **多视角表示**：不同头可以学习不同类型的关系（语法、语义、位置等）
- **并行计算**：多个头可以并行处理，提高计算效率
- **信息融合**：通过线性组合融合多个头的信息

## 2. 数学原理详解

### 2.1 核心公式
多头注意力的数学定义如下：

```
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O

其中：
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### 2.2 参数矩阵
对于每个头 i，我们有三个投影矩阵：
- `W_i^Q ∈ R^{d_model × d_k}`: Query投影矩阵
- `W_i^K ∈ R^{d_model × d_k}`: Key投影矩阵  
- `W_i^V ∈ R^{d_model × d_v}`: Value投影矩阵

输出投影矩阵：
- `W^O ∈ R^{hd_v × d_model}`: 最终输出投影矩阵

### 2.3 维度分析
假设：
- `d_model = 512` (模型维度)
- `h = 8` (注意力头数)
- `d_k = d_v = d_model / h = 64` (每个头的维度)

输入维度：`[batch_size, seq_len, d_model]`
每个头的输出维度：`[batch_size, seq_len, d_v]`
拼接后维度：`[batch_size, seq_len, h × d_v] = [batch_size, seq_len, d_model]`

## 3. 计算过程详解

### 3.1 步骤分解
1. **线性投影**：将输入Q、K、V分别投影到h个子空间
2. **并行注意力**：在每个子空间中计算注意力
3. **拼接融合**：将所有头的输出拼接
4. **最终投影**：通过W^O矩阵得到最终输出

### 3.2 具体计算示例
假设输入序列长度为4，d_model=8，h=2：

```python
# 输入矩阵 (4个token，每个token 8维)
Q = K = V = [[1,2,3,4,5,6,7,8],    # token 1
             [2,3,4,5,6,7,8,9],    # token 2  
             [3,4,5,6,7,8,9,10],   # token 3
             [4,5,6,7,8,9,10,11]]  # token 4

# 头1的投影矩阵 (d_k = d_model/h = 4)
W_1^Q = [[0.1, 0.2, 0.3, 0.4],
         [0.2, 0.3, 0.4, 0.5],
         [0.3, 0.4, 0.5, 0.6],
         [0.4, 0.5, 0.6, 0.7],
         [0.5, 0.6, 0.7, 0.8],
         [0.6, 0.7, 0.8, 0.9],
         [0.7, 0.8, 0.9, 1.0],
         [0.8, 0.9, 1.0, 1.1]]

W_1^K = [[0.2, 0.1, 0.4, 0.3],
         [0.3, 0.2, 0.5, 0.4],
         [0.4, 0.3, 0.6, 0.5],
         [0.5, 0.4, 0.7, 0.6],
         [0.6, 0.5, 0.8, 0.7],
         [0.7, 0.6, 0.9, 0.8],
         [0.8, 0.7, 1.0, 0.9],
         [0.9, 0.8, 1.1, 1.0]]

W_1^V = [[0.3, 0.4, 0.1, 0.2],
         [0.4, 0.5, 0.2, 0.3],
         [0.5, 0.6, 0.3, 0.4],
         [0.6, 0.7, 0.4, 0.5],
         [0.7, 0.8, 0.5, 0.6],
         [0.8, 0.9, 0.6, 0.7],
         [0.9, 1.0, 0.7, 0.8],
         [1.0, 1.1, 0.8, 0.9]]

# 步骤1: 计算头1的Q、K、V
Q1 = Q @ W_1^Q  # shape: [4, 4]
K1 = K @ W_1^K  # shape: [4, 4]  
V1 = V @ W_1^V  # shape: [4, 4]

# 具体计算结果 (示例)
Q1 = [[22.0, 25.2, 28.4, 31.6],
      [26.0, 29.8, 33.6, 37.4],
      [30.0, 34.4, 38.8, 43.2],
      [34.0, 39.0, 44.0, 49.0]]

K1 = [[22.8, 19.6, 32.0, 28.8],
      [26.8, 23.0, 37.6, 33.8],
      [30.8, 26.4, 43.2, 38.8],
      [34.8, 29.8, 48.8, 43.8]]

V1 = [[25.6, 28.8, 19.2, 22.4],
      [30.4, 34.2, 22.8, 26.6],
      [35.2, 39.6, 26.4, 30.8],
      [40.0, 45.0, 30.0, 35.0]]

# 步骤2: 计算注意力分数
scores1 = Q1 @ K1^T / sqrt(d_k)  # shape: [4, 4]
scores1 = scores1 / sqrt(4) = scores1 / 2

# 步骤3: 应用softmax得到注意力权重
attention_weights1 = softmax(scores1)  # shape: [4, 4]

# 步骤4: 计算头1的输出
head1_output = attention_weights1 @ V1  # shape: [4, 4]

# 类似地计算头2
# 头2使用不同的投影矩阵W_2^Q, W_2^K, W_2^V
W_2^Q = [[0.2, 0.4, 0.1, 0.3],
         [0.3, 0.5, 0.2, 0.4],
         [0.4, 0.6, 0.3, 0.5],
         [0.5, 0.7, 0.4, 0.6],
         [0.6, 0.8, 0.5, 0.7],
         [0.7, 0.9, 0.6, 0.8],
         [0.8, 1.0, 0.7, 0.9],
         [0.9, 1.1, 0.8, 1.0]]

Q2 = Q @ W_2^Q  # shape: [4, 4]
K2 = K @ W_2^K  # shape: [4, 4]
V2 = V @ W_2^V  # shape: [4, 4]

# 计算头2的注意力
scores2 = Q2 @ K2^T / sqrt(4)
attention_weights2 = softmax(scores2)
head2_output = attention_weights2 @ V2  # shape: [4, 4]

# 步骤5: 拼接所有头的输出
concat_output = [head1_output, head2_output]  # shape: [4, 8]
# 即每行拼接: [head1_row, head2_row]

# 步骤6: 最终线性投影
W^O = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],
       [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
       [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
       [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1],
       [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2],
       [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3],
       [0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4],
       [0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]]

final_output = concat_output @ W^O  # shape: [4, 8]

# 最终输出维度与输入相同: [4, 8]
```

### 3.3 关键观察
从这个计算示例中，我们可以观察到：

1. **维度保持**：输入和输出的维度完全相同 `[seq_len, d_model]`
2. **并行计算**：每个头的计算完全独立，可以并行进行
3. **信息融合**：通过拼接和最终投影，融合了多个头的信息
4. **参数效率**：虽然有多个头，但总参数量与单头相当

### 3.4 计算量分析
对于这个示例：
- **线性投影**: 3 × h × (seq_len × d_model × d_k) = 3 × 2 × (4 × 8 × 4) = 768 次乘法
- **注意力计算**: h × (seq_len × seq_len × d_k) = 2 × (4 × 4 × 4) = 128 次乘法  
- **最终投影**: seq_len × d_model × d_model = 4 × 8 × 8 = 256 次乘法
- **总计**: 768 + 128 + 256 = 1152 次乘法运算

## 4. 不同头的作用机制

### 4.1 语言学视角
研究表明，不同的注意力头会学习到不同类型的语言关系：

- **句法头**：关注语法结构，如主谓关系、修饰关系
- **语义头**：关注语义相关性，如同义词、反义词
- **位置头**：关注位置信息，如相邻词、距离关系
- **功能头**：关注功能词，如介词、连词的作用

### 4.2 注意力模式
不同头会形成不同的注意力模式：
- **局部模式**：主要关注相邻的词
- **全局模式**：关注整个序列的信息
- **稀疏模式**：只关注少数几个重要位置
- **均匀模式**：对所有位置给予相似的注意力

## 5. 计算复杂度分析

### 5.1 时间复杂度
- **单头注意力**：O(n²d)
- **多头注意力**：O(n²d) (并行计算)
- **额外开销**：线性投影 O(nd²)

总体时间复杂度：O(n²d + nd²)

### 5.2 空间复杂度
- **参数存储**：O(hd²) 
- **中间结果**：O(hn²d)
- **注意力权重**：O(hn²)

### 5.3 并行化优势
多头注意力的一个重要优势是可以完全并行化：
- 不同头之间没有依赖关系
- 可以在不同GPU上并行计算
- 内存访问模式友好

## 6. 超参数选择

### 6.1 头数选择
- **常见选择**：8头（BERT）、12头（GPT-3）、16头（大模型）
- **经验法则**：d_model应该能被h整除
- **性能权衡**：更多头 ≠ 更好性能

### 6.2 维度分配
- **均匀分配**：d_k = d_v = d_model / h
- **不均匀分配**：某些头使用更大维度
- **实践建议**：通常使用均匀分配

## 7. 实现技巧

### 7.1 高效实现
- **批量矩阵乘法**：使用torch.bmm或torch.matmul
- **内存优化**：重用中间结果
- **数值稳定性**：注意softmax的数值稳定性

### 7.2 初始化策略
- **Xavier初始化**：适用于线性层
- **小随机值**：避免梯度消失
- **层归一化**：稳定训练过程

## 8. 常见问题与解决方案

### 8.1 梯度消失/爆炸
- **问题**：深层网络中梯度传播困难
- **解决**：残差连接、层归一化、梯度裁剪

### 8.2 注意力坍塌
- **问题**：所有头学习相似的注意力模式
- **解决**：正则化、不同的初始化、注意力多样性损失

### 8.3 计算效率
- **问题**：大序列长度下计算开销巨大
- **解决**：稀疏注意力、局部注意力、线性注意力

## 9. 与其他注意力机制的比较

### 9.1 vs 单头注意力
| 特性 | 单头注意力 | 多头注意力 |
|------|------------|------------|
| 表示能力 | 有限 | 丰富 |
| 计算复杂度 | O(n²d) | O(n²d) |
| 参数量 | 少 | 多 |
| 并行性 | 无 | 高 |

### 9.2 vs 卷积神经网络
| 特性 | CNN | 多头注意力 |
|------|-----|------------|
| 感受野 | 局部 | 全局 |
| 位置不变性 | 是 | 否 |
| 长距离依赖 | 困难 | 容易 |
| 计算复杂度 | O(nd) | O(n²d) |

## 10. 总结

多头注意力机制是Transformer架构的核心创新之一，它通过以下方式解决了单头注意力的局限性：

1. **多视角表示**：不同头学习不同类型的关系
2. **并行计算**：提高计算效率
3. **信息融合**：通过线性组合整合多种信息
4. **可解释性**：不同头的注意力模式可以分析

理解多头注意力的关键在于：
- 掌握数学公式和计算过程
- 理解不同头的作用机制
- 熟悉实现技巧和优化方法
- 了解超参数选择的原则

在接下来的实现部分，我们将把这些理论知识转化为可运行的代码。 