# Day 03: 多头注意力机制 (Multi-Head Attention)

## 学习目标
- 深入理解多头注意力的设计原理和动机
- 掌握多头注意力的数学公式和计算过程
- 实现高效的多头注意力机制
- 通过实验验证不同头数的效果差异

## 核心概念

### 1. 多头注意力的动机
- **表示子空间**：不同的注意力头可以关注不同类型的关系
- **并行计算**：多个头可以并行计算，提高效率
- **信息融合**：多个视角的信息融合提供更丰富的表示

### 2. 数学公式
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
其中 head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### 3. 关键参数
- `d_model`: 模型维度 (通常512或768)
- `h`: 注意力头数 (通常8或12)
- `d_k = d_v = d_model / h`: 每个头的维度

## 文件结构
- `theory.md`: 理论知识详解
- `implementation.py`: 多头注意力实现
- `experiments.py`: 实验验证代码
- `outputs/`: 实验结果和可视化

## 学习时间安排
- 理论学习: 2小时
- 代码实现: 2.5小时  
- 实验验证: 0.5小时

## 前置知识
- Day 01: 注意力机制基础
- Day 02: 自注意力机制
- 线性代数基础（矩阵乘法、向量空间） 