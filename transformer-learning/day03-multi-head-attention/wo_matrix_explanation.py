"""
W^O çŸ©é˜µè¯¦è§£ï¼šå¤šå¤´æ³¨æ„åŠ›ä¸­çš„è¾“å‡ºæŠ•å½±çŸ©é˜µ
æ·±å…¥è§£æ W^O çš„æ¥æºã€ä½œç”¨å’Œé‡è¦æ€§
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

def explain_wo_matrix():
    """
    è¯¦ç»†è§£é‡Š W^O çŸ©é˜µçš„æ¥æºå’Œä½œç”¨
    """
    print("=" * 80)
    print("W^O çŸ©é˜µè¯¦è§£ï¼šå¤šå¤´æ³¨æ„åŠ›ä¸­çš„è¾“å‡ºæŠ•å½±çŸ©é˜µ")
    print("=" * 80)
    
    print("\nğŸ¤” é—®é¢˜ï¼šW^O æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ")
    print("\nğŸ“š ç­”æ¡ˆï¼šW^O æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä¸€ä¸ª**å¯å­¦ä¹ å‚æ•°çŸ©é˜µ**")
    print("å®ƒä¸æ˜¯æ‰‹å·¥è®¾è®¡çš„ï¼Œè€Œæ˜¯é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°çš„ï¼")
    
    print("\n" + "="*60)
    print("1ï¸âƒ£ W^O çš„å®šä¹‰å’Œæ¥æº")
    print("="*60)
    
    print("""
ğŸ” å®šä¹‰ï¼š
W^O âˆˆ R^{hÃ—d_k Ã— d_model} æ˜¯è¾“å‡ºæŠ•å½±çŸ©é˜µ

ğŸ¯ æ¥æºï¼š
- W^O æ˜¯ç¥ç»ç½‘ç»œçš„ä¸€ä¸ª**å¯å­¦ä¹ å‚æ•°**
- åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶éšæœºåˆå§‹åŒ–
- é€šè¿‡åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒ
- æœ€ç»ˆå­¦ä¹ åˆ°å¦‚ä½•æœ€å¥½åœ°èåˆå¤šå¤´ä¿¡æ¯

ğŸ“ ç»´åº¦ï¼š
- è¾“å…¥ç»´åº¦ï¼šh Ã— d_k (æ‹¼æ¥åçš„å¤šå¤´è¾“å‡º)
- è¾“å‡ºç»´åº¦ï¼šd_model (æ¢å¤åˆ°åŸå§‹æ¨¡å‹ç»´åº¦)
- çŸ©é˜µå½¢çŠ¶ï¼š[hÃ—d_k, d_model]
    """)
    
    print("\n" + "="*60)
    print("2ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦ W^Oï¼Ÿ")
    print("="*60)
    
    print("""
ğŸ¯ æ ¸å¿ƒä½œç”¨ï¼š
1. **ç»´åº¦æ¢å¤**ï¼šå°†æ‹¼æ¥åçš„ hÃ—d_k ç»´åº¦æ˜ å°„å› d_model
2. **ä¿¡æ¯èåˆ**ï¼šå­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ç»„åˆå¤šä¸ªå¤´çš„ä¿¡æ¯
3. **è¡¨ç¤ºå­¦ä¹ **ï¼šå­¦ä¹ æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤º
4. **æ®‹å·®è¿æ¥**ï¼šç¡®ä¿è¾“å‡ºå¯ä»¥ä¸è¾“å…¥è¿›è¡Œæ®‹å·®è¿æ¥

ğŸ”„ æ²¡æœ‰ W^O ä¼šæ€æ ·ï¼Ÿ
- è¾“å‡ºç»´åº¦ä¸åŒ¹é…ï¼šhÃ—d_k â‰  d_model
- æ— æ³•è¿›è¡Œæ®‹å·®è¿æ¥
- å¤šå¤´ä¿¡æ¯ç®€å•æ‹¼æ¥ï¼Œç¼ºä¹å­¦ä¹ çš„èåˆ
    """)

def demonstrate_wo_importance():
    """
    æ¼”ç¤º W^O çŸ©é˜µçš„é‡è¦æ€§
    """
    print("\n" + "="*60)
    print("3ï¸âƒ£ W^O çš„é‡è¦æ€§æ¼”ç¤º")
    print("="*60)
    
    # è®¾ç½®å‚æ•°
    seq_len, d_model, num_heads = 4, 8, 2
    d_k = d_model // num_heads  # d_k = 4
    
    print(f"å‚æ•°è®¾ç½®ï¼šseq_len={seq_len}, d_model={d_model}, num_heads={num_heads}, d_k={d_k}")
    
    # æ¨¡æ‹Ÿå¤šå¤´è¾“å‡ºï¼ˆæ‹¼æ¥åï¼‰
    concat_output = torch.randn(seq_len, num_heads * d_k)  # [4, 8]
    print(f"\næ‹¼æ¥åçš„å¤šå¤´è¾“å‡ºå½¢çŠ¶ï¼š{concat_output.shape}")
    print("æ‹¼æ¥è¾“å‡ºï¼ˆå‰2è¡Œï¼‰ï¼š")
    print(concat_output[:2])
    
    # æƒ…å†µ1ï¼šæ²¡æœ‰ W^Oï¼ˆç›´æ¥ä½¿ç”¨æ‹¼æ¥è¾“å‡ºï¼‰
    print(f"\nâŒ æƒ…å†µ1ï¼šæ²¡æœ‰ W^O")
    print(f"è¾“å‡ºç»´åº¦ï¼š{concat_output.shape}")
    print("é—®é¢˜ï¼šç»´åº¦ä¸åŒ¹é…ï¼Œæ— æ³•ä¸è¾“å…¥è¿›è¡Œæ®‹å·®è¿æ¥")
    
    # æƒ…å†µ2ï¼šä½¿ç”¨æ’ç­‰æ˜ å°„ä½œä¸º W^O
    print(f"\nâš ï¸ æƒ…å†µ2ï¼šä½¿ç”¨æ’ç­‰æ˜ å°„")
    W_O_identity = torch.eye(d_model)  # 8x8 æ’ç­‰çŸ©é˜µ
    output_identity = torch.matmul(concat_output, W_O_identity)
    print(f"W^O = æ’ç­‰çŸ©é˜µï¼Œè¾“å‡ºå½¢çŠ¶ï¼š{output_identity.shape}")
    print("é—®é¢˜ï¼šæ²¡æœ‰å­¦ä¹ åˆ°å¦‚ä½•èåˆå¤šå¤´ä¿¡æ¯")
    
    # æƒ…å†µ3ï¼šä½¿ç”¨å¯å­¦ä¹ çš„ W^O
    print(f"\nâœ… æƒ…å†µ3ï¼šä½¿ç”¨å¯å­¦ä¹ çš„ W^O")
    W_O_learned = torch.randn(d_model, d_model) * 0.1  # æ¨¡æ‹Ÿå­¦ä¹ åˆ°çš„æƒé‡
    output_learned = torch.matmul(concat_output, W_O_learned)
    print(f"W^O = å­¦ä¹ åˆ°çš„çŸ©é˜µï¼Œè¾“å‡ºå½¢çŠ¶ï¼š{output_learned.shape}")
    print("ä¼˜åŠ¿ï¼šå­¦ä¹ åˆ°æœ€ä¼˜çš„å¤šå¤´ä¿¡æ¯èåˆæ–¹å¼")
    
    return concat_output, W_O_identity, W_O_learned, output_learned

def show_wo_initialization_strategies():
    """
    å±•ç¤º W^O çš„ä¸åŒåˆå§‹åŒ–ç­–ç•¥
    """
    print("\n" + "="*60)
    print("4ï¸âƒ£ W^O çš„åˆå§‹åŒ–ç­–ç•¥")
    print("="*60)
    
    d_model = 8
    
    # 1. Xavier/Glorot åˆå§‹åŒ–
    print("1. Xavier/Glorot åˆå§‹åŒ–ï¼ˆæ¨èï¼‰")
    W_O_xavier = torch.empty(d_model, d_model)
    nn.init.xavier_uniform_(W_O_xavier)
    print(f"Xavieråˆå§‹åŒ–çš„ W^Oï¼š")
    print(W_O_xavier)
    print(f"æ–¹å·®ï¼š{W_O_xavier.var().item():.4f}")
    
    # 2. æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
    print(f"\n2. æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–")
    W_O_normal = torch.randn(d_model, d_model) * 0.02
    print(f"æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„ W^Oï¼š")
    print(W_O_normal)
    print(f"æ–¹å·®ï¼š{W_O_normal.var().item():.4f}")
    
    # 3. é›¶åˆå§‹åŒ–ï¼ˆä¸æ¨èï¼‰
    print(f"\n3. é›¶åˆå§‹åŒ–ï¼ˆä¸æ¨èï¼‰")
    W_O_zero = torch.zeros(d_model, d_model)
    print(f"é›¶åˆå§‹åŒ–çš„ W^Oï¼š")
    print(W_O_zero)
    print("é—®é¢˜ï¼šä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œæ‰€æœ‰ç¥ç»å…ƒå­¦ä¹ ç›¸åŒçš„ç‰¹å¾")

def analyze_wo_learning_process():
    """
    åˆ†æ W^O çš„å­¦ä¹ è¿‡ç¨‹
    """
    print("\n" + "="*60)
    print("5ï¸âƒ£ W^O çš„å­¦ä¹ è¿‡ç¨‹")
    print("="*60)
    
    print("""
ğŸ”„ è®­ç»ƒè¿‡ç¨‹ï¼š
1. **å‰å‘ä¼ æ’­**ï¼š
   - å¤šå¤´æ³¨æ„åŠ›è®¡ç®—å„è‡ªçš„è¾“å‡º
   - æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡ºï¼šconcat_output
   - é€šè¿‡ W^O è¿›è¡Œçº¿æ€§å˜æ¢ï¼šoutput = concat_output @ W^O

2. **æŸå¤±è®¡ç®—**ï¼š
   - è®¡ç®—é¢„æµ‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾çš„æŸå¤±
   - æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€MSEç­‰ï¼‰

3. **åå‘ä¼ æ’­**ï¼š
   - è®¡ç®—æŸå¤±å¯¹ W^O çš„æ¢¯åº¦ï¼šâˆ‚L/âˆ‚W^O
   - æ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™ä¼ æ’­

4. **å‚æ•°æ›´æ–°**ï¼š
   - W^O = W^O - learning_rate Ã— âˆ‚L/âˆ‚W^O
   - ä½¿ç”¨ä¼˜åŒ–å™¨ï¼ˆAdamã€SGDç­‰ï¼‰è¿›è¡Œæ›´æ–°

ğŸ¯ å­¦ä¹ ç›®æ ‡ï¼š
W^O å­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ç»„åˆå¤šä¸ªå¤´çš„ä¿¡æ¯ï¼Œä½¿å¾—æœ€ç»ˆè¾“å‡ºï¼š
- ä¿ç•™é‡è¦çš„è¯­ä¹‰ä¿¡æ¯
- è¿‡æ»¤æ‰å™ªå£°å’Œå†—ä½™ä¿¡æ¯
- é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚
    """)

def compare_with_without_wo():
    """
    å¯¹æ¯”æœ‰æ—  W^O çš„æ•ˆæœ
    """
    print("\n" + "="*60)
    print("6ï¸âƒ£ æœ‰æ—  W^O çš„å¯¹æ¯”å®éªŒ")
    print("="*60)
    
    # åˆ›å»ºç®€å•çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    class MultiHeadAttentionWithoutWO(nn.Module):
        def __init__(self, d_model, num_heads):
            super().__init__()
            self.d_model = d_model
            self.num_heads = num_heads
            self.d_k = d_model // num_heads
            
            self.W_q = nn.Linear(d_model, d_model, bias=False)
            self.W_k = nn.Linear(d_model, d_model, bias=False)
            self.W_v = nn.Linear(d_model, d_model, bias=False)
            # æ³¨æ„ï¼šæ²¡æœ‰ W^O
        
        def forward(self, x):
            batch_size, seq_len, d_model = x.size()
            
            Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            
            scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
            attention = torch.softmax(scores, dim=-1)
            output = torch.matmul(attention, V)
            
            # ç›´æ¥æ‹¼æ¥ï¼Œæ²¡æœ‰ W^O
            output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            return output
    
    class MultiHeadAttentionWithWO(nn.Module):
        def __init__(self, d_model, num_heads):
            super().__init__()
            self.d_model = d_model
            self.num_heads = num_heads
            self.d_k = d_model // num_heads
            
            self.W_q = nn.Linear(d_model, d_model, bias=False)
            self.W_k = nn.Linear(d_model, d_model, bias=False)
            self.W_v = nn.Linear(d_model, d_model, bias=False)
            self.W_o = nn.Linear(d_model, d_model, bias=False)  # æœ‰ W^O
        
        def forward(self, x):
            batch_size, seq_len, d_model = x.size()
            
            Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
            
            scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
            attention = torch.softmax(scores, dim=-1)
            output = torch.matmul(attention, V)
            
            # æ‹¼æ¥åé€šè¿‡ W^O
            output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
            output = self.W_o(output)  # å…³é”®å·®å¼‚
            return output
    
    # æµ‹è¯•
    batch_size, seq_len, d_model, num_heads = 2, 4, 8, 2
    x = torch.randn(batch_size, seq_len, d_model)
    
    model_without_wo = MultiHeadAttentionWithoutWO(d_model, num_heads)
    model_with_wo = MultiHeadAttentionWithWO(d_model, num_heads)
    
    output_without = model_without_wo(x)
    output_with = model_with_wo(x)
    
    print(f"è¾“å…¥å½¢çŠ¶ï¼š{x.shape}")
    print(f"æ—  W^O è¾“å‡ºå½¢çŠ¶ï¼š{output_without.shape}")
    print(f"æœ‰ W^O è¾“å‡ºå½¢çŠ¶ï¼š{output_with.shape}")
    
    # è®¡ç®—å‚æ•°é‡
    params_without = sum(p.numel() for p in model_without_wo.parameters())
    params_with = sum(p.numel() for p in model_with_wo.parameters())
    
    print(f"\nå‚æ•°é‡å¯¹æ¯”ï¼š")
    print(f"æ—  W^Oï¼š{params_without} å‚æ•°")
    print(f"æœ‰ W^Oï¼š{params_with} å‚æ•°")
    print(f"W^O å¢åŠ çš„å‚æ•°ï¼š{params_with - params_without}")

def practical_example():
    """
    å®é™…çš„ W^O ç¤ºä¾‹
    """
    print("\n" + "="*60)
    print("7ï¸âƒ£ å®é™…çš„ W^O ç¤ºä¾‹")
    print("="*60)
    
    print("""
ğŸ“ åœ¨å®é™…ä»£ç ä¸­ï¼ŒW^O é€šå¸¸è¿™æ ·å®šä¹‰ï¼š

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        
        # è¾“å…¥æŠ•å½±
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model) 
        self.W_v = nn.Linear(d_model, d_model)
        
        # è¾“å‡ºæŠ•å½± - è¿™å°±æ˜¯ W^Oï¼
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        # ... å¤šå¤´æ³¨æ„åŠ›è®¡ç®— ...
        # concat_output: [batch, seq_len, d_model]
        
        # é€šè¿‡ W^O è¿›è¡Œæœ€ç»ˆæŠ•å½±
        output = self.W_o(concat_output)
        return output
```

ğŸ”‘ å…³é”®ç‚¹ï¼š
1. W^O å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„ nn.Linear å±‚
2. å®ƒçš„æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ 
3. åˆå§‹åŒ–é€šå¸¸ä½¿ç”¨ Xavier æˆ– He åˆå§‹åŒ–
4. é€šè¿‡åå‘ä¼ æ’­è‡ªåŠ¨æ›´æ–°
    """)

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰è§£é‡Š
    explain_wo_matrix()
    concat_output, W_O_identity, W_O_learned, output_learned = demonstrate_wo_importance()
    show_wo_initialization_strategies()
    analyze_wo_learning_process()
    compare_with_without_wo()
    practical_example()
    
    print("\n" + "="*80)
    print("ğŸ¯ æ€»ç»“ï¼šW^O çŸ©é˜µçš„æœ¬è´¨")
    print("="*80)
    print("""
W^O ä¸æ˜¯æ‰‹å·¥è®¾è®¡çš„å›ºå®šçŸ©é˜µï¼Œè€Œæ˜¯ï¼š

1. **å¯å­¦ä¹ å‚æ•°**ï¼šé€šè¿‡è®­ç»ƒè‡ªåŠ¨å­¦ä¹ å¾—åˆ°
2. **ä¿¡æ¯èåˆå™¨**ï¼šå­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ç»„åˆå¤šå¤´ä¿¡æ¯  
3. **ç»´åº¦æ˜ å°„å™¨**ï¼šå°†æ‹¼æ¥çš„å¤šå¤´è¾“å‡ºæ˜ å°„å›åŸå§‹ç»´åº¦
4. **è¡¨ç¤ºå­¦ä¹ å™¨**ï¼šå­¦ä¹ ä»»åŠ¡ç›¸å…³çš„æœ€ç»ˆè¡¨ç¤º

åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼ŒW^O çš„å…·ä½“æ•°å€¼åªæ˜¯ä¸ºäº†æ¼”ç¤ºè®¡ç®—è¿‡ç¨‹ï¼Œ
å®é™…åº”ç”¨ä¸­è¿™äº›æ•°å€¼æ˜¯é€šè¿‡å¤§é‡æ•°æ®è®­ç»ƒå­¦ä¹ å¾—åˆ°çš„ï¼
    """) 