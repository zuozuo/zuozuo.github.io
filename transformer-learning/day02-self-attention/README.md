# Day 2: 自注意力机制

## 学习目标
- 掌握自注意力的核心概念和数学原理
- 理解自注意力与传统注意力的本质区别
- 学习位置编码的必要性和多种实现方法
- 分析自注意力的计算复杂度特性
- 实现完整的自注意力机制

## 时间分配
- **理论学习**: 2小时
- **代码实现**: 2.5小时
- **实验验证**: 0.5小时

## 学习内容

### 1. 理论基础
- 自注意力机制的定义和直觉理解
- 自注意力vs传统注意力的核心区别
- 位置编码的必要性分析
- 计算复杂度：O(n²d) vs O(nd²)

### 2. 位置编码深入
- 绝对位置编码（正弦余弦编码）
- 相对位置编码
- 可学习位置编码
- 位置编码的数学原理

### 3. 代码实现
- 从零实现自注意力机制
- 实现多种位置编码方法
- 可视化自注意力权重矩阵
- 性能对比实验

### 4. 实验验证
- 在不同长度序列上测试
- 对比有无位置编码的效果
- 分析注意力模式
- 计算复杂度实际测试

## 文件结构
```
day02-self-attention/
├── README.md                    # 当天学习总结
├── theory.md                    # 理论学习笔记
├── implementation.py            # 自注意力机制实现
├── positional_encoding.py      # 位置编码实现
├── experiments.py               # 实验验证代码
├── visualization.py             # 可视化工具
└── outputs/                     # 输出文件
    ├── self_attention_demo.png  # 自注意力演示
    ├── position_encoding.png    # 位置编码可视化
    └── complexity_analysis.png  # 复杂度分析图
```

## 核心概念

### 自注意力公式
```
SelfAttention(X) = softmax(XW_Q(XW_K)^T / √d_k)(XW_V)
```

其中：
- X: 输入序列 [seq_len, d_model]
- W_Q, W_K, W_V: 学习参数矩阵
- 每个位置都同时作为Query、Key、Value

### 位置编码公式
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### 关键区别
| 特性 | 传统注意力 | 自注意力 |
|------|------------|----------|
| Query来源 | 解码器状态 | 输入序列本身 |
| Key/Value来源 | 编码器输出 | 输入序列本身 |
| 计算复杂度 | O(nd²) | O(n²d) |
| 并行化程度 | 部分并行 | 完全并行 |

## 今日重点
1. **理解自注意力的"自"字含义**：序列与自身计算注意力
2. **位置编码的重要性**：没有位置信息，模型无法区分词序
3. **计算复杂度权衡**：序列长度vs特征维度的影响

## 学习心得

### 核心理解突破
1. **自注意力的本质**：终于深刻理解了"自"字的含义 - 不是自己关注自己，而是序列中的每个位置都与同一序列中的所有位置计算注意力。这种设计让模型能够直接建模长距离依赖关系。

2. **位置编码的重要性**：通过实验清楚地看到，没有位置编码的自注意力机制完全无法区分词序。正弦余弦位置编码的设计非常巧妙，既保证了位置的唯一性，又具备外推能力。

3. **计算复杂度的权衡**：O(n²d)的复杂度意味着序列长度的影响是平方级的，这解释了为什么Transformer在处理长序列时会遇到困难。

### 技术实现收获
1. **矩阵运算的优雅**：自注意力机制的实现展现了矩阵运算的优雅，所有位置可以并行计算，这是相比RNN的巨大优势。

2. **掩码机制的灵活性**：通过不同的掩码可以实现因果注意力、填充掩码等多种功能，设计非常灵活。

3. **可视化的价值**：注意力权重的可视化让抽象的概念变得直观，能够清楚地看到模型在"关注"什么。

### 遇到的问题与解决
1. **维度理解混乱**：最初对Q、K、V矩阵的维度变换感到困惑，通过手工计算小例子才真正理解。
2. **位置编码公式**：正弦余弦公式看起来复杂，但理解其频率设计原理后发现非常合理。
3. **缩放因子√d_k**：通过方差分析理解了为什么需要这个缩放因子。

### 明天的期待
对多头注意力机制很期待，想了解如何通过多个"头"来捕获不同类型的关系。

## 明天预习
- 多头注意力机制的设计原理
- 并行计算的实现方法
- 不同头数对模型性能的影响 