# Day 2: 自注意力机制

## 学习目标
- 掌握自注意力的核心概念和数学原理
- 理解自注意力与传统注意力的本质区别
- 学习位置编码的必要性和多种实现方法
- 分析自注意力的计算复杂度特性
- 实现完整的自注意力机制

## 时间分配
- **理论学习**: 2小时
- **代码实现**: 2.5小时
- **实验验证**: 0.5小时

## 学习内容

### 1. 理论基础
- 自注意力机制的定义和直觉理解
- 自注意力vs传统注意力的核心区别
- 位置编码的必要性分析
- 计算复杂度：O(n²d) vs O(nd²)

### 2. 位置编码深入
- 绝对位置编码（正弦余弦编码）
- 相对位置编码
- 可学习位置编码
- 位置编码的数学原理

### 3. 代码实现
- 从零实现自注意力机制
- 实现多种位置编码方法
- 可视化自注意力权重矩阵
- 性能对比实验

### 4. 实验验证
- 在不同长度序列上测试
- 对比有无位置编码的效果
- 分析注意力模式
- 计算复杂度实际测试

## 文件结构
```
day02-self-attention/
├── README.md                    # 当天学习总结
├── theory.md                    # 理论学习笔记
├── implementation.py            # 自注意力机制实现
├── positional_encoding.py      # 位置编码实现
├── experiments.py               # 实验验证代码
├── visualization.py             # 可视化工具
└── outputs/                     # 输出文件
    ├── self_attention_demo.png  # 自注意力演示
    ├── position_encoding.png    # 位置编码可视化
    └── complexity_analysis.png  # 复杂度分析图
```

## 核心概念

### 自注意力公式
```
SelfAttention(X) = softmax(XW_Q(XW_K)^T / √d_k)(XW_V)
```

其中：
- X: 输入序列 [seq_len, d_model]
- W_Q, W_K, W_V: 学习参数矩阵
- 每个位置都同时作为Query、Key、Value

### 位置编码公式
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### 关键区别
| 特性 | 传统注意力 | 自注意力 |
|------|------------|----------|
| Query来源 | 解码器状态 | 输入序列本身 |
| Key/Value来源 | 编码器输出 | 输入序列本身 |
| 计算复杂度 | O(nd²) | O(n²d) |
| 并行化程度 | 部分并行 | 完全并行 |

## 今日重点
1. **理解自注意力的"自"字含义**：序列与自身计算注意力
2. **位置编码的重要性**：没有位置信息，模型无法区分词序
3. **计算复杂度权衡**：序列长度vs特征维度的影响

## 学习心得
[在这里记录今天的学习心得和遇到的问题]

## 明天预习
- 多头注意力机制的设计原理
- 并行计算的实现方法
- 不同头数对模型性能的影响 