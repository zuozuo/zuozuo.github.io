# Day 2 理论学习：自注意力机制深度解析

## 1. 自注意力机制的核心概念

### 1.1 什么是自注意力？

自注意力（Self-Attention）是注意力机制的一种特殊形式，其核心特点是**序列与自身计算注意力**。

**直觉理解**：
- 传统注意力：问"这个词应该关注输入序列的哪些部分？"
- 自注意力：问"这个词应该关注同一序列中的哪些其他词？"

**形象比喻**：
想象你在阅读一个句子："The animal didn't cross the street because it was too tired"
- 当处理"it"时，自注意力帮助模型理解"it"指的是"animal"而不是"street"
- 模型通过计算"it"与句子中每个词的相关性来做出这个判断

### 1.2 数学定义

给定输入序列 X ∈ R^(n×d)，自注意力的计算过程：

```
1. 生成Q, K, V矩阵：
   Q = XW_Q  # Query矩阵
   K = XW_K  # Key矩阵  
   V = XW_V  # Value矩阵

2. 计算注意力权重：
   Attention(Q,K,V) = softmax(QK^T/√d_k)V
```

**关键观察**：
- Q, K, V都来源于同一个输入序列X
- 这就是"自"注意力的含义

## 2. 自注意力 vs 传统注意力

### 2.1 结构对比

| 维度 | 传统注意力 | 自注意力 |
|------|------------|----------|
| **Query来源** | 解码器当前状态 | 输入序列本身 |
| **Key来源** | 编码器所有状态 | 输入序列本身 |
| **Value来源** | 编码器所有状态 | 输入序列本身 |
| **注意力对象** | 不同序列间 | 同一序列内 |

### 2.2 计算复杂度分析

**传统注意力复杂度**：O(nd²)
- n: 序列长度
- d: 特征维度
- 每个时间步计算一次注意力

**自注意力复杂度**：O(n²d)
- 需要计算序列中每对位置的相似度
- 当n >> d时，自注意力更昂贵
- 当d >> n时，传统注意力更昂贵

### 2.3 并行化能力

**传统注意力**：
- 解码时必须逐步进行
- 无法完全并行化

**自注意力**：
- 所有位置可以同时计算
- 完全并行化，训练效率高

## 3. 位置编码的必要性

### 3.1 问题分析

自注意力机制本身是**位置无关**的：
- 如果交换输入序列中任意两个位置的词，输出结果完全相同
- 这对于语言理解是致命的，因为词序承载重要语义信息

**例子**：
- "狗咬了人" vs "人咬了狗"
- 没有位置信息，模型无法区分这两个句子

### 3.2 位置编码的作用

位置编码为每个位置添加唯一的"位置标识"：
```
输入 = 词嵌入 + 位置编码
```

这样，即使是相同的词在不同位置也会有不同的表示。

### 3.3 正弦余弦位置编码

Transformer原论文使用的位置编码公式：

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**设计原理**：
1. **唯一性**：每个位置有唯一的编码
2. **相对位置**：模型可以学习相对位置关系
3. **外推性**：可以处理训练时未见过的序列长度

**频率分析**：
- 低维度使用高频率（快速变化）
- 高维度使用低频率（缓慢变化）
- 形成类似"二进制编码"的效果

### 3.4 位置编码的几何解释

可以将位置编码看作高维空间中的螺旋：
- 每个位置在高维空间中有唯一坐标
- 相邻位置在空间中也相对接近
- 距离远的位置在空间中距离也远

## 4. 自注意力的计算流程详解

### 4.1 步骤分解

以序列长度n=4，特征维度d=6为例：

```
输入: X = [x1, x2, x3, x4]  # shape: (4, 6)

步骤1: 线性变换
Q = XW_Q  # (4, 6) × (6, 6) = (4, 6)
K = XW_K  # (4, 6) × (6, 6) = (4, 6)  
V = XW_V  # (4, 6) × (6, 6) = (4, 6)

步骤2: 计算相似度矩阵
S = QK^T / √d_k  # (4, 6) × (6, 4) = (4, 4)

步骤3: 注意力权重
A = softmax(S)  # (4, 4)

步骤4: 加权求和
Output = AV  # (4, 4) × (4, 6) = (4, 6)
```

### 4.2 注意力矩阵的含义

注意力矩阵A[i,j]表示：
- 位置i的输出对位置j的输入的关注程度
- 每行和为1（softmax归一化）
- 对角线元素通常较大（自我关注）

## 5. 自注意力的优势与局限

### 5.1 优势

1. **长距离依赖**：可以直接建模任意距离的词对关系
2. **并行计算**：所有位置同时计算，训练效率高
3. **可解释性**：注意力权重提供模型决策的可视化
4. **灵活性**：不依赖于特定的序列结构

### 5.2 局限性

1. **计算复杂度**：O(n²)对长序列不友好
2. **内存消耗**：需要存储n×n的注意力矩阵
3. **位置编码依赖**：必须额外添加位置信息
4. **局部性缺失**：不像CNN那样有内置的局部偏置

## 6. 实际应用中的考虑

### 6.1 序列长度的影响

- **短序列**（n < 100）：自注意力效率很高
- **中等序列**（100 < n < 1000）：需要权衡计算成本
- **长序列**（n > 1000）：考虑稀疏注意力或其他优化

### 6.2 特征维度的选择

- d_model通常选择512或1024
- d_k = d_v = d_model / num_heads
- 需要平衡表达能力和计算效率

### 6.3 位置编码的选择

1. **固定编码**：正弦余弦编码，不需要学习
2. **学习编码**：可学习参数，但外推性差
3. **相对编码**：直接建模相对位置关系

## 7. 数学推导补充

### 7.1 为什么要除以√d_k？

缩放因子√d_k的作用：
- 防止点积过大导致softmax饱和
- 当d_k较大时，点积的方差约为d_k
- 除以√d_k使方差归一化为1

### 7.2 softmax的梯度特性

softmax函数的特点：
- 输出和为1，形成概率分布
- 梯度在极值处接近0，有助于稳定训练
- 但也可能导致梯度消失问题

## 8. 小结

自注意力机制是Transformer的核心创新：
1. **核心思想**：序列与自身计算注意力
2. **关键优势**：并行计算、长距离建模
3. **重要组件**：位置编码不可或缺
4. **计算特点**：O(n²)复杂度需要权衡

理解了自注意力，就理解了Transformer成功的关键所在。明天我们将学习多头注意力，看看如何进一步增强这一机制的表达能力。 