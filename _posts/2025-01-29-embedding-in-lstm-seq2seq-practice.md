---
layout: post
title: "ä»ç†è®ºåˆ°å®è·µï¼šæ·±åº¦è§£æEmbeddingåœ¨LSTM Seq2Seqæ¨¡å‹ä¸­çš„åº”ç”¨"
date: 2025-01-29 20:00:00 +0800
categories: [äººå·¥æ™ºèƒ½, æ·±åº¦å­¦ä¹ ]
tags: [embedding, lstm, seq2seq, æœºå™¨ç¿»è¯‘, pytorch, nlp]
author: Yonghui Zuo
description: "é€šè¿‡å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨ä»£ç å®ç°ï¼Œæ·±åº¦å‰–æembeddingåœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨æœºåˆ¶ã€è®­ç»ƒè¿‡ç¨‹å’Œå®è·µæŠ€å·§"
pin: false
math: true
mermaid: true
image:
  path: /assets/img/posts/embedding-lstm-cover.jpg
  alt: "Embeddingåœ¨LSTM Seq2Seqä¸­çš„åº”ç”¨ç¤ºæ„å›¾"
---

## å‰è¨€

åœ¨æ·±åº¦å­¦ä¹ çš„æµªæ½®ä¸­ï¼ŒembeddingæŠ€æœ¯æ‰®æ¼”ç€"è¯­ä¹‰æ¡¥æ¢"çš„å…³é”®è§’è‰²ï¼Œè€ŒLSTMåºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰æ¨¡å‹åˆ™æ˜¯å¤„ç†å˜é•¿åºåˆ—ä»»åŠ¡çš„ç»å…¸æ¶æ„ã€‚æœ¬æ–‡å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ä¸­è‹±æœºå™¨ç¿»è¯‘LSTMå®ç°ï¼Œæ·±åº¦å‰–æembeddingåœ¨å®é™…é¡¹ç›®ä¸­çš„å·¥ä½œæœºåˆ¶ã€è®­ç»ƒè¿‡ç¨‹å’Œä¼˜åŒ–ç­–ç•¥ã€‚

æˆ‘ä»¬ä¸ä»…ä¼šä»ç†è®ºå±‚é¢ç†è§£embeddingçš„æœ¬è´¨ï¼Œæ›´ä¼šé€šè¿‡PyTorchä»£ç çš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼Œå±•ç¤ºembeddingå¦‚ä½•åœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­å‘æŒ¥ä½œç”¨ï¼Œå¸®åŠ©ä½ å»ºç«‹ä»æ¦‚å¿µåˆ°å®è·µçš„å®Œæ•´è®¤çŸ¥ä½“ç³»ã€‚

## 1. æ¶æ„æ¦‚è§ˆï¼šEmbeddingåœ¨Seq2Seqä¸­çš„æˆ˜ç•¥åœ°ä½

### 1.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TD
    A[è¾“å…¥åºåˆ—: æˆ‘ çˆ± NLP] --> B[è¯æ±‡è¡¨æ˜ å°„]
    B --> C[ç´¢å¼•åºåˆ—: 4,5,6]
    C --> D[ç¼–ç å™¨Embeddingå±‚]
    D --> E[LSTMç¼–ç å™¨]
    E --> F[ä¸Šä¸‹æ–‡å‘é‡]
    F --> G[LSTMè§£ç å™¨]
    G --> H[è§£ç å™¨Embeddingå±‚]
    H --> I[è¾“å‡ºæŠ•å½±å±‚]
    I --> J[è¾“å‡ºåºåˆ—: I love NLP]
    
    style D fill:#e1f5fe
    style H fill:#e8f5e8
    style F fill:#fff3e0
```

### 1.2 æ ¸å¿ƒæ•°æ®æµåˆ†æ

è®©æˆ‘ä»¬é€šè¿‡å®é™…ä»£ç è¿½è¸ªembeddingçš„æ•°æ®æµè½¬ï¼š

```python
# ç¬¬1æ­¥ï¼šæ–‡æœ¬é¢„å¤„ç†
src_sentence = "æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"
src_indices = src_vocab.sentence_to_indices(src_sentence)  
# ç»“æœï¼š[4, 5, 6, 7, 8]

# ç¬¬2æ­¥ï¼šç¼–ç å™¨embeddingæŸ¥è¡¨
embedded = encoder.embedding(input_seq)  # [1, 5, 64]
# æ¯ä¸ªè¯ç´¢å¼•è¢«æ›¿æ¢ä¸º64ç»´ç¨ å¯†å‘é‡

# ç¬¬3æ­¥ï¼šLSTMå¤„ç†embeddingåºåˆ—
context_vector, encoder_hidden = encoder.lstm(embedded)
# embeddingå‘é‡åºåˆ— â†’ å›ºå®šé•¿åº¦ä¸Šä¸‹æ–‡å‘é‡

# ç¬¬4æ­¥ï¼šè§£ç å™¨embeddingå‚ä¸ç”Ÿæˆ
decoder_embedded = decoder.embedding(decoder_input)
output = decoder.lstm(decoder_embedded, encoder_hidden)
# ç›®æ ‡è¯­è¨€è¯æ±‡çš„embeddingå‚ä¸è§£ç è¿‡ç¨‹
```

## 2. ç¼–ç å™¨ä¸­çš„Embeddingï¼šä»ç¦»æ•£åˆ°è¿ç»­çš„è¯­ä¹‰æ˜ å°„

### 2.1 LSTMEncoderæ ¸å¿ƒå®ç°æ·±åº¦è§£æ

```python
class LSTMEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # ğŸ”‘ å…³é”®ï¼šè¯åµŒå…¥å±‚ - è¯­ä¹‰ç©ºé—´çš„å…¥å£
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        # LSTMå±‚ - åºåˆ—å»ºæ¨¡æ ¸å¿ƒ
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, bidirectional=False)
```

### 2.2 Embeddingåˆå§‹åŒ–çš„æ•°å­¦åŸç†

embeddingå±‚çš„åˆå§‹åŒ–ç›´æ¥å½±å“æ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼š

```python
# PyTorché»˜è®¤åˆå§‹åŒ–ï¼šN(0,1)æ ‡å‡†æ­£æ€åˆ†å¸ƒ
embedding.weight ~ N(0, 1)

# å®é™…é¡¹ç›®ä¸­çš„æ”¹è¿›åˆå§‹åŒ–
def init_embedding_weights(embedding_layer, vocab_size, embed_size):
    """æ”¹è¿›çš„embeddingåˆå§‹åŒ–ç­–ç•¥"""
    # Xavieråˆå§‹åŒ–ï¼šé€‚åˆå¤§å¤šæ•°æ¿€æ´»å‡½æ•°
    std = np.sqrt(2.0 / (vocab_size + embed_size))
    embedding_layer.weight.data.normal_(0, std)
    
    # ç‰¹æ®Šå¤„ç†ï¼špadding tokenä¿æŒé›¶å‘é‡
    if embedding_layer.padding_idx is not None:
        embedding_layer.weight.data[embedding_layer.padding_idx].fill_(0)
```

### 2.3 å‰å‘ä¼ æ’­ä¸­çš„Embeddingå˜æ¢

```python
def forward(self, input_seq, input_lengths):
    # input_seq: [batch_size, seq_len] = [2, 5]
    # è¾“å…¥ç¤ºä¾‹ï¼š[[4, 5, 6, 7, 8], [9, 10, 0, 0, 0]]
    
    batch_size = input_seq.size(0)
    
    # ğŸ”¥ æ ¸å¿ƒå˜æ¢ï¼šç¦»æ•£ç´¢å¼• â†’ è¿ç»­å‘é‡
    embedded = self.embedding(input_seq)  # [2, 5, 64]
    # æ¯ä¸ªè¯è¢«æ›¿æ¢ä¸º64ç»´ç¨ å¯†å‘é‡
    
    print(f"Embeddingå‰: {input_seq.shape} - ç¦»æ•£ç´¢å¼•")
    print(f"Embeddingå: {embedded.shape} - è¿ç»­å‘é‡")
    
    # åºåˆ—æ‰“åŒ…ï¼šå¤„ç†å˜é•¿è¾“å…¥
    packed = nn.utils.rnn.pack_padded_sequence(
        embedded, input_lengths, batch_first=True, enforce_sorted=False)
    
    # LSTMå¤„ç†embeddingåºåˆ—
    packed_output, (hidden, cell) = self.lstm(packed)
    
    # ä¸Šä¸‹æ–‡å‘é‡ï¼šæ•´ä¸ªè¾“å…¥åºåˆ—çš„è¯­ä¹‰æ‘˜è¦
    context_vector = hidden[-1]  # [batch_size, hidden_size]
    
    return context_vector, (hidden, cell)
```

## 3. è§£ç å™¨ä¸­çš„Embeddingï¼šç”Ÿæˆè¿‡ç¨‹çš„è¯­ä¹‰å¼•å¯¼

### 3.1 è‡ªå›å½’ç”Ÿæˆä¸­çš„Embeddingå¾ªç¯

```python
def _generate_sequence(self, encoder_hidden, max_length):
    """è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„embeddingåº”ç”¨"""
    outputs = []
    hidden = encoder_hidden
    # åˆå§‹è¾“å…¥ï¼š
</rewritten_file>

## å®Œæ•´ä»£ç å®ç°

æƒ³è¦æŸ¥çœ‹å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨å®ç°ä»£ç å—ï¼Ÿæˆ‘ä»¬ä¸ºæ‚¨å‡†å¤‡äº†ä¸€ä¸ªäº¤äº’å¼çš„ä»£ç å±•ç¤ºé¡µé¢ï¼š

### ğŸ”— **[æŸ¥çœ‹å®Œæ•´ä»£ç å®ç°](/demos/lstm_encoder_decoder.html)**

è¿™ä¸ªä¸“é—¨è®¾è®¡çš„ä»£ç å±•ç¤ºé¡µé¢åŒ…å«ï¼š

- **å®Œæ•´çš„276è¡ŒPyTorchä»£ç **ï¼šåŒ…å«è¯æ±‡è¡¨æ„å»ºã€LSTMç¼–ç å™¨ã€è§£ç å™¨å’Œè®­ç»ƒæµç¨‹
- **è¯­æ³•é«˜äº®æ˜¾ç¤º**ï¼šä½¿ç”¨ç°ä»£åŒ–çš„ä»£ç ä¸»é¢˜ï¼Œæä¾›æœ€ä½³çš„é˜…è¯»ä½“éªŒ  
- **ä¸€é”®å¤åˆ¶åŠŸèƒ½**ï¼šè½»æ¾å¤åˆ¶ä»£ç åˆ°æ‚¨çš„é¡¹ç›®ä¸­
- **ç›´æ¥ä¸‹è½½é€‰é¡¹**ï¼šå¯ä»¥ç›´æ¥ä¸‹è½½Pythonæ–‡ä»¶åˆ°æœ¬åœ°
- **é¡¹ç›®æ¦‚è¿°ä¿¡æ¯**ï¼šåŒ…å«è¿è¡Œè¦æ±‚ã€å¿«é€Ÿå¼€å§‹æŒ‡å—ç­‰
- **ç« èŠ‚å¯¼èˆª**ï¼šå¿«é€Ÿè·³è½¬åˆ°æ‚¨æ„Ÿå…´è¶£çš„ä»£ç éƒ¨åˆ†

### ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **å­¦ä¹ è·¯å¾„**ï¼šå…ˆé˜…è¯»æœ¬æ–‡çš„ç†è®ºåˆ†æï¼Œå†æŸ¥çœ‹å®Œæ•´ä»£ç å®ç°
2. **å®è·µæ“ä½œ**ï¼šå°†ä»£ç ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå°è¯•è¿è¡Œå¹¶ä¿®æ”¹å‚æ•°
3. **æ·±å…¥ç ”ç©¶**ï¼šåŸºäºè¿™ä¸ªåŸºç¡€å®ç°ï¼Œæ¢ç´¢æ›´é«˜çº§çš„seq2seqå˜ä½“

---

*å¸Œæœ›è¿™ä¸ªå®Œæ•´çš„embeddingç†è®ºä¸LSTMå®è·µç›¸ç»“åˆçš„æŒ‡å—èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼*