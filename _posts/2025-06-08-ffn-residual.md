---
layout: post
title: "前馈神经网络与残差连接"
subtitle: ""
date: 2025-06-08 08:00:00 +0800
background: 
categories: [人工智能, 深度学习]
tags: [Attention, Transformer, LLM, 数学原理, 算法解析]
author: Yonghui Zuo
description: ""
pin: true
math: true
mermaid: true
---

# Day 04 理论详解：前馈神经网络与残差连接

## 1. 前馈神经网络(Feed-Forward Network)

### 1.1 FFN在Transformer中的作用

前馈神经网络是Transformer架构中的关键组件，位于每个编码器和解码器层中。它的主要作用包括：

1. **非线性变换**：为模型引入非线性，增强表达能力
2. **特征提取**：对注意力机制的输出进行进一步的特征提取和变换
3. **维度变换**：通过升维-降维的过程增强特征表示
4. **位置特定处理**：对每个位置独立进行相同的变换

### 1.2 FFN的数学定义

标准的Transformer FFN包含两个线性变换和一个激活函数：

```
FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
```

其中：
- `x` 是输入向量，维度为 `[seq_len, d_model]`
- `W₁` 是第一个权重矩阵，维度为 `[d_model, d_ff]`
- `W₂` 是第二个权重矩阵，维度为 `[d_ff, d_model]`
- `b₁, b₂` 是偏置向量
- `max(0, ·)` 是ReLU激活函数

### 1.3 维度变换的设计原理

FFN采用"扩张-压缩"的维度变换策略：

1. **扩张阶段**：`d_model → d_ff`
   - 通常 `d_ff = 4 × d_model`
   - 增加表示空间的维度，提供更丰富的特征表示

2. **压缩阶段**：`d_ff → d_model`
   - 将高维特征压缩回原始维度
   - 保持与其他组件的维度一致性

### 1.4 激活函数的选择

#### ReLU激活函数
```
ReLU(x) = max(0, x)
```
- **优点**：计算简单，梯度不饱和
- **缺点**：存在"死神经元"问题

#### GELU激活函数
```
GELU(x) = x · Φ(x) = x · P(X ≤ x), X ~ N(0,1)
```
近似公式：
```
GELU(x) ≈ 0.5x(1 + tanh(√(2/π)(x + 0.044715x³)))
```
- **优点**：平滑的非线性，更好的梯度特性
- **应用**：在BERT、GPT等现代模型中广泛使用

## 2. 残差连接(Residual Connection)

### 2.1 残差连接的动机

深度神经网络训练中面临的主要问题：

1. **梯度消失**：随着网络深度增加，梯度在反向传播过程中逐渐消失
2. **梯度爆炸**：梯度可能变得过大，导致训练不稳定
3. **退化问题**：更深的网络可能比浅层网络表现更差

### 2.2 残差连接的数学原理

残差连接的核心思想是学习残差函数而不是直接学习目标函数：

```
y = F(x) + x
```

其中：
- `x` 是输入
- `F(x)` 是要学习的残差函数
- `y` 是输出

### 2.3 残差连接的优势

#### 梯度流动改善
在反向传播过程中：
```
∂y/∂x = ∂F(x)/∂x + 1
```

由于恒等映射的存在，梯度至少为1，有效缓解了梯度消失问题。

#### 恒等映射的保证
如果残差函数 `F(x) = 0`，则 `y = x`，网络至少能学到恒等映射，不会比浅层网络更差。

#### 特征重用
残差连接允许网络重用之前层的特征，提高了特征的利用效率。

### 2.4 在Transformer中的应用

Transformer中的残差连接应用于：
1. 多头注意力层：`x + MultiHeadAttention(x)`
2. 前馈神经网络层：`x + FFN(x)`

## 3. Layer Normalization

### 3.1 归一化的必要性

神经网络训练中的问题：
1. **内部协变量偏移**：层间输入分布的变化
2. **梯度问题**：不稳定的梯度导致训练困难
3. **收敛速度**：归一化可以加速收敛

### 3.2 Layer Normalization vs Batch Normalization

#### Batch Normalization
- 在batch维度上进行归一化
- 公式：`BN(x) = γ(x - μ_B)/σ_B + β`
- 问题：依赖batch大小，在序列任务中效果不佳

#### Layer Normalization
- 在特征维度上进行归一化
- 公式：`LN(x) = γ(x - μ_L)/σ_L + β`
- 优势：不依赖batch大小，适合序列任务

### 3.3 Layer Normalization的数学定义

对于输入向量 `x ∈ R^d`：

```
μ = (1/d) Σᵢ xᵢ                    # 均值
σ² = (1/d) Σᵢ (xᵢ - μ)²           # 方差
LN(x) = γ ⊙ (x - μ)/√(σ² + ε) + β  # 归一化
```

其中：
- `γ, β` 是可学习的缩放和偏移参数
- `ε` 是防止除零的小常数
- `⊙` 表示逐元素乘法

### 3.4 Pre-LN vs Post-LN

#### Post-LN (原始Transformer)
```
x = x + Sublayer(LayerNorm(x))
```

#### Pre-LN (现代变体)
```
x = x + Sublayer(LayerNorm(x))
```

**Pre-LN的优势**：
- 更稳定的训练
- 更好的梯度流动
- 减少梯度爆炸的风险

## 4. 子层结构组合

### 4.1 Add & Norm结构

Transformer中的标准子层结构：
```
output = LayerNorm(x + Sublayer(x))
```

这个结构结合了：
1. **残差连接**：`x + Sublayer(x)`
2. **层归一化**：`LayerNorm(...)`

### 4.2 完整的Transformer层

一个完整的Transformer编码器层包含：

1. **多头自注意力子层**：
   ```
   x₁ = LayerNorm(x + MultiHeadAttention(x, x, x))
   ```

2. **前馈神经网络子层**：
   ```
   x₂ = LayerNorm(x₁ + FFN(x₁))
   ```

### 4.3 信息流分析

在Transformer层中，信息流动路径：

1. **直接路径**：通过残差连接直接传递
2. **变换路径**：通过注意力机制和FFN进行变换
3. **归一化**：在每个子层后进行归一化

这种设计确保了：
- 信息的有效传递
- 梯度的稳定流动
- 特征的逐步精炼

## 5. 设计原理总结

### 5.1 协同工作机制

1. **注意力机制**：捕获序列中的依赖关系
2. **FFN**：对每个位置进行非线性变换
3. **残差连接**：保证信息流动和梯度传播
4. **Layer Normalization**：稳定训练过程

### 5.2 为什么这样设计有效

1. **模块化设计**：每个组件职责明确，易于理解和优化
2. **信息保持**：残差连接确保重要信息不会丢失
3. **非线性增强**：FFN提供必要的非线性变换能力
4. **训练稳定性**：归一化和残差连接共同保证训练稳定

### 5.3 现代改进

1. **Pre-LN**：将Layer Norm移到子层之前
2. **不同激活函数**：使用GELU、Swish等替代ReLU
3. **GLU变体**：使用门控线性单元改进FFN
4. **权重初始化**：更好的初始化策略

这些理论基础为我们实现高质量的Transformer组件提供了坚实的基础。在接下来的实现中，我们将把这些理论转化为可运行的代码。 
