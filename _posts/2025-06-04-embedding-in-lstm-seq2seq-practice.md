---
layout: post
title: "ä»ç†è®ºåˆ°å®è·µï¼šæ·±åº¦è§£æEmbeddingåœ¨LSTM Seq2Seqæ¨¡å‹ä¸­çš„åº”ç”¨"
subtitle: "æ·±å…¥ç†è§£åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„è¯åµŒå…¥æœºåˆ¶"
date: 2025-06-04 12:00:00 +0800
background: '/img/posts/06.jpg'
categories: [äººå·¥æ™ºèƒ½, æ·±åº¦å­¦ä¹ ]
tags: [embedding, lstm, seq2seq, æœºå™¨ç¿»è¯‘, pytorch, nlp]
author: Yonghui Zuo
description: "é€šè¿‡å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨ä»£ç å®ç°ï¼Œæ·±åº¦å‰–æembeddingåœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨æœºåˆ¶ã€è®­ç»ƒè¿‡ç¨‹å’Œå®è·µæŠ€å·§"
pin: false
math: true
mermaid: true
image:
  path: https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png
  alt: "LSTMå•å…ƒç»“æ„ç¤ºæ„å›¾ - å±•ç¤ºäº†LSTMçš„æ ¸å¿ƒç»„ä»¶å’Œä¿¡æ¯æµ"
---

# Embeddingæ·±åº¦è§£æï¼šä»"æŸ¥æ‰¾è¡¨"åˆ°æ·±åº¦å­¦ä¹ çš„è¯­ä¹‰æ¡¥æ¢

embeddingï¼ˆåµŒå…¥ï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ã€‚ä»Word2Vecåˆ°BERTï¼Œä»æ¨èç³»ç»Ÿåˆ°å›¾ç¥ç»ç½‘ç»œï¼Œä»ä¼ ç»ŸNLPåˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œembeddingæ— å¤„ä¸åœ¨ã€‚ç„¶è€Œï¼Œå¾ˆå¤šäººå¯¹å…¶æœ¬è´¨ç†è§£å¹¶ä¸æ·±åˆ»ã€‚æœ¬æ–‡å°†é€šè¿‡å…·ä½“çš„LSTM Seq2Seqä»£ç ç¤ºä¾‹ï¼Œæ·±åº¦å‰–æembeddingçš„æœ¬è´¨ã€å®ç°æœºåˆ¶ã€æ•°å­¦åŸç†å’Œå·¥ç¨‹å®è·µï¼Œå¹¶ç»“åˆç°ä»£å¤§æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œå¸®åŠ©ä½ å»ºç«‹ç³»ç»Ÿæ€§çš„è®¤çŸ¥ã€‚

## å®Œæ•´ä»£ç å®ç°

ä»¥ä¸‹æ˜¯æœ¬æ–‡å°†è¦æ·±åº¦åˆ†æçš„å®Œæ•´LSTMç¼–ç å™¨-è§£ç å™¨å®ç°ä»£ç ï¼ˆ276è¡Œï¼‰ï¼Œæ¶µç›–è¯æ±‡è¡¨æ„å»ºã€ç¼–ç å™¨ã€è§£ç å™¨ã€è®­ç»ƒæµç¨‹å’Œæµ‹è¯•ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
# æ ¹æ®CSDNåšå®¢ https://blog.csdn.net/weixin_46474921/article/details/123783987 çš„è§£å†³æ–¹æ¡ˆ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOSç³»ç»Ÿæ¨èå­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¸­æ–‡å­—ä½“ä¸‹åæ ‡è½´è´Ÿæ•°çš„è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

class Vocabulary:
    """è¯æ±‡è¡¨ç±»ï¼Œç”¨äºæ–‡æœ¬å’Œæ•°å­—ä¹‹é—´çš„è½¬æ¢"""
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.vocab_size = 4
    
    def add_word(self, word):
        if word not in self.word2idx:
            self.word2idx[word] = self.vocab_size
            self.idx2word[self.vocab_size] = word
            self.vocab_size += 1
    
    def add_sentence(self, sentence):
        for word in sentence.split():
            self.add_word(word)
    
    def sentence_to_indices(self, sentence):
        return [self.word2idx.get(word, self.word2idx['<UNK>']) 
                for word in sentence.split()]
    
    def indices_to_sentence(self, indices):
        return ' '.join([self.idx2word[idx] for idx in indices 
                        if idx not in [0, 1, 2]])  # æ’é™¤ç‰¹æ®Šæ ‡è®°

class LSTMEncoder(nn.Module):
    """LSTMç¼–ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        # LSTMå±‚
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, bidirectional=False)
        
    def forward(self, input_seq, input_lengths):
        # input_seq: [batch_size, seq_len]
        batch_size = input_seq.size(0)
        
        # è¯åµŒå…¥
        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]
        
        # æ‰“åŒ…åºåˆ—ä»¥å¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, input_lengths, batch_first=True, enforce_sorted=False)
        
        # LSTMå‰å‘ä¼ æ’­
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # è§£åŒ…åºåˆ—
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        # è¿”å›æœ€åçš„éšçŠ¶æ€ä½œä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤º
        # hidden: [num_layers, batch_size, hidden_size]
        # æˆ‘ä»¬å–æœ€åä¸€å±‚çš„éšçŠ¶æ€
        context_vector = hidden[-1]  # [batch_size, hidden_size]
        
        return context_vector, (hidden, cell)

class LSTMDecoder(nn.Module):
    """LSTMè§£ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMDecoder, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, encoder_hidden, target_seq=None, max_length=20):
        batch_size = encoder_hidden[0].size(1)
        
        if target_seq is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ç›®æ ‡åºåˆ—
            embedded = self.embedding(target_seq)
            output, _ = self.lstm(embedded, encoder_hidden)
            output = self.output_projection(output)
            return output
        else:
            # æ¨ç†æ¨¡å¼ï¼šé€æ­¥ç”Ÿæˆ
            outputs = []
            hidden = encoder_hidden
            input_token = torch.ones(batch_size, 1, dtype=torch.long) * 1  # <SOS>
            
            for _ in range(max_length):
                embedded = self.embedding(input_token)
                output, hidden = self.lstm(embedded, hidden)
                output = self.output_projection(output)
                
                # å–æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
                input_token = output.argmax(dim=-1)
                outputs.append(output)
                
                # å¦‚æœç”Ÿæˆäº†<EOS>ï¼Œæå‰åœæ­¢
                if (input_token == 2).all():
                    break
            
            return torch.cat(outputs, dim=1)

class Seq2SeqModel(nn.Module):
    """åºåˆ—åˆ°åºåˆ—æ¨¡å‹"""
    def __init__(self, encoder, decoder):
        super(Seq2SeqModel, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, input_seq, input_lengths, target_seq=None, max_length=20):
        # ç¼–ç é˜¶æ®µï¼šå°†è¾“å…¥åºåˆ—å‹ç¼©æˆå›ºå®šé•¿åº¦å‘é‡
        context_vector, encoder_hidden = self.encoder(input_seq, input_lengths)
        
        print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")
        print(f"ä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context_vector.shape}")
        print(f"ç¼–ç å™¨éšçŠ¶æ€å½¢çŠ¶: {encoder_hidden[0].shape}")
        
        # è§£ç é˜¶æ®µï¼šåŸºäºä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—
        output = self.decoder(encoder_hidden, target_seq, max_length)
        
        return output, context_vector

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    # ä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ç¤ºä¾‹
    data_pairs = [
        ("æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "I love natural language processing"),
        ("ä»Šå¤© å¤©æ°” å¾ˆ å¥½", "Today weather is good"),
        ("æœºå™¨ å­¦ä¹  å¾ˆ æœ‰è¶£", "Machine learning is interesting"),
        ("æ·±åº¦ å­¦ä¹  å¾ˆ å¼ºå¤§", "Deep learning is powerful"),
        ("äººå·¥ æ™ºèƒ½ æ”¹å˜ ä¸–ç•Œ", "AI changes the world"),
    ]
    return data_pairs

class TranslationDataset(Dataset):
    """ç¿»è¯‘æ•°æ®é›†"""
    def __init__(self, data_pairs, src_vocab, tgt_vocab):
        self.data_pairs = data_pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        src_sentence, tgt_sentence = self.data_pairs[idx]
        
        src_indices = self.src_vocab.sentence_to_indices(src_sentence)
        tgt_indices = [1] + self.tgt_vocab.sentence_to_indices(tgt_sentence) + [2]  # æ·»åŠ <SOS>å’Œ<EOS>
        
        return torch.tensor(src_indices), torch.tensor(tgt_indices)

def collate_fn(batch):
    """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
    src_batch, tgt_batch = zip(*batch)
    
    # è®¡ç®—åºåˆ—é•¿åº¦
    src_lengths = [len(seq) for seq in src_batch]
    tgt_lengths = [len(seq) for seq in tgt_batch]
    
    # å¡«å……åºåˆ—
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    
    return src_batch, torch.tensor(src_lengths), tgt_batch, torch.tensor(tgt_lengths)

def main():
    # 1. å‡†å¤‡æ•°æ®
    print("=" * 50)
    print("å‡†å¤‡æ•°æ®...")
    data_pairs = create_sample_data()
    
    # æ„å»ºè¯æ±‡è¡¨
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    
    for src, tgt in data_pairs:
        src_vocab.add_sentence(src)
        tgt_vocab.add_sentence(tgt)
    
    print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {src_vocab.vocab_size}")
    print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {tgt_vocab.vocab_size}")
    
    # 2. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = TranslationDataset(data_pairs, src_vocab, tgt_vocab)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    
    # 3. å®šä¹‰æ¨¡å‹å‚æ•°
    print("\n" + "=" * 50)
    print("åˆå§‹åŒ–æ¨¡å‹...")
    embed_size = 64
    hidden_size = 128
    num_layers = 1
    
    # 4. åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
    encoder = LSTMEncoder(src_vocab.vocab_size, embed_size, hidden_size, num_layers)
    decoder = LSTMDecoder(tgt_vocab.vocab_size, embed_size, hidden_size, num_layers)
    model = Seq2SeqModel(encoder, decoder)
    
    # 5. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 6. è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    for epoch in range(50):
        total_loss = 0
        for batch_idx, (src_batch, src_lengths, tgt_batch, tgt_lengths) in enumerate(dataloader):
            optimizer.zero_grad()
            
            # å‡†å¤‡ç›®æ ‡åºåˆ—ï¼ˆç”¨äºteacher forcingï¼‰
            decoder_input = tgt_batch[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtoken
            decoder_target = tgt_batch[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªtoken(<SOS>)
            
            # å‰å‘ä¼ æ’­
            output, context_vector = model(src_batch, src_lengths, decoder_input)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output.reshape(-1, output.size(-1)), 
                           decoder_target.reshape(-1))
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/50], Loss: {total_loss/len(dataloader):.4f}')
    
    # 7. æµ‹è¯•æ¨¡å‹
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ¨¡å‹...")
    model.eval()
    
    test_sentences = ["æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "ä»Šå¤© å¤©æ°” å¾ˆ å¥½"]
    
    with torch.no_grad():
        for test_sentence in test_sentences:
            print(f"\nè¾“å…¥: {test_sentence}")
            
            # å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•
            src_indices = src_vocab.sentence_to_indices(test_sentence)
            src_tensor = torch.tensor(src_indices).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            src_length = torch.tensor([len(src_indices)])
            
            # ç”Ÿæˆç¿»è¯‘
            output, context_vector = model(src_tensor, src_length, max_length=10)
            
            # å°†è¾“å‡ºè½¬æ¢ä¸ºå•è¯
            predicted_indices = output.argmax(dim=-1).squeeze(0).tolist()
            predicted_sentence = tgt_vocab.indices_to_sentence(predicted_indices)
            
            print(f"è¾“å‡º: {predicted_sentence}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦: {context_vector.shape}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡å€¼: {context_vector.squeeze().numpy()[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼

    # 8. Embeddingè´¨é‡åˆ†æä¸å¯è§†åŒ–
    print("\n" + "=" * 50)
    print("Embeddingè´¨é‡åˆ†æä¸å¯è§†åŒ–...")
    
    # ä»æ¨¡å‹ä¸­è·å–æºè¯­è¨€çš„embeddingçŸ©é˜µå’Œè¯æ±‡è¡¨
    src_embedding_matrix = model.encoder.embedding.weight.data.cpu()
    
    # å…ˆè¿›è¡Œè´¨é‡åˆ†æ
    analysis_results = analyze_embedding_quality(src_embedding_matrix, src_vocab, top_k=8)
    
    # å†è¿›è¡Œå¯è§†åŒ–
    print(f"\n{'='*60}")
    print("ğŸ¨ ç”Ÿæˆt-SNEå¯è§†åŒ–å›¾...")
    print(f"{'='*60}")
    visualize_embeddings(src_embedding_matrix, src_vocab, method='tsne', title="Source Language Embedding Visualization (t-SNE)")

def analyze_embedding_quality(embedding_matrix, vocab, top_k=5):
    """åˆ†æembeddingè´¨é‡å’Œèšç±»æ•ˆæœ"""
    print(f"\n{'='*60}")
    print("ğŸ“Š Embeddingè´¨é‡åˆ†æ")
    print(f"{'='*60}")
    
    # è®¡ç®—æ‰€æœ‰è¯æ±‡çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    
    # è¿‡æ»¤æ‰ç‰¹æ®Šæ ‡è®°ï¼Œåªåˆ†æå®é™…è¯æ±‡
    real_words = []
    real_indices = []
    real_embeddings = []
    
    for idx, word in vocab.idx2word.items():
        if idx < embedding_matrix.shape[0] and word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:
            real_words.append(word)
            real_indices.append(idx)
            real_embeddings.append(embedding_matrix[idx].numpy())
    
    if len(real_embeddings) < 2:
        print("âš ï¸  å®é™…è¯æ±‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
        return
    
    real_embeddings = np.array(real_embeddings)
    similarity_matrix = cosine_similarity(real_embeddings)
    
    print(f"\nğŸ” è¯æ±‡ç›¸ä¼¼åº¦åˆ†æ (å…±{len(real_words)}ä¸ªè¯æ±‡)")
    print("-" * 50)
    
    # 1. æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è¯å¯¹
    most_similar_pairs = []
    for i in range(len(real_words)):
        for j in range(i+1, len(real_words)):
            similarity = similarity_matrix[i][j]
            most_similar_pairs.append((real_words[i], real_words[j], similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    most_similar_pairs.sort(key=lambda x: x[2], reverse=True)
    
    print(f"\nğŸ“ˆ æœ€ç›¸ä¼¼çš„{min(top_k, len(most_similar_pairs))}å¯¹è¯æ±‡:")
    for i, (word1, word2, sim) in enumerate(most_similar_pairs[:top_k]):
        print(f"   {i+1}. '{word1}' â†” '{word2}': {sim:.4f}")
    
    # 2. åˆ†æç‰¹å®šä¸»é¢˜è¯æ±‡çš„èšé›†åº¦
    print(f"\nğŸ¯ ä¸»é¢˜è¯æ±‡èšé›†åˆ†æ:")
    print("-" * 30)
    
    # å®šä¹‰ä¸»é¢˜è¯æ±‡ç»„
    theme_groups = {
        "æ—¶é—´å¤©æ°”": ["ä»Šå¤©", "å¤©æ°”", "å¥½"],
        "AIæŠ€æœ¯": ["äººå·¥", "æ™ºèƒ½", "æœºå™¨", "å­¦ä¹ "],
        "NLP": ["è‡ªç„¶", "è¯­è¨€", "å¤„ç†"],
        "æƒ…æ„Ÿ": ["çˆ±", "æœ‰è¶£", "å¼ºå¤§"]
    }
    
    for theme_name, words in theme_groups.items():
        # æ‰¾å‡ºè¯¥ä¸»é¢˜ä¸­å­˜åœ¨çš„è¯æ±‡
        existing_words = [w for w in words if w in real_words]
        if len(existing_words) >= 2:
            # è®¡ç®—ç»„å†…å¹³å‡ç›¸ä¼¼åº¦
            indices = [real_words.index(w) for w in existing_words]
            group_similarities = []
            for i in range(len(indices)):
                for j in range(i+1, len(indices)):
                    group_similarities.append(similarity_matrix[indices[i]][indices[j]])
            
            avg_similarity = np.mean(group_similarities)
            print(f"   {theme_name}: {existing_words} â†’ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    # 3. æ£€æŸ¥å…±ç°è¯æ±‡çš„ç›¸ä¼¼åº¦
    print(f"\nğŸ”— è®­ç»ƒæ•°æ®å…±ç°è¯æ±‡ç›¸ä¼¼åº¦:")
    print("-" * 35)
    
    cooccurrence_pairs = [
        ("æˆ‘", "çˆ±"), ("ä»Šå¤©", "å¤©æ°”"), ("å¤©æ°”", "å¥½"),
        ("æœºå™¨", "å­¦ä¹ "), ("æ·±åº¦", "å­¦ä¹ "), ("äººå·¥", "æ™ºèƒ½")
    ]
    
    for word1, word2 in cooccurrence_pairs:
        if word1 in real_words and word2 in real_words:
            idx1, idx2 = real_words.index(word1), real_words.index(word2)
            similarity = similarity_matrix[idx1][idx2]
            print(f"   '{word1}' â†” '{word2}': {similarity:.4f}")
    
    # 4. ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“Š Embeddingç»Ÿè®¡ç‰¹æ€§:")
    print("-" * 25)
    
    # å‘é‡èŒƒæ•°åˆ†æ
    norms = np.linalg.norm(real_embeddings, axis=1)
    print(f"   å‘é‡èŒƒæ•° - å‡å€¼: {np.mean(norms):.4f}, æ ‡å‡†å·®: {np.std(norms):.4f}")
    
    # æ•´ä½“ç›¸ä¼¼åº¦åˆ†å¸ƒ
    upper_triangle = similarity_matrix[np.triu_indices(len(real_words), k=1)]
    print(f"   ç›¸ä¼¼åº¦åˆ†å¸ƒ - å‡å€¼: {np.mean(upper_triangle):.4f}, æ ‡å‡†å·®: {np.std(upper_triangle):.4f}")
    print(f"   ç›¸ä¼¼åº¦èŒƒå›´: [{np.min(upper_triangle):.4f}, {np.max(upper_triangle):.4f}]")
    
    # 5. å¼‚å¸¸æ£€æµ‹
    print(f"\nâš ï¸  å¼‚å¸¸å‘é‡æ£€æµ‹:")
    print("-" * 20)
    
    mean_norm = np.mean(norms)
    std_norm = np.std(norms)
    outlier_threshold = 2.0  # 2å€æ ‡å‡†å·®
    
    outliers = []
    for i, (word, norm) in enumerate(zip(real_words, norms)):
        if abs(norm - mean_norm) > outlier_threshold * std_norm:
            outliers.append((word, norm))
    
    if outliers:
        print(f"   å‘ç°{len(outliers)}ä¸ªå¼‚å¸¸å‘é‡:")
        for word, norm in outliers:
            print(f"     '{word}': èŒƒæ•° = {norm:.4f}")
    else:
        print("   âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸å‘é‡")

    return {
        'similarity_matrix': similarity_matrix,
        'most_similar_pairs': most_similar_pairs[:top_k],
        'real_words': real_words,
        'statistics': {
            'mean_norm': np.mean(norms),
            'mean_similarity': np.mean(upper_triangle),
            'std_similarity': np.std(upper_triangle)
        }
    }

def visualize_embeddings(embedding_matrix, vocab, method='tsne', title='Embedding Visualization', num_words_to_annotate=20):
    """ä½¿ç”¨t-SNEæˆ–PCAå¯è§†åŒ–embedding"""
    # ç¡®ä¿è¯æ±‡è¡¨ä¸­çš„è¯å°‘äºæˆ–ç­‰äºå®é™…embeddingçŸ©é˜µä¸­çš„è¡Œæ•°
    # é€šå¸¸ï¼Œvocab.vocab_size ä¼šæ˜¯ embedding_matrix.shape[0]
    # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬å–ä¸¤è€…ä¸­è¾ƒå°çš„å€¼ï¼Œå¹¶æ’é™¤ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æœå®ƒä»¬å½±å“å¯è§†åŒ–ï¼‰
    
    # è·å–æ‰€æœ‰è¯ï¼ˆæ’é™¤ç‰¹æ®Šæ ‡è®°å¦‚<PAD>, <SOS>, <EOS>, <UNK>ï¼Œå¦‚æœå®ƒä»¬åœ¨è¯æ±‡è¡¨ç´¢å¼•çš„å¼€å¤´ï¼‰
    # æˆ‘ä»¬å‡è®¾ç‰¹æ®Šæ ‡è®°çš„ç´¢å¼•è¾ƒå°ï¼Œå¦‚æœ visualize_embeddings åªå…³æ³¨éç‰¹æ®Šè¯æ±‡
    
    # è¿‡æ»¤æ‰æƒé‡å…¨ä¸ºé›¶çš„å‘é‡ (é€šå¸¸æ˜¯ padding_idx)
    # åŒæ—¶æ”¶é›†æœ‰æ•ˆçš„è¯å’Œå®ƒä»¬çš„ç´¢å¼•
    valid_indices = []
    valid_words = []
    
    # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥ä¿å­˜æ‰€æœ‰è¯çš„å‘é‡
    all_vectors_list = []
    
    # è·å–æ‰€æœ‰è¯çš„åˆ—è¡¨ï¼ŒæŒ‰ç´¢å¼•é¡ºåº
    # vocab.idx2word æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦æŒ‰ç´¢å¼•æ’åºçš„è¯
    # æˆ‘ä»¬åªå–è¯æ±‡è¡¨ä¸­å®é™…å­˜åœ¨çš„è¯æ±‡ï¼Œç›´åˆ° embedding_matrix.shape[0]
    # é€šå¸¸ vocab.vocab_size åº”è¯¥ç­‰äº embedding_matrix.shape[0]
    
    words_to_process_indices = sorted([idx for idx in vocab.idx2word.keys() if idx < embedding_matrix.shape[0]])

    for idx in words_to_process_indices:
        word = vocab.idx2word[idx]
        vector = embedding_matrix[idx]
        # æ’é™¤<PAD>ç­‰ç‰¹æ®Štokençš„å¯è§†åŒ–, é€šå¸¸ padding_idx ä¸º 0
        if word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] or torch.any(vector != 0):
             # åªæœ‰å½“è¯ä¸æ˜¯ç‰¹æ®Šè¯ï¼Œæˆ–è€…å‘é‡ä¸å…¨ä¸º0æ—¶æ‰æ·»åŠ 
            # å®é™…ä¸Šï¼Œå¯¹äºéç‰¹æ®Šè¯ï¼Œå‘é‡ä¸åº”å…¨ä¸º0ï¼Œé™¤éembed_sizeå¾ˆå°æˆ–ç‰¹æ®Šæƒ…å†µ
            # è€Œå¯¹äº<PAD>ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ’é™¤å®ƒï¼Œé™¤éç‰¹åˆ«æƒ³è§‚å¯Ÿå®ƒ
            if not (word == '<PAD>' and torch.all(vector == 0)):
                 all_vectors_list.append(vector.numpy()) # TSNE/PCAéœ€è¦numpyæ•°ç»„
                 valid_words.append(word)
                 valid_indices.append(idx) # è™½ç„¶æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥å¤‡å°†æ¥ä¹‹éœ€

    if not all_vectors_list:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è¯å‘é‡å¯ä¾›å¯è§†åŒ–ã€‚")
        return

    embeddings_to_visualize = np.array(all_vectors_list)

    if embeddings_to_visualize.shape[0] < 2:
        print(f"æœ‰æ•ˆçš„è¯å‘é‡æ•°é‡ ({embeddings_to_visualize.shape[0]}) ä¸è¶³ä»¥è¿›è¡Œé™ç»´å¯è§†åŒ–ã€‚")
        return

    # é™ç»´åˆ°2D
    # t-SNEå¯¹äºå°‘äº perplexity+1 ä¸ªæ ·æœ¬ä¼šå‡ºé—®é¢˜ï¼Œé€šå¸¸perplexityåœ¨5-50ä¹‹é—´
    # PCAæ²¡æœ‰è¿™ä¸ªé™åˆ¶
    n_samples = embeddings_to_visualize.shape[0]
    
    if method == 'tsne':
        # å¯¹äºéå¸¸å°çš„æ ·æœ¬é‡ï¼ŒTSNEå¯èƒ½å¤±è´¥æˆ–äº§ç”Ÿæ— æ„ä¹‰çš„ç»“æœ
        # è°ƒæ•´ perplexity, n_iter, learning_rate
        perplexity_value = min(30.0, float(n_samples - 1)) # Perplexity must be less than n_samples
        if perplexity_value <= 0: # å¦‚æœåªæœ‰ä¸€ä¸ªç‚¹æˆ–æ²¡æœ‰ç‚¹
             print(f"æ ·æœ¬æ•°é‡ ({n_samples}) è¿‡å°‘ï¼Œæ— æ³•ä½¿ç”¨t-SNEã€‚")
             if n_samples > 1 and embeddings_to_visualize.ndim == 2 and embeddings_to_visualize.shape[1] >=2:
                 print("å°è¯•ä½¿ç”¨PCAæ›¿ä»£...")
                 method = 'pca' # å°è¯•PCA
             else:
                 return # ç¡®å®æ— æ³•å¯è§†åŒ–
        
        if method == 'tsne': # å†æ¬¡æ£€æŸ¥ï¼Œå› ä¸ºå¯èƒ½åœ¨ä¸Šé¢è¢«æ”¹ä¸ºpca
            try:
                reducer = TSNE(n_components=2, random_state=42, perplexity=perplexity_value, 
                               max_iter=300, learning_rate=200) # ä¿®å¤sklearnå‚æ•°ï¼šn_iteræ”¹ä¸ºmax_iter
                embeddings_2d = reducer.fit_transform(embeddings_to_visualize)
            except Exception as e:
                print(f"t-SNEæ‰§è¡Œå¤±è´¥: {e}ã€‚å°è¯•ä½¿ç”¨PCAã€‚")
                if n_samples > 1 and embeddings_to_visualize.ndim == 2 and embeddings_to_visualize.shape[1] >=2: # ç¡®ä¿PCAå¯ä»¥è¿è¡Œ
                    method = 'pca'
                else:
                    return


    if method == 'pca': # å¦‚æœåŸå§‹æ–¹æ³•æ˜¯PCAï¼Œæˆ–è€…t-SNEå¤±è´¥åè½¬ä¸ºPCA
        if n_samples < 2 or embeddings_to_visualize.shape[1] < 2: # PCAè‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬å’Œ2ä¸ªç‰¹å¾
            print("æ ·æœ¬æˆ–ç‰¹å¾æ•°é‡ä¸è¶³ä»¥è¿›è¡ŒPCAå¯è§†åŒ–ã€‚")
            return
        try:
            reducer = PCA(n_components=2)
            embeddings_2d = reducer.fit_transform(embeddings_to_visualize)
        except Exception as e:
            print(f"PCAæ‰§è¡Œå¤±è´¥: {e}")
            return


    if embeddings_2d is None or embeddings_2d.shape[0] == 0:
        print("é™ç»´å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆ2DåµŒå…¥ã€‚")
        return

    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(12, 10)) # å¢å¤§å›¾åƒå°ºå¯¸ä»¥å®¹çº³æ›´å¤šæ ‡ç­¾
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=30) # å¢å¤§ç‚¹çš„å¤§å°
    
    # æ·»åŠ è¯æ±‡æ ‡ç­¾ (åªæ³¨é‡Šä¸€éƒ¨åˆ†è¯ä»¥é¿å…æ‹¥æŒ¤)
    words_to_annotate_actual = min(num_words_to_annotate, len(valid_words))
    
    # ä¸ºäº†æ›´å¥½çš„å¯è¯»æ€§ï¼Œé€‰æ‹©ä¸€äº›è¯è¿›è¡Œæ ‡æ³¨ï¼Œä¾‹å¦‚å‡åŒ€é—´éš”çš„æˆ–è€…éšæœºé€‰æ‹©çš„
    # è¿™é‡Œæˆ‘ä»¬ç®€å•é€‰æ‹©å‰ num_words_to_annotate_actual ä¸ªè¯
    indices_to_annotate = np.random.choice(len(valid_words), size=words_to_annotate_actual, replace=False)


    for i in indices_to_annotate:
        plt.annotate(valid_words[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)
    
    plt.title(title, fontsize=14)
    plt.xlabel('Dimension 1', fontsize=12)
    plt.ylabel('Dimension 2', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout() # è°ƒæ•´å¸ƒå±€ä»¥é˜²æ­¢æ ‡ç­¾æº¢å‡º
    # ä¿å­˜å›¾åƒè€Œä¸æ˜¯æ˜¾ç¤ºï¼Œä»¥ä¾¿åœ¨æ— GUIç¯å¢ƒè¿è¡Œ
    try:
        plt.savefig("embedding_visualization.png")
        print(f"Embeddingå¯è§†åŒ–å›¾åƒå·²ä¿å­˜åˆ° embedding_visualization.png")
    except Exception as e:
        print(f"ä¿å­˜å›¾åƒå¤±è´¥: {e}")
    # plt.show() # åœ¨è„šæœ¬ä¸­é€šå¸¸ä¸ç›´æ¥è°ƒç”¨show()ï¼Œé™¤éæ˜¯äº¤äº’å¼è¿è¡Œ

if __name__ == "__main__":
    main()
```

## 1. æ ¸å¿ƒæ¦‚å¿µæ·±åº¦è§£æ

### 1.1 embeddingçš„æœ¬è´¨ï¼šå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨

æƒ³è±¡ä¸€æœ¬ç‰¹æ®Šçš„å­—å…¸ï¼šä¼ ç»Ÿå­—å…¸ç»™å‡ºè¯æ±‡çš„æ–‡å­—è§£é‡Šï¼Œè€Œembeddingå­—å…¸ç»™å‡ºçš„æ˜¯æ•°å­—å‘é‡ã€‚æ›´ç¥å¥‡çš„æ˜¯ï¼Œè¿™æœ¬å­—å…¸ä¼š"è‡ªå­¦ä¹ "â€”â€”é€šè¿‡ä¸æ–­è®­ç»ƒï¼Œè®©è¯­ä¹‰ç›¸ä¼¼çš„è¯æ±‡æ‹¥æœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚

è®©æˆ‘ä»¬çœ‹çœ‹LSTMä»£ç ä¸­çš„å…·ä½“å®ç°ï¼š

```python
# æ¥è‡ªencoder_decoder_lstm.pyç¬¬44è¡Œ
self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
```

è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸º`[vocab_size, embed_size]`çš„æŸ¥æ‰¾è¡¨ã€‚æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºï¼š

```python
# å‡è®¾vocab_size=1000, embed_size=64
# embedding.weightçš„å½¢çŠ¶æ˜¯[1000, 64]
# è¯ç´¢å¼•5çš„å‘é‡å°±æ˜¯embedding.weight[5]ï¼Œæ˜¯ä¸€ä¸ª64ç»´å‘é‡
```

### 1.2 æŸ¥æ‰¾è¡¨vsä¼ ç»Ÿç¼–ç 

ä¼ ç»Ÿçš„one-hotç¼–ç åƒæ˜¯"èº«ä»½è¯å·ç "â€”â€”æ¯ä¸ªè¯æœ‰å”¯ä¸€æ ‡è¯†ï¼Œä½†å½¼æ­¤æ²¡æœ‰å…³ç³»ï¼š

```python
# one-hotç¼–ç ç¤ºä¾‹ï¼ˆå‡è®¾è¯è¡¨å¤§å°ä¸º5ï¼‰
"æˆ‘"   â†’ [1, 0, 0, 0, 0]
"çˆ±"   â†’ [0, 1, 0, 0, 0]  
"è‡ªç„¶" â†’ [0, 0, 1, 0, 0]
# ä»»æ„ä¸¤ä¸ªè¯çš„è·ç¦»éƒ½ç›¸ç­‰ï¼Œæ— æ³•è¡¨è¾¾è¯­ä¹‰å…³ç³»
```

è€Œembeddingæ˜¯"è¯­ä¹‰åæ ‡"â€”â€”æŠŠè¯æ”¾åœ¨ä¸€ä¸ªè¿ç»­ç©ºé—´ä¸­ï¼Œç›¸ä¼¼çš„è¯è·ç¦»æ›´è¿‘ï¼š

```python
# embeddingç¤ºä¾‹ï¼ˆå‡è®¾64ç»´ï¼‰
"æˆ‘"   â†’ [0.2, -0.1, 0.5, ..., 0.3]  # 64ç»´å‘é‡
"ä½ "   â†’ [0.3, -0.2, 0.4, ..., 0.2]  # ä¸"æˆ‘"ç›¸ä¼¼ï¼Œå‘é‡æ¥è¿‘
"è‹¹æœ" â†’ [-0.8, 0.9, -0.3, ..., 0.7] # ä¸"æˆ‘"ä¸åŒï¼Œå‘é‡å·®å¼‚å¤§
```

### 1.3 å…³é”®æœ¯è¯­è¯´æ˜
- **Embeddingå±‚ï¼ˆnn.Embeddingï¼‰**ï¼šPyTorchä¸­å®ç°æŸ¥æ‰¾è¡¨çš„æ¨¡å—
- **weight**ï¼šembeddingå±‚çš„æ ¸å¿ƒå‚æ•°ï¼Œå­˜å‚¨æ‰€æœ‰è¯å‘é‡çš„çŸ©é˜µ
- **padding_idx**ï¼šæŒ‡å®šå“ªä¸ªç´¢å¼•ä¿æŒé›¶å‘é‡ï¼ˆé€šå¸¸æ˜¯0ï¼Œè¡¨ç¤ºå¡«å……ç¬¦ï¼‰
- **ç¨ å¯†å‘é‡ï¼ˆDense Vectorï¼‰**ï¼šembeddingäº§ç”Ÿçš„è¿ç»­ã€ä½ç»´å‘é‡
- **ç¨€ç–å‘é‡ï¼ˆSparse Vectorï¼‰**ï¼šone-hotè¿™æ ·çš„é«˜ç»´ã€å¤§éƒ¨åˆ†ä¸º0çš„å‘é‡

## 2. æŠ€æœ¯ç»†èŠ‚æ·±åº¦å‰–æ

### 2.1 æ•°å­¦åŸç†ä¸ç­‰ä»·æ€§æ¨å¯¼

#### ç¦»æ•£ç©ºé—´åˆ°è¿ç»­ç©ºé—´çš„æ˜ å°„
è®¾è¯è¡¨å¤§å°ä¸º$N$ï¼Œembeddingç»´åº¦ä¸º$d$ã€‚embeddingå±‚çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå‚æ•°çŸ©é˜µï¼š

$$E \in \mathbb{R}^{N \times d}$$

æŸ¥æ‰¾æ“ä½œçš„æ•°å­¦è¡¨è¾¾ï¼š
$$f: \{0, 1, 2, ..., N-1\} \rightarrow \mathbb{R}^d$$
$$f(i) = E[i] \text{ ï¼ˆç¬¬iè¡Œå‘é‡ï¼‰}$$

#### one-hot + çº¿æ€§å±‚çš„ç­‰ä»·æ€§è¯æ˜

one-hotå‘é‡$\mathbf{x}_i \in \mathbb{R}^N$ï¼Œåªæœ‰ç¬¬$i$ä¸ªä½ç½®ä¸º1ï¼š
$$\mathbf{x}_i = [0, 0, ..., 1, ..., 0]^T$$

çº¿æ€§å±‚æƒé‡$W \in \mathbb{R}^{N \times d}$ï¼Œè¾“å‡ºä¸ºï¼š
$$\mathbf{y} = \mathbf{x}_i^T W = W[i]$$

è¿™ä¸embeddingæŸ¥è¡¨$E[i]$å®Œå…¨ç­‰ä»·ï¼å› æ­¤ï¼š
> **embeddingå±‚ = one-hotç¼–ç  + çº¿æ€§å±‚ï¼ˆæ— biasï¼‰**

è®©æˆ‘ä»¬ç”¨ä»£ç éªŒè¯è¿™ä¸ªç­‰ä»·æ€§ï¼š

```python
import torch
import torch.nn as nn

vocab_size, embed_size = 1000, 64
word_idx = 42

# æ–¹æ³•1ï¼šembeddingæŸ¥æ‰¾
embedding = nn.Embedding(vocab_size, embed_size)
result1 = embedding(torch.tensor(word_idx))

# æ–¹æ³•2ï¼šone-hot + çº¿æ€§å±‚
one_hot = torch.zeros(vocab_size)
one_hot[word_idx] = 1
linear = nn.Linear(vocab_size, embed_size, bias=False)
linear.weight.data = embedding.weight.data.T  # å…±äº«æƒé‡
result2 = linear(one_hot)

print(torch.allclose(result1, result2))  # True
```

#### embeddingæ¢¯åº¦è®¡ç®—çš„æ·±åº¦æ¨å¯¼

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œembedding.weightçš„æ¢¯åº¦æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡é“¾å¼æ³•åˆ™æ¥ç†è§£ï¼š

è®¾æŸå¤±å‡½æ•°ä¸º$L$ï¼Œå¯¹äºè¯ç´¢å¼•$i$ï¼Œå…¶embeddingå‘é‡$\mathbf{e}_i = E[i]$ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼š

$$\frac{\partial L}{\partial E[i]} = \frac{\partial L}{\partial \mathbf{e}_i}$$

**å…³é”®æ´å¯Ÿ**ï¼šåªæœ‰åœ¨å½“å‰batchä¸­å‡ºç°çš„è¯ç´¢å¼•ï¼Œå…¶å¯¹åº”çš„embeddingè¡Œæ‰ä¼šæœ‰éé›¶æ¢¯åº¦ã€‚

```python
# æ¢¯åº¦è®¡ç®—ç¤ºä¾‹
def demonstrate_embedding_gradients():
    embedding = nn.Embedding(5, 3)
    embedding.weight.data.fill_(1.0)  # åˆå§‹åŒ–ä¸º1ä¾¿äºè§‚å¯Ÿ
    
    # è¾“å…¥åºåˆ—ï¼šç´¢å¼•[1, 2, 1]ï¼Œæ³¨æ„ç´¢å¼•1å‡ºç°2æ¬¡
    input_seq = torch.tensor([1, 2, 1])
    embedded = embedding(input_seq)
    
    # ç®€å•æŸå¤±ï¼šæ‰€æœ‰embeddingå‘é‡å…ƒç´ å’Œ
    loss = embedded.sum()
    loss.backward()
    
    print("æ¢¯åº¦åˆ†å¸ƒ:")
    for i in range(5):
        grad = embedding.weight.grad[i]
        print(f"ç´¢å¼•{i}: {grad} (å‡ºç°æ¬¡æ•°: {(input_seq == i).sum().item()})")
    
    # è¾“å‡ºæ˜¾ç¤ºï¼šç´¢å¼•1çš„æ¢¯åº¦æ˜¯ç´¢å¼•2çš„2å€ï¼ˆå› ä¸ºå‡ºç°2æ¬¡ï¼‰

demonstrate_embedding_gradients()
```

### 2.2 embeddingçš„æ•°æ®å†™å…¥æœºåˆ¶

#### ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åŒ–å†™å…¥
PyTorchçš„embeddingåˆå§‹åŒ–ç­–ç•¥åŠå…¶å½±å“ï¼š

```python
# é»˜è®¤åˆå§‹åŒ–ï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒN(0,1)
embedding_default = nn.Embedding(vocab_size, embed_size)

# Xavier/Glorotåˆå§‹åŒ–ï¼ˆæ›´é€‚åˆæŸäº›æ¿€æ´»å‡½æ•°ï¼‰
embedding_xavier = nn.Embedding(vocab_size, embed_size)
nn.init.xavier_uniform_(embedding_xavier.weight)

# Kaimingåˆå§‹åŒ–ï¼ˆé€‚åˆReLUæ¿€æ´»å‡½æ•°ï¼‰
embedding_kaiming = nn.Embedding(vocab_size, embed_size)
nn.init.kaiming_uniform_(embedding_kaiming.weight)

# æ¥è‡ªLSTMä»£ç çš„å®é™…ä¾‹å­
encoder = LSTMEncoder(src_vocab.vocab_size, embed_size, hidden_size, num_layers)
# æ­¤æ—¶embedding.weightå·²è¢«éšæœºåˆå§‹åŒ–
```

ç‰¹æ®Šå¤„ç†paddingï¼š
```python
# padding_idx=0çš„ä½ç½®ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºé›¶å‘é‡
self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
# embedding.weight[0] = [0, 0, 0, ..., 0]
```

#### ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ›´æ–°å†™å…¥
åœ¨LSTMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œembeddingæƒé‡é€šè¿‡åå‘ä¼ æ’­æ›´æ–°ï¼š

```python
# æ¥è‡ªencoder_decoder_lstm.pyç¬¬52è¡Œ
embedded = self.embedding(input_seq)  # å‰å‘ä¼ æ’­ï¼šæŸ¥è¡¨

# è®­ç»ƒå¾ªç¯ï¼ˆç¬¬241-249è¡Œï¼‰
loss.backward()      # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
optimizer.step()     # å‚æ•°æ›´æ–°ï¼šä¿®æ”¹embedding.weight
```

**ç¨€ç–æ›´æ–°çš„æ•°å­¦è¡¨ç¤º**ï¼š
è®¾batchä¸­å‡ºç°çš„è¯ç´¢å¼•é›†åˆä¸º$\mathcal{B}$ï¼Œåˆ™åªæœ‰è¿™äº›ç´¢å¼•å¯¹åº”çš„embeddingè¡Œä¼šè¢«æ›´æ–°ï¼š

$$E[i]_{t+1} = \begin{cases}
E[i]_t - \eta \nabla_{E[i]} L & \text{if } i \in \mathcal{B} \\
E[i]_t & \text{if } i \notin \mathcal{B}
\end{cases}$$

#### ç¬¬ä¸‰é˜¶æ®µï¼šæ‰‹åŠ¨å†™å…¥ï¼ˆå¯é€‰ï¼‰
æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¾ç½®embeddingæƒé‡ï¼Œå¸¸ç”¨äºåŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼š

```python
# åŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼ˆå¦‚Word2Vecã€GloVeï¼‰
pretrained_embeddings = load_pretrained_vectors()
model.embedding.weight.data = pretrained_embeddings

# æˆ–è€…éƒ¨åˆ†æ›¿æ¢
model.embedding.weight.data[word_idx] = custom_vector
```

### 2.3 embeddingå±‚çš„å®Œæ•´å‰–æ

embeddingå±‚ä¸ä»…ä»…æ˜¯weightï¼Œè¿˜åŒ…å«å¤šä¸ªé‡è¦å±æ€§ï¼š

```python
embedding = nn.Embedding(
    num_embeddings=10000,    # è¯è¡¨å¤§å°
    embedding_dim=300,       # å‘é‡ç»´åº¦  
    padding_idx=0,          # paddingç´¢å¼•ï¼Œè¯¥è¡Œä¸å‚ä¸è®­ç»ƒ
    max_norm=None,          # å‘é‡èŒƒæ•°çº¦æŸ
    norm_type=2.0,          # èŒƒæ•°ç±»å‹
    scale_grad_by_freq=False, # æ˜¯å¦æŒ‰è¯é¢‘ç¼©æ”¾æ¢¯åº¦
    sparse=False            # æ˜¯å¦ä½¿ç”¨ç¨€ç–æ›´æ–°
)

# æ ¸å¿ƒå‚æ•°ï¼šåªæœ‰weightæ˜¯å¯è®­ç»ƒçš„
print(list(embedding.parameters()))  # åªæœ‰embedding.weight
```

#### è´Ÿé‡‡æ ·å¯¹embeddingè´¨é‡çš„å½±å“

åœ¨Word2Vecç­‰æ¨¡å‹ä¸­ï¼Œè´Ÿé‡‡æ ·ç­–ç•¥æ˜¾è‘—å½±å“embeddingè´¨é‡ï¼š

```python
# åŸºäºè¯é¢‘çš„è´Ÿé‡‡æ ·æ¦‚ç‡
def negative_sampling_probability(word_freq, total_freq, power=0.75):
    """è®¡ç®—è´Ÿé‡‡æ ·æ¦‚ç‡"""
    return (word_freq / total_freq) ** power

# é«˜é¢‘è¯è¢«é‡‡æ ·ä¸ºè´Ÿæ ·æœ¬çš„æ¦‚ç‡æ›´é«˜ï¼Œæœ‰åŠ©äºå­¦ä¹ æ›´å¥½çš„embedding
```

### 2.4 å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹æ®Šembeddingæœºåˆ¶

#### Token Embeddingã€Position Embeddingã€Segment Embedding
åœ¨ç°ä»£LLMï¼ˆå¦‚BERTã€GPTï¼‰ä¸­ï¼Œembeddingä¸ä»…ä»…æ˜¯è¯åµŒå…¥ï¼š

```python
class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_seq_len, dropout=0.1):
        super().__init__()
        # Token embeddingï¼šå°†è¯æ±‡æ˜ å°„ä¸ºå‘é‡
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # Position embeddingï¼šç¼–ç ä½ç½®ä¿¡æ¯
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Segment embeddingï¼šåŒºåˆ†ä¸åŒæ®µè½ï¼ˆå¦‚BERTä¸­çš„å¥å­A/Bï¼‰
        self.segment_embedding = nn.Embedding(2, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, tokens, segments=None):
        seq_len = tokens.size(1)
        positions = torch.arange(seq_len).unsqueeze(0)
        
        # ä¸‰ç§embeddingç›¸åŠ 
        embeddings = self.token_embedding(tokens)
        embeddings += self.position_embedding(positions)
        
        if segments is not None:
            embeddings += self.segment_embedding(segments)
            
        return self.dropout(embeddings)
```

#### æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šç°ä»£LLMçš„æ–°è¶‹åŠ¿

```python
def apply_rotary_pos_emb(x, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    # x: [batch_size, seq_len, num_heads, head_dim]
    x1, x2 = x[..., ::2], x[..., 1::2]
    
    # æ—‹è½¬æ“ä½œ
    rotated = torch.stack([-x2, x1], dim=-1).flatten(-2)
    return x * cos + rotated * sin

# RoPEé€šè¿‡æ—‹è½¬è€ŒéåŠ æ³•çš„æ–¹å¼ç¼–ç ä½ç½®ï¼Œåœ¨é•¿åºåˆ—ä¸Šè¡¨ç°æ›´å¥½
```

### 2.5 LSTMä¸­çš„embeddingå®é™…å·¥ä½œæµç¨‹

è®©æˆ‘ä»¬è¿½è¸ªLSTMä»£ç ä¸­embeddingçš„å®Œæ•´æ•°æ®æµï¼š

```python
# 1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆç¬¬27-29è¡Œï¼‰
src_sentence = "æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"
src_indices = src_vocab.sentence_to_indices(src_sentence)
# ç»“æœï¼š[4, 5, 6, 7, 8] ï¼ˆå‡è®¾çš„ç´¢å¼•ï¼‰

# 2. è½¬æ¢ä¸ºtensorï¼ˆç¬¬52è¡Œï¼‰
input_seq = torch.tensor([[4, 5, 6, 7, 8]])  # [batch_size=1, seq_len=5]

# 3. embeddingæŸ¥è¡¨ï¼ˆç¬¬52è¡Œï¼‰
embedded = self.embedding(input_seq)
# å½¢çŠ¶ï¼š[1, 5, 64] ï¼ˆbatch_size, seq_len, embed_sizeï¼‰
# æ¯ä¸ªè¯è¢«æ›¿æ¢ä¸º64ç»´å‘é‡

# 4. LSTMå¤„ç†embeddedå‘é‡ï¼Œè€Œä¸æ˜¯åŸå§‹ç´¢å¼•
```

## 3. æ·±åº¦ç†è§£ä¸å¸¸è§è¯¯åŒº

### 3.1 æ ¸å¿ƒæ¦‚å¿µè¾¨æ

#### è¯¯åŒº1ï¼š"one-hotå‘é‡ä¹Ÿæ˜¯embedding"
**æ­£ç¡®ç†è§£**ï¼šone-hotåªæ˜¯ç¼–ç æ–¹å¼ï¼Œä¸æ˜¯embeddingã€‚embeddingç‰¹æŒ‡ï¼š
- ç¨ å¯†ï¼ˆdenseï¼‰å‘é‡
- å¯å­¦ä¹ ï¼ˆlearnableï¼‰å‚æ•°  
- æœ‰è¯­ä¹‰ç»“æ„ï¼ˆsemantic structureï¼‰

#### è¯¯åŒº2ï¼š"è®­ç»ƒæ—¶embeddingå±‚è¢«æ›´æ–°"
**æ­£ç¡®ç†è§£**ï¼šè¢«æ›´æ–°çš„æ˜¯embedding.weightå‚æ•°ï¼Œembeddingå±‚æœ¬èº«åªæ˜¯"å®¹å™¨"ï¼š

```python
# é”™è¯¯è¯´æ³•ï¼š"embeddingå±‚è¢«è®­ç»ƒ"
# æ­£ç¡®è¯´æ³•ï¼š"embedding.weightå‚æ•°è¢«è®­ç»ƒ"

for name, param in model.named_parameters():
    if 'embedding' in name:
        print(f"{name}: {param.shape}")
        # è¾“å‡ºï¼šencoder.embedding.weight: torch.Size([15, 64])
```

#### è¯¯åŒº3ï¼š"è¯å‘é‡"å’Œ"embedding.weight"æ˜¯ä¸åŒçš„ä¸œè¥¿  
**æ­£ç¡®ç†è§£**ï¼šæˆ‘ä»¬å¸¸è¯´çš„"è¯å‘é‡"å°±æ˜¯embedding.weightçš„æ¯ä¸€è¡Œï¼š

```python
# "æˆ‘"çš„è¯å‘é‡ = embedding.weight[word_idx]
word_vector = model.encoder.embedding.weight[4]  # å‡è®¾"æˆ‘"çš„ç´¢å¼•æ˜¯4
print(f"'æˆ‘'çš„è¯å‘é‡ç»´åº¦: {word_vector.shape}")  # [64]
```

### 3.2 æ€§èƒ½ä¸å·¥ç¨‹è€ƒé‡

#### å†…å­˜æ•ˆç‡å¯¹æ¯”
```python
vocab_size = 50000
embed_size = 300

# one-hotæ–¹å¼ï¼šæ¯ä¸ªè¯éœ€è¦50000ç»´
one_hot_memory = vocab_size * 4  # çº¦200KBæ¯ä¸ªè¯

# embeddingæ–¹å¼ï¼šæ¯ä¸ªè¯åªéœ€è¦300ç»´
embedding_memory = embed_size * 4  # çº¦1.2KBæ¯ä¸ªè¯
# å†…å­˜èŠ‚çœï¼š50000/300 â‰ˆ 167å€
```

#### è®¡ç®—æ•ˆç‡å¯¹æ¯”
```python
# one-hot + çº¿æ€§å±‚ï¼šéœ€è¦çŸ©é˜µä¹˜æ³•
# æ—¶é—´å¤æ‚åº¦ï¼šO(vocab_size * embed_size)

# embeddingæŸ¥è¡¨ï¼šç›´æ¥ç´¢å¼•è®¿é—®  
# æ—¶é—´å¤æ‚åº¦ï¼šO(1)
```

### 3.3 å¤§è§„æ¨¡embeddingçš„å·¥ç¨‹æŒ‘æˆ˜

#### åˆ†å¸ƒå¼embeddingä¼˜åŒ–
åœ¨å¤§å‹LLMè®­ç»ƒä¸­ï¼Œembeddingå±‚å¾€å¾€æ˜¯å‚æ•°æœ€å¤šçš„éƒ¨åˆ†ï¼š

```python
# GPT-3çš„embeddingå±‚å‚æ•°é‡ä¼°ç®—
vocab_size = 50257  # GPT-3è¯è¡¨å¤§å°
d_model = 12288     # GPT-3æœ€å¤§æ¨¡å‹ç»´åº¦
embedding_params = vocab_size * d_model  # çº¦6.2äº¿å‚æ•°

# åˆ†å¸ƒå¼ä¼˜åŒ–ç­–ç•¥ï¼š
# 1. å‚æ•°æœåŠ¡å™¨ï¼šå°†embeddingåˆ†ç‰‡å­˜å‚¨åœ¨ä¸åŒèŠ‚ç‚¹
# 2. æ¨¡å‹å¹¶è¡Œï¼šå°†embeddingæŒ‰è¯æ±‡æˆ–ç»´åº¦åˆ‡åˆ†
# 3. æ¢¯åº¦å‹ç¼©ï¼šä½¿ç”¨ä½ç²¾åº¦æˆ–ç¨€ç–æ¢¯åº¦é€šä¿¡
```

#### embeddingé‡åŒ–ä¸å‹ç¼©

```python
# é‡åŒ–embeddingä»¥èŠ‚çœå†…å­˜
def quantize_embedding(embedding_weight, bits=8):
    """å°†embeddingæƒé‡é‡åŒ–åˆ°æŒ‡å®šä½æ•°"""
    min_val, max_val = embedding_weight.min(), embedding_weight.max()
    scale = (max_val - min_val) / (2**bits - 1)
    
    quantized = torch.round((embedding_weight - min_val) / scale)
    return quantized.byte(), scale, min_val

# ä½ç§©åˆ†è§£å‡å°‘å‚æ•°é‡
def low_rank_embedding(vocab_size, embed_size, rank):
    """ä½¿ç”¨ä½ç§©åˆ†è§£å‡å°‘embeddingå‚æ•°"""
    return nn.Sequential(
        nn.Embedding(vocab_size, rank),
        nn.Linear(rank, embed_size, bias=False)
    )
```

#### é«˜çº§ç‰¹æ€§ä¸ä¼˜åŒ–

#### ç¨€ç–æ›´æ–°ä¼˜åŒ–
åœ¨å¤§è§„æ¨¡è¯è¡¨åœºæ™¯ä¸‹ï¼Œä½¿ç”¨ç¨€ç–æ›´æ–°å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ï¼š

```python
# å¯ç”¨ç¨€ç–æ›´æ–°
embedding = nn.Embedding(vocab_size, embed_size, sparse=True)
optimizer = torch.optim.SparseAdam(embedding.parameters())
```

#### å‘é‡èŒƒæ•°çº¦æŸ
é˜²æ­¢embeddingå‘é‡è¿‡å¤§ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ï¼š

```python
# çº¦æŸembeddingå‘é‡çš„L2èŒƒæ•°ä¸è¶…è¿‡1.0
embedding = nn.Embedding(vocab_size, embed_size, max_norm=1.0)
```

## 4. åº”ç”¨å®è·µä¸æœ€ä½³å®è·µ

### 4.1 LSTM Seq2Seqä¸­çš„embeddingåº”ç”¨

åœ¨æˆ‘ä»¬çš„æœºå™¨ç¿»è¯‘ç¤ºä¾‹ä¸­ï¼Œembeddingæ‰®æ¼”å…³é”®è§’è‰²ï¼š

```python
# ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä½¿ç”¨embedding
class LSTMEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        
class LSTMDecoder(nn.Module):  
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
```

**è®¾è®¡è€ƒè™‘**ï¼š
- æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä½¿ç”¨ç‹¬ç«‹çš„embeddingï¼ˆæ”¯æŒä¸åŒè¯è¡¨ï¼‰
- ç›¸åŒçš„embed_sizeç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
- padding_idx=0å¤„ç†å˜é•¿åºåˆ—

### 4.2 embeddingç»´åº¦é€‰æ‹©æŒ‡å—

| æ•°æ®è§„æ¨¡ | æ¨èç»´åº¦ | è¯´æ˜ |
|---------|---------|------|
| å°å‹ï¼ˆ<1ä¸‡è¯æ±‡ï¼‰ | 32-128 | é¿å…è¿‡æ‹Ÿåˆ |
| ä¸­å‹ï¼ˆ1-10ä¸‡è¯æ±‡ï¼‰ | 128-512 | å¹³è¡¡è¡¨è¾¾åŠ›ä¸æ•ˆç‡ |
| å¤§å‹ï¼ˆ>10ä¸‡è¯æ±‡ï¼‰ | 300-1024 | å……åˆ†è¡¨è¾¾å¤æ‚è¯­ä¹‰ |
| LLMçº§åˆ«ï¼ˆ>5ä¸‡è¯æ±‡ï¼‰ | 1024-12288 | å¤§æ¨¡å‹æ ‡å‡†é…ç½® |

ä»£ç ç¤ºä¾‹ä¸­ä½¿ç”¨64ç»´ï¼Œé€‚åˆå°è§„æ¨¡æ¼”ç¤ºä»»åŠ¡ã€‚

### 4.3 å†·å¯åŠ¨ä¸OOVå¤„ç†

#### æœªç™»å½•è¯ï¼ˆOOVï¼‰å¤„ç†ç­–ç•¥
```python
def sentence_to_indices(self, sentence):
    return [self.word2idx.get(word, self.word2idx['<UNK>']) 
            for word in sentence.split()]
```

#### é¢„è®­ç»ƒembeddingåˆå§‹åŒ–
```python
def load_pretrained_embeddings(vocab, embedding_dim):
    """åŠ è½½é¢„è®­ç»ƒè¯å‘é‡åˆå§‹åŒ–embedding"""
    pretrained = {}  # ä»Word2Vec/GloVeæ–‡ä»¶åŠ è½½
    
    embedding_matrix = torch.randn(len(vocab), embedding_dim)
    for word, idx in vocab.word2idx.items():
        if word in pretrained:
            embedding_matrix[idx] = torch.tensor(pretrained[word])
    
    return embedding_matrix
```

### 4.4 embeddingåœ¨RAGä¸prompt engineeringä¸­çš„åº”ç”¨

#### å‘é‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰
```python
class RAGEmbedding(nn.Module):
    """ç”¨äºRAGçš„embeddingæ¨¡å—"""
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        super().__init__()
        from sentence_transformers import SentenceTransformer
        self.encoder = SentenceTransformer(model_name)
        
    def encode_documents(self, documents):
        """å°†æ–‡æ¡£ç¼–ç ä¸ºå‘é‡ç”¨äºæ£€ç´¢"""
        return self.encoder.encode(documents)
    
    def encode_query(self, query):
        """å°†æŸ¥è¯¢ç¼–ç ä¸ºå‘é‡"""
        return self.encoder.encode([query])

# åœ¨RAGä¸­ï¼Œembeddingç”¨äºï¼š
# 1. å°†çŸ¥è¯†åº“æ–‡æ¡£ç¼–ç ä¸ºå‘é‡å­˜å‚¨
# 2. å°†ç”¨æˆ·æŸ¥è¯¢ç¼–ç ä¸ºå‘é‡è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…
# 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£ä½œä¸ºLLMçš„ä¸Šä¸‹æ–‡
```

#### Promptå‘é‡åŒ–ä¸æ£€ç´¢
```python
def prompt_embedding_search(query_embedding, prompt_database):
    """åŸºäºembeddingçš„promptæ£€ç´¢"""
    similarities = []
    for prompt_embed in prompt_database:
        sim = torch.cosine_similarity(query_embedding, prompt_embed, dim=0)
        similarities.append(sim)
    
    # è¿”å›æœ€ç›¸ä¼¼çš„prompt
    best_idx = torch.argmax(torch.tensor(similarities))
    return best_idx, similarities[best_idx]
```

### 4.5 å¤šè¯­è¨€ä¸è·¨é¢†åŸŸåº”ç”¨

#### å…±äº«embeddingç­–ç•¥
å¯¹äºç›¸ä¼¼ä»»åŠ¡ï¼Œå¯ä»¥å…±äº«embeddingå‡å°‘å‚æ•°ï¼š

```python
# å…±äº«ç¼–ç å™¨å’Œè§£ç å™¨çš„embedding
shared_embedding = nn.Embedding(vocab_size, embed_size)

class SharedEmbeddingSeq2Seq(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        self.shared_embedding = nn.Embedding(vocab_size, embed_size)
        self.encoder = LSTMEncoder(vocab_size, embed_size, hidden_size)
        self.decoder = LSTMDecoder(vocab_size, embed_size, hidden_size)
        
        # å…±äº«embeddingæƒé‡
        self.encoder.embedding = self.shared_embedding
        self.decoder.embedding = self.shared_embedding
```

## 5. è°ƒè¯•ä¸å¯è§†åŒ–

### 5.1 embeddingè´¨é‡æ£€æŸ¥

```python
def analyze_embedding_quality(embedding, vocab):
    """åˆ†æembeddingè´¨é‡"""
    # æ£€æŸ¥ç›¸ä¼¼è¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
    def cosine_similarity(vec1, vec2):
        return torch.cosine_similarity(vec1, vec2, dim=0)
    
    # ç¤ºä¾‹ï¼šæ£€æŸ¥"çˆ±"å’Œ"å–œæ¬¢"çš„ç›¸ä¼¼åº¦
    love_idx = vocab.word2idx["çˆ±"] 
    like_idx = vocab.word2idx["å–œæ¬¢"]
    
    love_vec = embedding.weight[love_idx]
    like_vec = embedding.weight[like_idx]
    
    similarity = cosine_similarity(love_vec, like_vec)
    print(f"'çˆ±'å’Œ'å–œæ¬¢'çš„ç›¸ä¼¼åº¦: {similarity:.3f}")
    
    # æ£€æŸ¥embeddingçš„ç»Ÿè®¡ç‰¹æ€§
    print(f"Embeddingå‡å€¼: {embedding.weight.mean():.3f}")
    print(f"Embeddingæ ‡å‡†å·®: {embedding.weight.std():.3f}")
    print(f"EmbeddingèŒƒæ•°åˆ†å¸ƒ: {embedding.weight.norm(dim=1).mean():.3f}")
```

### 5.2 embeddingå¯è§†åŒ–ï¼šæ·±åº¦è§£æt-SNEé™ç»´ç»“æœ

ä¸ºäº†ç›´è§‚åœ°ç†è§£embeddingç©ºé—´ä¸­å­¦åˆ°çš„è¯å‘é‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é™ç»´æŠ€æœ¯ï¼ˆå¦‚t-SNEæˆ–PCAï¼‰å°†é«˜ç»´è¯å‘é‡æŠ•å½±åˆ°2Då¹³é¢è¿›è¡Œå¯è§†åŒ–ã€‚é€šè¿‡t-SNEå¯è§†åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥ç†è§£LSTM seq2seqæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„è¯­ä¹‰è¡¨ç¤ºç»“æ„ã€‚

#### 6.2.1 t-SNEé™ç»´æŠ€æœ¯åŸç†

t-åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ï¼ˆt-SNEï¼‰æ˜¯ä¸€ç§éçº¿æ€§é™ç»´æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚åˆå¯è§†åŒ–é«˜ç»´æ•°æ®ï¼š

1. **ä¿æŒå±€éƒ¨ç›¸ä¼¼æ€§**ï¼šå°†é«˜ç»´ç©ºé—´ä¸­ç›¸ä¼¼çš„ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­ä¿æŒæ¥è¿‘
2. **æ¦‚ç‡åˆ†å¸ƒæ˜ å°„**ï¼šåœ¨é«˜ç»´ç©ºé—´ç”¨é«˜æ–¯åˆ†å¸ƒè¡¨ç¤ºç‚¹å¯¹ç›¸ä¼¼æ€§ï¼Œåœ¨ä½ç»´ç©ºé—´ç”¨tåˆ†å¸ƒè¡¨ç¤º
3. **æ¢¯åº¦ä¸‹é™ä¼˜åŒ–**ï¼šé€šè¿‡æœ€å°åŒ–KLæ•£åº¦æ¥å¯»æ‰¾æœ€ä¼˜çš„2Dè¡¨ç¤º

```python
def visualize_embeddings(embedding_matrix, vocab, method='tsne', title="Embedding Visualization"):
    """
    å¯è§†åŒ–embeddingå‘é‡çš„2DæŠ•å½±
    
    Args:
        embedding_matrix: è¯åµŒå…¥çŸ©é˜µ [vocab_size, embed_dim]
        vocab: è¯æ±‡è¡¨å¯¹è±¡
        method: é™ç»´æ–¹æ³•ï¼Œ'tsne' æˆ– 'pca'
        title: å›¾ç‰‡æ ‡é¢˜
    """
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    import numpy as np
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # é€‰æ‹©é™ç»´æ–¹æ³•
    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42, max_iter=1000)
        reduced_embeddings = reducer.fit_transform(embedding_matrix.numpy())
    else:  # PCA
        reducer = PCA(n_components=2, random_state=42)
        reduced_embeddings = reducer.fit_transform(embedding_matrix.numpy())
    
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], 
                         alpha=0.7, s=50, c=range(len(vocab.idx2word)))
    
    # æ ‡æ³¨è¯æ±‡ï¼ˆä¸ºé¿å…é‡å ï¼Œåªæ ‡æ³¨éƒ¨åˆ†è¯æ±‡ï¼‰
    sample_indices = np.random.choice(len(vocab.idx2word), 
                                    min(10, len(vocab.idx2word)), 
                                    replace=False)
    
    for idx in sample_indices:
        if idx in vocab.idx2word and idx > 3:  # è·³è¿‡ç‰¹æ®Šç¬¦å·
            word = vocab.idx2word[idx]
            plt.annotate(word, 
                        (reduced_embeddings[idx, 0], reduced_embeddings[idx, 1]),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=10, alpha=0.8)
    
    plt.title(f'{title} ({method.upper()})', fontsize=14, fontweight='bold')
    plt.xlabel(f'{method.upper()} Dimension 1', fontsize=12)
    plt.ylabel(f'{method.upper()} Dimension 2', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(f'embedding_visualization_{method}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return reduced_embeddings
```

#### 6.2.2 å¯è§†åŒ–ç»“æœçš„è¯­ä¹‰è§£é‡Š

å½“LSTMæ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œt-SNEå¯è§†åŒ–å›¾å°†æ˜¾ç¤ºä»¥ä¸‹é‡è¦ç‰¹å¾ï¼š

1. **è¯­ä¹‰èšç±»ç°è±¡**ï¼š
   - ç›¸ä¼¼è¯­ä¹‰çš„è¯æ±‡åœ¨2Dç©ºé—´ä¸­å½¢æˆèšç±»
   - ä¾‹å¦‚ï¼š"æˆ‘"ã€"ä½ "ç­‰äººç§°ä»£è¯å¯èƒ½èšé›†åœ¨ä¸€èµ·
   - "çˆ±"ã€"å–œæ¬¢"ç­‰æƒ…æ„Ÿè¯æ±‡å½¢æˆå¦ä¸€ä¸ªèšç±»

2. **è·ç¦»è¡¨ç¤ºç›¸ä¼¼æ€§**ï¼š
   - åœ¨å¯è§†åŒ–å›¾ä¸­ï¼Œä¸¤ä¸ªè¯æ±‡ç‚¹ä¹‹é—´çš„è·ç¦»åæ˜ äº†å®ƒä»¬åœ¨åŸå§‹64ç»´embeddingç©ºé—´ä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦
   - è·ç¦»è¶Šè¿‘ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼

3. **ç‰¹æ®Šæ ‡è®°çš„åˆ†å¸ƒ**ï¼š
   - `<PAD>`ã€`<SOS>`ã€`<EOS>`ç­‰ç‰¹æ®Šæ ‡è®°é€šå¸¸è¿œç¦»å®é™…è¯æ±‡
   - è¿™è¡¨æ˜æ¨¡å‹å­¦ä¼šäº†åŒºåˆ†åŠŸèƒ½æ€§æ ‡è®°å’Œè¯­ä¹‰æ€§è¯æ±‡

#### 6.2.3 æ•°å­¦åŸç†æ·±åº¦è§£æ

åœ¨æˆ‘ä»¬çš„LSTM seq2seqæ¨¡å‹ä¸­ï¼Œæ¯ä¸ªè¯çš„embeddingå‘é‡$\mathbf{e}_i \in \mathbb{R}^{64}$é€šè¿‡ä»¥ä¸‹è¿‡ç¨‹å­¦ä¹ ï¼š

1. **åˆå§‹åŒ–**ï¼šéšæœºåˆå§‹åŒ–ä¸º$\mathbf{e}_i \sim \mathcal{N}(0, 1)$
2. **ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼šé€šè¿‡LSTMçš„éšçŠ¶æ€ä¼ æ’­ï¼Œç›¸é‚»å‡ºç°çš„è¯è·å¾—ç›¸ä¼¼çš„æ¢¯åº¦æ›´æ–°
3. **ä»»åŠ¡é©±åŠ¨ä¼˜åŒ–**ï¼šç¿»è¯‘ä»»åŠ¡çš„æŸå¤±å‡½æ•°æŒ‡å¯¼embeddingå­¦ä¹ è¯­ä¹‰å¯¹åº”å…³ç³»

t-SNEå°†è¿™ä¸ª64ç»´ç©ºé—´æ˜ å°„åˆ°2Dï¼š
$$\mathbf{e}_i^{64D} \xrightarrow{t-SNE} \mathbf{e}_i^{2D}$$

ä¿æŒçš„ç›¸ä¼¼æ€§å…³ç³»ï¼š
$$\text{sim}(\mathbf{e}_i, \mathbf{e}_j) \propto \frac{1}{1 + ||\mathbf{e}_i^{2D} - \mathbf{e}_j^{2D}||^2}$$

### 6.3 Embeddingè´¨é‡å®šé‡éªŒè¯ç³»ç»Ÿ

åŸºäºæˆ‘ä»¬åœ¨LSTMè„šæœ¬ä¸­é›†æˆçš„åˆ†æåŠŸèƒ½ï¼Œæ¨¡å‹è®­ç»ƒå®Œæˆåä¼šè‡ªåŠ¨æ‰§è¡Œå…¨é¢çš„embeddingè´¨é‡è¯„ä¼°ã€‚ä»¥ä¸‹æ˜¯å®é™…è¿è¡Œç»“æœçš„æ·±åº¦è§£æï¼š

#### 6.3.1 è¯æ±‡ç›¸ä¼¼æ€§åˆ†æ

å½“è„šæœ¬è¿è¡Œ`analyze_embedding_quality()`å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```
=== Embeddingè´¨é‡åˆ†æ ===
è¯æ±‡è¡¨å¤§å°: 18
æ€»è¯æ±‡å¯¹æ•°: 153

ç›¸ä¼¼æ€§åˆ†æ (å‰10å¯¹):
'å­¦ä¹ ' â†” 'æœ‰è¶£': 0.3578
'æˆ‘' â†” 'æ™ºèƒ½': 0.3263  
'å¤©æ°”' â†” 'å¥½': 0.2856
'å¤„ç†' â†” 'è¯­è¨€': 0.2445
'ä»Šå¤©' â†” 'å¾ˆ': 0.2234
```

**æ·±åº¦è§£è¯»**ï¼š

1. **è·¨é¢†åŸŸè¯­ä¹‰å…³è”**ï¼š'å­¦ä¹ 'å’Œ'æœ‰è¶£'çš„é«˜ç›¸ä¼¼åº¦(0.3578)è¡¨æ˜æ¨¡å‹å­¦ä¼šäº†å°†å­¦ä¹ è¡Œä¸ºä¸ç§¯ææƒ…æ„Ÿè”ç³»ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è·¨è¯­ä¹‰åŸŸå…³è”ã€‚

2. **äººå·¥æ™ºèƒ½æ¦‚å¿µèšåˆ**ï¼š'æˆ‘'å’Œ'æ™ºèƒ½'çš„ç›¸ä¼¼åº¦(0.3263)å¯èƒ½åæ˜ äº†è®­ç»ƒæ•°æ®ä¸­"æˆ‘(ç ”ç©¶)äººå·¥æ™ºèƒ½"çš„è¯­å¢ƒæ¨¡å¼ã€‚

3. **ä¸Šä¸‹æ–‡å…±ç°å­¦ä¹ **ï¼š'å¤©æ°”'å’Œ'å¥½'ã€'å¤„ç†'å’Œ'è¯­è¨€'çš„ç›¸ä¼¼åº¦ä½“ç°äº†æ¨¡å‹å¯¹å›ºå®šæ­é…çš„å­¦ä¹ èƒ½åŠ›ã€‚

#### 6.3.2 ä¸»é¢˜èšç±»åˆ†æ

```
ä¸»é¢˜èšç±»åˆ†æ:
æ—¶é—´å¤©æ°”ç±»: ['ä»Šå¤©', 'å¤©æ°”', 'å¾ˆ', 'å¥½'] - å¹³å‡ç›¸ä¼¼åº¦: 0.1092
AIæŠ€æœ¯ç±»: ['äººå·¥', 'æ™ºèƒ½', 'æœºå™¨', 'æ·±åº¦'] - å¹³å‡ç›¸ä¼¼åº¦: 0.0406  
NLPç±»: ['è‡ªç„¶', 'è¯­è¨€', 'å¤„ç†'] - å¹³å‡ç›¸ä¼¼åº¦: 0.0619
æƒ…æ„Ÿç±»: ['çˆ±', 'æœ‰è¶£', 'å¼ºå¤§'] - å¹³å‡ç›¸ä¼¼åº¦: -0.0317
```

**å…³é”®å‘ç°**ï¼š

1. **æ—¶é—´å¤©æ°”èšç±»æ•ˆæœæœ€ä½³**(0.1092)ï¼šè¿™åæ˜ äº†è®­ç»ƒå¥å­"ä»Šå¤©å¤©æ°”å¾ˆå¥½"çš„é«˜é¢‘å…±ç°ï¼Œæ¨¡å‹æœ‰æ•ˆå­¦ä¹ äº†è¿™ç§è¯­ä¹‰å…³è”ã€‚

2. **NLPæœ¯è¯­ä¸­ç­‰èšåˆ**(0.0619)ï¼š'è‡ªç„¶'ã€'è¯­è¨€'ã€'å¤„ç†'å½¢æˆäº†ä¸“ä¸šæœ¯è¯­clusterï¼Œä½†ç›¸ä¼¼åº¦ä¸­ç­‰ï¼Œå¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®ä¸­è¿™äº›è¯çš„è¯­å¢ƒè¾ƒä¸ºå¤šæ ·ã€‚

3. **æƒ…æ„Ÿè¯æ±‡è´Ÿèšç±»**(-0.0317)ï¼šå‡ºä¹æ„æ–™çš„æ˜¯ï¼Œæƒ…æ„Ÿè¯æ±‡æ˜¾ç¤ºäº†è´Ÿçš„å¹³å‡ç›¸ä¼¼åº¦ï¼Œè¿™å¯èƒ½è¡¨æ˜ï¼š
   - è®­ç»ƒæ•°æ®ä¸è¶³ä»¥å»ºç«‹å¼ºæƒ…æ„Ÿå…³è”
   - è¿™äº›è¯åœ¨ä¸åŒå¥å­ä¸­å‡ºç°ï¼Œç¼ºä¹ç›´æ¥çš„ä¸Šä¸‹æ–‡è”ç³»
   - æ¨¡å‹æ›´å¤šå­¦ä¹ åˆ°äº†å¥æ³•ç»“æ„è€Œéè¯­ä¹‰æƒ…æ„Ÿ

#### 6.3.3 ç»Ÿè®¡å¥åº·åº¦åˆ†æ

```
ç»Ÿè®¡åˆ†æ:
å‘é‡èŒƒæ•°å‡å€¼: 8.1169 (æ ‡å‡†å·®: 0.6234)
ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€å°å€¼: -0.3186, æœ€å¤§å€¼: 0.3578
é›¶å‘é‡æ£€æŸ¥: <PAD> tokenæ­£ç¡®è®¾ç½®ä¸ºé›¶å‘é‡
```

**å¥åº·åº¦æŒ‡æ ‡è§£è¯»**ï¼š

1. **å‘é‡èŒƒæ•°ç¨³å®šæ€§**ï¼š8.1169çš„å‡å€¼å’Œ0.6234çš„æ ‡å‡†å·®è¡¨æ˜embeddingå‘é‡å…·æœ‰ç¨³å®šçš„å¤§å°ï¼Œæ²¡æœ‰å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„è¿¹è±¡ã€‚

2. **ç›¸ä¼¼åº¦åˆç†åˆ†å¸ƒ**ï¼š[-0.3186, 0.3578]çš„èŒƒå›´è¡¨æ˜æ¨¡å‹å­¦ä¼šäº†åŒºåˆ†ç›¸ä¼¼å’Œä¸ç›¸ä¼¼çš„è¯æ±‡ï¼Œç›¸ä¼¼åº¦åˆ†å¸ƒåˆç†ã€‚

3. **ç‰¹æ®Šæ ‡è®°å¤„ç†æ­£ç¡®**ï¼š`<PAD>`æ ‡è®°ç¡®å®è¢«è®¾ç½®ä¸ºé›¶å‘é‡ï¼Œç¬¦åˆé¢„æœŸã€‚

#### 6.3.4 å¼‚å¸¸æ£€æµ‹ä¸è¯Šæ–­

```
å¼‚å¸¸æ£€æµ‹:
å¼‚å¸¸å‘é‡ (èŒƒæ•°åå·® > 2.0Ïƒ):
è¯æ±‡: 'è‡ªç„¶', ç´¢å¼•: 6, èŒƒæ•°: 6.8176 (åå·®: -2.08Ïƒ)
```

**å¼‚å¸¸åˆ†æ**ï¼š

è¿™ä¸ªå¼‚å¸¸æ£€æµ‹ç»“æœè¡¨æ˜'è‡ªç„¶'è¿™ä¸ªè¯çš„embeddingå‘é‡èŒƒæ•°å¼‚å¸¸å°ï¼Œå¯èƒ½çš„åŸå› ï¼š

1. **è®­ç»ƒä¸å……åˆ†**ï¼š'è‡ªç„¶'å¯èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°é¢‘ç‡è¾ƒä½
2. **æ¢¯åº¦æ›´æ–°ä¸ä¸€è‡´**ï¼šåœ¨ä¸åŒçš„è¯­å¢ƒä¸­'è‡ªç„¶'æ¥æ”¶åˆ°ç›¸äº’å†²çªçš„æ¢¯åº¦ä¿¡å·
3. **åˆå§‹åŒ–å½±å“**ï¼šéšæœºåˆå§‹åŒ–å¯èƒ½å¯¼è‡´æŸäº›è¯æ±‡å¼€å§‹æ—¶å°±å¤„äºä¸åˆ©ä½ç½®

#### 6.3.5 ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

ç›¸æ¯”äºWord2Vecç­‰ä¸“é—¨çš„è¯å‘é‡è®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬çš„LSTM seq2seqå­¦åˆ°çš„embeddingå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **ä»»åŠ¡å¯¼å‘æ€§**ï¼šembeddingæ˜¯ä¸ºç¿»è¯‘ä»»åŠ¡ä¼˜åŒ–çš„ï¼Œè€Œéçº¯ç²¹çš„è¯­ä¹‰ç›¸ä¼¼æ€§
2. **ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§**ï¼šè™½ç„¶embeddingæœ¬èº«æ˜¯é™æ€çš„ï¼Œä½†å®ƒåæ˜ äº†seq2seqä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡å…³ç³»
3. **è·¨è¯­è¨€å¯¹é½**ï¼šç¼–ç å™¨çš„embeddingéšå«åœ°å­¦ä¹ äº†ä¸ç›®æ ‡è¯­è¨€çš„å¯¹åº”å…³ç³»

è¿™ç§åˆ†ææ–¹æ³•ä¸ºç†è§£å’Œæ”¹è¿›seq2seqæ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨è¯Šæ–­æ¨¡å‹æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–è®­ç»ƒç­–ç•¥æ–¹é¢ã€‚

### 6.4 embeddingå¼‚å¸¸æ£€æµ‹

```python
def detect_embedding_anomalies(embedding, threshold=3.0):
    """æ£€æµ‹å¼‚å¸¸çš„embeddingå‘é‡"""
    norms = embedding.weight.norm(dim=1)
    mean_norm = norms.mean()
    std_norm = norms.std()
    
    # æ£€æµ‹å¼‚å¸¸å¤§æˆ–å¼‚å¸¸å°çš„å‘é‡
    outliers = torch.where(torch.abs(norms - mean_norm) > threshold * std_norm)[0]
    
    print(f"æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸embedding:")
    for idx in outliers[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"ç´¢å¼• {idx}: èŒƒæ•° {norms[idx]:.3f}")
```

### ğŸ” ä»£ç è¦ç‚¹å›é¡¾

ä¸Šé¢çš„å®Œæ•´ä»£ç å®ç°æ¶µç›–äº†ä»¥ä¸‹æ ¸å¿ƒéƒ¨åˆ†ï¼š

- **Vocabularyç±»**ï¼šè¯æ±‡è¡¨ç®¡ç†å’Œç´¢å¼•è½¬æ¢
- **LSTMEncoderç±»**ï¼šç¼–ç å™¨å®ç°ï¼ŒåŒ…å«embeddingå±‚å’ŒLSTMå±‚
- **LSTMDecoderç±»**ï¼šè§£ç å™¨å®ç°ï¼Œæ”¯æŒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼
- **Seq2SeqModelç±»**ï¼šå®Œæ•´çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹
- **æ•°æ®å¤„ç†**ï¼šDatasetã€DataLoaderå’Œæ‰¹å¤„ç†å‡½æ•°
- **è®­ç»ƒå¾ªç¯**ï¼šå®Œæ•´çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹
- **Embeddingå¯è§†åŒ–**ï¼šè®­ç»ƒåè‡ªåŠ¨ç”Ÿæˆå¹¶ä¿å­˜è¯å‘é‡çš„t-SNEå¯è§†åŒ–å›¾åƒ

![æºè¯­è¨€Embedding t-SNE å¯è§†åŒ–](/assets/img/posts/embedding_lstm_visualization_fixed.png "æºè¯­è¨€Embedding t-SNE å¯è§†åŒ– - ä¿®å¤ä¸­æ–‡å­—ä½“æ˜¾ç¤ºé—®é¢˜åçš„å¯è§†åŒ–ç»“æœ")

## å®Œæ•´ä»£ç å®ç°

æƒ³è¦æŸ¥çœ‹å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨å®ç°ä»£ç å—ï¼Ÿå®Œæ•´çš„276è¡ŒPyTorchä»£ç å·²ç»åœ¨**æ–‡ç« å¼€å¤´**çš„"å®Œæ•´ä»£ç å®ç°"éƒ¨åˆ†æä¾›ï¼ŒåŒ…å«è¯æ±‡è¡¨æ„å»ºã€LSTMç¼–ç å™¨ã€è§£ç å™¨å’Œè®­ç»ƒæµç¨‹ã€‚

### ğŸ“ **ä»£ç æ–‡ä»¶ä¸‹è½½**

å¦‚æœæ‚¨å¸Œæœ›ä¸‹è½½ä»£ç æ–‡ä»¶åˆ°æœ¬åœ°è¿è¡Œï¼Œå¯ä»¥è®¿é—®ï¼š
**[ä¸‹è½½ä»£ç æ–‡ä»¶](/demos/lstm_encoder_decoder.html)**

### ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **å­¦ä¹ è·¯å¾„**ï¼šç»“åˆæ–‡ç« å¼€å¤´çš„å®Œæ•´ä»£ç ä¸æœ¬æ–‡çš„ç†è®ºåˆ†æ
2. **å®è·µæ“ä½œ**ï¼šå°†ä»£ç å¤åˆ¶åˆ°æœ¬åœ°ï¼Œå°è¯•è¿è¡Œå¹¶ä¿®æ”¹å‚æ•°  
3. **æ·±å…¥ç ”ç©¶**ï¼šåŸºäºè¿™ä¸ªåŸºç¡€å®ç°ï¼Œæ¢ç´¢æ›´é«˜çº§çš„seq2seqå˜ä½“

## å»¶ä¼¸é˜…è¯»

### ç»å…¸è®ºæ–‡
- [Efficient Estimation of Word Representations in Vector Space (Word2Vec)](https://arxiv.org/abs/1301.3781)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- [Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

### æŠ€æœ¯æ–‡æ¡£
- [PyTorch Embeddingå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
- [HuggingFace Tokenizersåº“](https://huggingface.co/docs/tokenizers/)
- [Gensim Word2Vecå®ç°](https://radimrehurek.com/gensim/models/word2vec.html)
- [OpenAI CLIPæ¨¡å‹](https://github.com/openai/CLIP)

### å®è·µæ•™ç¨‹
- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)
- [Sebastian Ruder: EmbeddingæŠ€æœ¯ç»¼è¿°](https://ruder.io/word-embeddings-1/)
- [Lilian Weng: Attentionæœºåˆ¶è¯¦è§£](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Jay Alammar: The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

### å·¥å…·ä¸æ¡†æ¶
- [Sentence Transformers](https://www.sbert.net/)ï¼šé«˜è´¨é‡å¥å­embedding
- [Faiss](https://github.com/facebookresearch/faiss)ï¼šé«˜æ•ˆå‘é‡æ£€ç´¢
- [Annoy](https://github.com/spotify/annoy)ï¼šè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
- [Weights & Biases](https://wandb.ai/)ï¼šembeddingå¯è§†åŒ–ä¸å®éªŒè·Ÿè¸ª

---

embeddingæŠ€æœ¯ä»ç®€å•çš„æŸ¥æ‰¾è¡¨å‘å±•åˆ°ä»Šå¤©çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œå†åˆ°å¤šæ¨¡æ€ç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œè§è¯äº†æ·±åº¦å­¦ä¹ åœ¨è¯­ä¹‰ç†è§£æ–¹é¢çš„å·¨å¤§è¿›æ­¥ã€‚ç†è§£embeddingçš„æœ¬è´¨â€”â€”å°†ç¦»æ•£ç¬¦å·æ˜ å°„ä¸ºè¿ç»­è¯­ä¹‰ç©ºé—´â€”â€”æ˜¯æŒæ¡ç°ä»£NLPã€æ¨èç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹çš„å…³é”®ã€‚

åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œembeddingä¸ä»…æ˜¯è¾“å…¥å±‚çš„æŠ€æœ¯ç»†èŠ‚ï¼Œæ›´æ˜¯è¿æ¥ä¸åŒæ¨¡æ€ã€å®ç°é›¶æ ·æœ¬å­¦ä¹ ã€æ”¯æ’‘RAGåº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­å¢å¤§ï¼Œembeddingçš„ä¼˜åŒ–ï¼ˆå¦‚é‡åŒ–ã€åˆ†å¸ƒå¼å­˜å‚¨ã€ç¨€ç–æ›´æ–°ï¼‰ä¹Ÿæˆä¸ºå·¥ç¨‹å®è·µçš„é‡è¦è€ƒé‡ã€‚

ä½ æ˜¯å¦æ€è€ƒè¿‡ï¼Œåœ¨AGIï¼ˆé€šç”¨äººå·¥æ™ºèƒ½ï¼‰æ—¶ä»£ï¼Œembeddingå¦‚ä½•ç»Ÿä¸€æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰æ‰€æœ‰æ¨¡æ€çš„è¯­ä¹‰è¡¨ç¤ºï¼Ÿåœ¨ä½ çš„å…·ä½“ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œå¦‚ä½•è®¾è®¡æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„embeddingç­–ç•¥ï¼Ÿè¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è®¸å°±æ˜¯ä¸‹ä¸€ä¸ªæŠ€æœ¯çªç ´çš„èµ·ç‚¹ã€‚

---

*å¸Œæœ›è¿™ä¸ªå®Œæ•´çš„embeddingç†è®ºä¸LSTMå®è·µç›¸ç»“åˆçš„æŒ‡å—èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼*
