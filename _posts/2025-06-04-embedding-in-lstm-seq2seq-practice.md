---
layout: post
title: "ä»ç†è®ºåˆ°å®è·µï¼šæ·±åº¦è§£æEmbeddingåœ¨LSTM Seq2Seqæ¨¡å‹ä¸­çš„åº”ç”¨"
date: 2025-06-04 20:00:00 +0800
categories: [äººå·¥æ™ºèƒ½, æ·±åº¦å­¦ä¹ ]
tags: [embedding, lstm, seq2seq, æœºå™¨ç¿»è¯‘, pytorch, nlp]
author: Yonghui Zuo
description: "é€šè¿‡å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨ä»£ç å®ç°ï¼Œæ·±åº¦å‰–æembeddingåœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨æœºåˆ¶ã€è®­ç»ƒè¿‡ç¨‹å’Œå®è·µæŠ€å·§"
pin: false
math: true
mermaid: true
image:
  path: https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png
  alt: "LSTMå•å…ƒç»“æ„ç¤ºæ„å›¾ - å±•ç¤ºäº†LSTMçš„æ ¸å¿ƒç»„ä»¶å’Œä¿¡æ¯æµ"
---

# Embeddingæ·±åº¦è§£æï¼šä»"æŸ¥æ‰¾è¡¨"åˆ°æ·±åº¦å­¦ä¹ çš„è¯­ä¹‰æ¡¥æ¢

embeddingï¼ˆåµŒå…¥ï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ã€‚ä»Word2Vecåˆ°BERTï¼Œä»æ¨èç³»ç»Ÿåˆ°å›¾ç¥ç»ç½‘ç»œï¼Œä»ä¼ ç»ŸNLPåˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œembeddingæ— å¤„ä¸åœ¨ã€‚ç„¶è€Œï¼Œå¾ˆå¤šäººå¯¹å…¶æœ¬è´¨ç†è§£å¹¶ä¸æ·±åˆ»ã€‚æœ¬æ–‡å°†é€šè¿‡å…·ä½“çš„LSTM Seq2Seqä»£ç ç¤ºä¾‹ï¼Œæ·±åº¦å‰–æembeddingçš„æœ¬è´¨ã€å®ç°æœºåˆ¶ã€æ•°å­¦åŸç†å’Œå·¥ç¨‹å®è·µï¼Œå¹¶ç»“åˆç°ä»£å¤§æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œå¸®åŠ©ä½ å»ºç«‹ç³»ç»Ÿæ€§çš„è®¤çŸ¥ã€‚

## å®Œæ•´ä»£ç å®ç°

ä»¥ä¸‹æ˜¯æœ¬æ–‡å°†è¦æ·±åº¦åˆ†æçš„å®Œæ•´LSTMç¼–ç å™¨-è§£ç å™¨å®ç°ä»£ç ï¼ˆ276è¡Œï¼‰ï¼Œæ¶µç›–è¯æ±‡è¡¨æ„å»ºã€ç¼–ç å™¨ã€è§£ç å™¨ã€è®­ç»ƒæµç¨‹å’Œæµ‹è¯•ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

class Vocabulary:
    """è¯æ±‡è¡¨ç±»ï¼Œç”¨äºæ–‡æœ¬å’Œæ•°å­—ä¹‹é—´çš„è½¬æ¢"""
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '< SOS >': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '< SOS >', 2: '<EOS>', 3: '<UNK>'}
        self.vocab_size = 4
    
    def add_word(self, word):
        if word not in self.word2idx:
            self.word2idx[word] = self.vocab_size
            self.idx2word[self.vocab_size] = word
            self.vocab_size += 1
    
    def add_sentence(self, sentence):
        for word in sentence.split():
            self.add_word(word)
    
    def sentence_to_indices(self, sentence):
        return [self.word2idx.get(word, self.word2idx['<UNK>']) 
                for word in sentence.split()]
    
    def indices_to_sentence(self, indices):
        return ' '.join([self.idx2word[idx] for idx in indices 
                        if idx not in [0, 1, 2]])  # æ’é™¤ç‰¹æ®Šæ ‡è®°

class LSTMEncoder(nn.Module):
    """LSTMç¼–ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        # LSTMå±‚
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, bidirectional=False)
        
    def forward(self, input_seq, input_lengths):
        # input_seq: [batch_size, seq_len]
        batch_size = input_seq.size(0)
        
        # è¯åµŒå…¥
        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]
        
        # æ‰“åŒ…åºåˆ—ä»¥å¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, input_lengths, batch_first=True, enforce_sorted=False)
        
        # LSTMå‰å‘ä¼ æ’­
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # è§£åŒ…åºåˆ—
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        # è¿”å›æœ€åçš„éšçŠ¶æ€ä½œä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤º
        # hidden: [num_layers, batch_size, hidden_size]
        # æˆ‘ä»¬å–æœ€åä¸€å±‚çš„éšçŠ¶æ€
        context_vector = hidden[-1]  # [batch_size, hidden_size]
        
        return context_vector, (hidden, cell)

class LSTMDecoder(nn.Module):
    """LSTMè§£ç å™¨"""
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super(LSTMDecoder, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, encoder_hidden, target_seq=None, max_length=20):
        batch_size = encoder_hidden[0].size(1)
        
        if target_seq is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ç›®æ ‡åºåˆ—
            embedded = self.embedding(target_seq)
            output, _ = self.lstm(embedded, encoder_hidden)
            output = self.output_projection(output)
            return output
        else:
            # æ¨ç†æ¨¡å¼ï¼šé€æ­¥ç”Ÿæˆ
            outputs = []
            hidden = encoder_hidden
            input_token = torch.ones(batch_size, 1, dtype=torch.long) * 1  # < SOS >
            
            for _ in range(max_length):
                embedded = self.embedding(input_token)
                output, hidden = self.lstm(embedded, hidden)
                output = self.output_projection(output)
                
                # å–æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
                input_token = output.argmax(dim=-1)
                outputs.append(output)
                
                # å¦‚æœç”Ÿæˆäº†<EOS>ï¼Œæå‰åœæ­¢
                if (input_token == 2).all():
                    break
            
            return torch.cat(outputs, dim=1)

class Seq2SeqModel(nn.Module):
    """åºåˆ—åˆ°åºåˆ—æ¨¡å‹"""
    def __init__(self, encoder, decoder):
        super(Seq2SeqModel, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, input_seq, input_lengths, target_seq=None, max_length=20):
        # ç¼–ç é˜¶æ®µï¼šå°†è¾“å…¥åºåˆ—å‹ç¼©æˆå›ºå®šé•¿åº¦å‘é‡
        context_vector, encoder_hidden = self.encoder(input_seq, input_lengths)
        
        print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")
        print(f"ä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context_vector.shape}")
        print(f"ç¼–ç å™¨éšçŠ¶æ€å½¢çŠ¶: {encoder_hidden[0].shape}")
        
        # è§£ç é˜¶æ®µï¼šåŸºäºä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—
        output = self.decoder(encoder_hidden, target_seq, max_length)
        
        return output, context_vector

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    # ä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ç¤ºä¾‹
    data_pairs = [
        ("æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "I love natural language processing"),
        ("ä»Šå¤© å¤©æ°” å¾ˆ å¥½", "Today weather is good"),
        ("æœºå™¨ å­¦ä¹  å¾ˆ æœ‰è¶£", "Machine learning is interesting"),
        ("æ·±åº¦ å­¦ä¹  å¾ˆ å¼ºå¤§", "Deep learning is powerful"),
        ("äººå·¥ æ™ºèƒ½ æ”¹å˜ ä¸–ç•Œ", "AI changes the world"),
    ]
    return data_pairs

class TranslationDataset(Dataset):
    """ç¿»è¯‘æ•°æ®é›†"""
    def __init__(self, data_pairs, src_vocab, tgt_vocab):
        self.data_pairs = data_pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        src_sentence, tgt_sentence = self.data_pairs[idx]
        
        src_indices = self.src_vocab.sentence_to_indices(src_sentence)
        tgt_indices = [1] + self.tgt_vocab.sentence_to_indices(tgt_sentence) + [2]  # æ·»åŠ < SOS >å’Œ<EOS>
        
        return torch.tensor(src_indices), torch.tensor(tgt_indices)

def collate_fn(batch):
    """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
    src_batch, tgt_batch = zip(*batch)
    
    # è®¡ç®—åºåˆ—é•¿åº¦
    src_lengths = [len(seq) for seq in src_batch]
    tgt_lengths = [len(seq) for seq in tgt_batch]
    
    # å¡«å……åºåˆ—
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    
    return src_batch, torch.tensor(src_lengths), tgt_batch, torch.tensor(tgt_lengths)

def main():
    # 1. å‡†å¤‡æ•°æ®
    print("=" * 50)
    print("å‡†å¤‡æ•°æ®...")
    data_pairs = create_sample_data()
    
    # æ„å»ºè¯æ±‡è¡¨
    src_vocab = Vocabulary()
    tgt_vocab = Vocabulary()
    
    for src, tgt in data_pairs:
        src_vocab.add_sentence(src)
        tgt_vocab.add_sentence(tgt)
    
    print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {src_vocab.vocab_size}")
    print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {tgt_vocab.vocab_size}")
    
    # 2. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = TranslationDataset(data_pairs, src_vocab, tgt_vocab)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    
    # 3. å®šä¹‰æ¨¡å‹å‚æ•°
    print("\n" + "=" * 50)
    print("åˆå§‹åŒ–æ¨¡å‹...")
    embed_size = 64
    hidden_size = 128
    num_layers = 1
    
    # 4. åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
    encoder = LSTMEncoder(src_vocab.vocab_size, embed_size, hidden_size, num_layers)
    decoder = LSTMDecoder(tgt_vocab.vocab_size, embed_size, hidden_size, num_layers)
    model = Seq2SeqModel(encoder, decoder)
    
    # 5. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 6. è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    for epoch in range(50):
        total_loss = 0
        for batch_idx, (src_batch, src_lengths, tgt_batch, tgt_lengths) in enumerate(dataloader):
            optimizer.zero_grad()
            
            # å‡†å¤‡ç›®æ ‡åºåˆ—ï¼ˆç”¨äºteacher forcingï¼‰
            decoder_input = tgt_batch[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtoken
            decoder_target = tgt_batch[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªtoken(< SOS >)
            
            # å‰å‘ä¼ æ’­
            output, context_vector = model(src_batch, src_lengths, decoder_input)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output.reshape(-1, output.size(-1)), 
                           decoder_target.reshape(-1))
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/50], Loss: {total_loss/len(dataloader):.4f}')
    
    # 7. æµ‹è¯•æ¨¡å‹
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ¨¡å‹...")
    model.eval()
    
    test_sentences = ["æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†", "ä»Šå¤© å¤©æ°” å¾ˆ å¥½"]
    
    with torch.no_grad():
        for test_sentence in test_sentences:
            print(f"\nè¾“å…¥: {test_sentence}")
            
            # å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•
            src_indices = src_vocab.sentence_to_indices(test_sentence)
            src_tensor = torch.tensor(src_indices).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            src_length = torch.tensor([len(src_indices)])
            
            # ç”Ÿæˆç¿»è¯‘
            output, context_vector = model(src_tensor, src_length, max_length=10)
            
            # å°†è¾“å‡ºè½¬æ¢ä¸ºå•è¯
            predicted_indices = output.argmax(dim=-1).squeeze(0).tolist()
            predicted_sentence = tgt_vocab.indices_to_sentence(predicted_indices)
            
            print(f"è¾“å‡º: {predicted_sentence}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦: {context_vector.shape}")
            print(f"ä¸Šä¸‹æ–‡å‘é‡å€¼: {context_vector.squeeze().numpy()[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼

if __name__ == "__main__":
    main()
```

### ğŸ” ä»£ç è¦ç‚¹å›é¡¾

ä¸Šé¢çš„å®Œæ•´ä»£ç å®ç°æ¶µç›–äº†ä»¥ä¸‹æ ¸å¿ƒéƒ¨åˆ†ï¼š

- **Vocabularyç±»**ï¼šè¯æ±‡è¡¨ç®¡ç†å’Œç´¢å¼•è½¬æ¢
- **LSTMEncoderç±»**ï¼šç¼–ç å™¨å®ç°ï¼ŒåŒ…å«embeddingå±‚å’ŒLSTMå±‚
- **LSTMDecoderç±»**ï¼šè§£ç å™¨å®ç°ï¼Œæ”¯æŒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼
- **Seq2SeqModelç±»**ï¼šå®Œæ•´çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹
- **æ•°æ®å¤„ç†**ï¼šDatasetã€DataLoaderå’Œæ‰¹å¤„ç†å‡½æ•°
- **è®­ç»ƒå¾ªç¯**ï¼šå®Œæ•´çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹

## 1. æ ¸å¿ƒæ¦‚å¿µæ·±åº¦è§£æ

### 1.1 embeddingçš„æœ¬è´¨ï¼šå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨

æƒ³è±¡ä¸€æœ¬ç‰¹æ®Šçš„å­—å…¸ï¼šä¼ ç»Ÿå­—å…¸ç»™å‡ºè¯æ±‡çš„æ–‡å­—è§£é‡Šï¼Œè€Œembeddingå­—å…¸ç»™å‡ºçš„æ˜¯æ•°å­—å‘é‡ã€‚æ›´ç¥å¥‡çš„æ˜¯ï¼Œè¿™æœ¬å­—å…¸ä¼š"è‡ªå­¦ä¹ "â€”â€”é€šè¿‡ä¸æ–­è®­ç»ƒï¼Œè®©è¯­ä¹‰ç›¸ä¼¼çš„è¯æ±‡æ‹¥æœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚

è®©æˆ‘ä»¬çœ‹çœ‹LSTMä»£ç ä¸­çš„å…·ä½“å®ç°ï¼š

```python
# æ¥è‡ªencoder_decoder_lstm.pyç¬¬44è¡Œ
self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
```

è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸º`[vocab_size, embed_size]`çš„æŸ¥æ‰¾è¡¨ã€‚æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºï¼š

```python
# å‡è®¾vocab_size=1000, embed_size=64
# embedding.weightçš„å½¢çŠ¶æ˜¯[1000, 64]
# è¯ç´¢å¼•5çš„å‘é‡å°±æ˜¯embedding.weight[5]ï¼Œæ˜¯ä¸€ä¸ª64ç»´å‘é‡
```

### 1.2 æŸ¥æ‰¾è¡¨vsä¼ ç»Ÿç¼–ç 

ä¼ ç»Ÿçš„one-hotç¼–ç åƒæ˜¯"èº«ä»½è¯å·ç "â€”â€”æ¯ä¸ªè¯æœ‰å”¯ä¸€æ ‡è¯†ï¼Œä½†å½¼æ­¤æ²¡æœ‰å…³ç³»ï¼š

```python
# one-hotç¼–ç ç¤ºä¾‹ï¼ˆå‡è®¾è¯è¡¨å¤§å°ä¸º5ï¼‰
"æˆ‘"   â†’ [1, 0, 0, 0, 0]
"çˆ±"   â†’ [0, 1, 0, 0, 0]  
"è‡ªç„¶" â†’ [0, 0, 1, 0, 0]
# ä»»æ„ä¸¤ä¸ªè¯çš„è·ç¦»éƒ½ç›¸ç­‰ï¼Œæ— æ³•è¡¨è¾¾è¯­ä¹‰å…³ç³»
```

è€Œembeddingæ˜¯"è¯­ä¹‰åæ ‡"â€”â€”æŠŠè¯æ”¾åœ¨ä¸€ä¸ªè¿ç»­ç©ºé—´ä¸­ï¼Œç›¸ä¼¼çš„è¯è·ç¦»æ›´è¿‘ï¼š

```python
# embeddingç¤ºä¾‹ï¼ˆå‡è®¾64ç»´ï¼‰
"æˆ‘"   â†’ [0.2, -0.1, 0.5, ..., 0.3]  # 64ç»´å‘é‡
"ä½ "   â†’ [0.3, -0.2, 0.4, ..., 0.2]  # ä¸"æˆ‘"ç›¸ä¼¼ï¼Œå‘é‡æ¥è¿‘
"è‹¹æœ" â†’ [-0.8, 0.9, -0.3, ..., 0.7] # ä¸"æˆ‘"ä¸åŒï¼Œå‘é‡å·®å¼‚å¤§
```

### 1.3 å…³é”®æœ¯è¯­è¯´æ˜
- **Embeddingå±‚ï¼ˆnn.Embeddingï¼‰**ï¼šPyTorchä¸­å®ç°æŸ¥æ‰¾è¡¨çš„æ¨¡å—
- **weight**ï¼šembeddingå±‚çš„æ ¸å¿ƒå‚æ•°ï¼Œå­˜å‚¨æ‰€æœ‰è¯å‘é‡çš„çŸ©é˜µ
- **padding_idx**ï¼šæŒ‡å®šå“ªä¸ªç´¢å¼•ä¿æŒé›¶å‘é‡ï¼ˆé€šå¸¸æ˜¯0ï¼Œè¡¨ç¤ºå¡«å……ç¬¦ï¼‰
- **ç¨ å¯†å‘é‡ï¼ˆDense Vectorï¼‰**ï¼šembeddingäº§ç”Ÿçš„è¿ç»­ã€ä½ç»´å‘é‡
- **ç¨€ç–å‘é‡ï¼ˆSparse Vectorï¼‰**ï¼šone-hotè¿™æ ·çš„é«˜ç»´ã€å¤§éƒ¨åˆ†ä¸º0çš„å‘é‡

### 1.4 æ•°å­¦åŸç†ä¸ç­‰ä»·æ€§æ¨å¯¼

#### ç¦»æ•£ç©ºé—´åˆ°è¿ç»­ç©ºé—´çš„æ˜ å°„
è®¾è¯è¡¨å¤§å°ä¸º$N$ï¼Œembeddingç»´åº¦ä¸º$d$ã€‚embeddingå±‚çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå‚æ•°çŸ©é˜µï¼š

$$E \in \mathbb{R}^{N \times d}$$

æŸ¥æ‰¾æ“ä½œçš„æ•°å­¦è¡¨è¾¾ï¼š
$$f: \{0, 1, 2, ..., N-1\} \rightarrow \mathbb{R}^d$$
$$f(i) = E[i] \text{ ï¼ˆç¬¬iè¡Œå‘é‡ï¼‰}$$

#### one-hot + çº¿æ€§å±‚çš„ç­‰ä»·æ€§è¯æ˜

one-hotå‘é‡$\mathbf{x}_i \in \mathbb{R}^N$ï¼Œåªæœ‰ç¬¬$i$ä¸ªä½ç½®ä¸º1ï¼š
$$\mathbf{x}_i = [0, 0, ..., 1, ..., 0]^T$$

çº¿æ€§å±‚æƒé‡$W \in \mathbb{R}^{N \times d}$ï¼Œè¾“å‡ºä¸ºï¼š
$$\mathbf{y} = \mathbf{x}_i^T W = W[i]$$

è¿™ä¸embeddingæŸ¥è¡¨$E[i]$å®Œå…¨ç­‰ä»·ï¼å› æ­¤ï¼š
> **embeddingå±‚ = one-hotç¼–ç  + çº¿æ€§å±‚ï¼ˆæ— biasï¼‰**

è®©æˆ‘ä»¬ç”¨ä»£ç éªŒè¯è¿™ä¸ªç­‰ä»·æ€§ï¼š

```python
import torch
import torch.nn as nn

vocab_size, embed_size = 1000, 64
word_idx = 42

# æ–¹æ³•1ï¼šembeddingæŸ¥æ‰¾
embedding = nn.Embedding(vocab_size, embed_size)
result1 = embedding(torch.tensor(word_idx))

# æ–¹æ³•2ï¼šone-hot + çº¿æ€§å±‚
one_hot = torch.zeros(vocab_size)
one_hot[word_idx] = 1
linear = nn.Linear(vocab_size, embed_size, bias=False)
linear.weight.data = embedding.weight.data.T  # å…±äº«æƒé‡
result2 = linear(one_hot)

print(torch.allclose(result1, result2))  # True
```

#### embeddingæ¢¯åº¦è®¡ç®—çš„æ·±åº¦æ¨å¯¼

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œembedding.weightçš„æ¢¯åº¦æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡é“¾å¼æ³•åˆ™æ¥ç†è§£ï¼š

è®¾æŸå¤±å‡½æ•°ä¸º$L$ï¼Œå¯¹äºè¯ç´¢å¼•$i$ï¼Œå…¶embeddingå‘é‡$\mathbf{e}_i = E[i]$ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼š

$$\frac{\partial L}{\partial E[i]} = \frac{\partial L}{\partial \mathbf{e}_i}$$

**å…³é”®æ´å¯Ÿ**ï¼šåªæœ‰åœ¨å½“å‰batchä¸­å‡ºç°çš„è¯ç´¢å¼•ï¼Œå…¶å¯¹åº”çš„embeddingè¡Œæ‰ä¼šæœ‰éé›¶æ¢¯åº¦ã€‚

```python
# æ¢¯åº¦è®¡ç®—ç¤ºä¾‹
def demonstrate_embedding_gradients():
    embedding = nn.Embedding(5, 3)
    embedding.weight.data.fill_(1.0)  # åˆå§‹åŒ–ä¸º1ä¾¿äºè§‚å¯Ÿ
    
    # è¾“å…¥åºåˆ—ï¼šç´¢å¼•[1, 2, 1]ï¼Œæ³¨æ„ç´¢å¼•1å‡ºç°2æ¬¡
    input_seq = torch.tensor([1, 2, 1])
    embedded = embedding(input_seq)
    
    # ç®€å•æŸå¤±ï¼šæ‰€æœ‰embeddingå‘é‡å…ƒç´ å’Œ
    loss = embedded.sum()
    loss.backward()
    
    print("æ¢¯åº¦åˆ†å¸ƒ:")
    for i in range(5):
        grad = embedding.weight.grad[i]
        print(f"ç´¢å¼•{i}: {grad} (å‡ºç°æ¬¡æ•°: {(input_seq == i).sum().item()})")
    
    # è¾“å‡ºæ˜¾ç¤ºï¼šç´¢å¼•1çš„æ¢¯åº¦æ˜¯ç´¢å¼•2çš„2å€ï¼ˆå› ä¸ºå‡ºç°2æ¬¡ï¼‰

demonstrate_embedding_gradients()
```

### 1.5 embeddingçš„æ•°æ®å†™å…¥æœºåˆ¶

#### ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åŒ–å†™å…¥
PyTorchçš„embeddingåˆå§‹åŒ–ç­–ç•¥åŠå…¶å½±å“ï¼š

```python
# é»˜è®¤åˆå§‹åŒ–ï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒN(0,1)
embedding_default = nn.Embedding(vocab_size, embed_size)

# Xavier/Glorotåˆå§‹åŒ–ï¼ˆæ›´é€‚åˆæŸäº›æ¿€æ´»å‡½æ•°ï¼‰
embedding_xavier = nn.Embedding(vocab_size, embed_size)
nn.init.xavier_uniform_(embedding_xavier.weight)

# Kaimingåˆå§‹åŒ–ï¼ˆé€‚åˆReLUæ¿€æ´»å‡½æ•°ï¼‰
embedding_kaiming = nn.Embedding(vocab_size, embed_size)
nn.init.kaiming_uniform_(embedding_kaiming.weight)

# æ¥è‡ªLSTMä»£ç çš„å®é™…ä¾‹å­
encoder = LSTMEncoder(src_vocab.vocab_size, embed_size, hidden_size, num_layers)
# æ­¤æ—¶embedding.weightå·²è¢«éšæœºåˆå§‹åŒ–
```

ç‰¹æ®Šå¤„ç†paddingï¼š
```python
# padding_idx=0çš„ä½ç½®ä¼šè¢«å¼ºåˆ¶è®¾ä¸ºé›¶å‘é‡
self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
# embedding.weight[0] = [0, 0, 0, ..., 0]
```

#### ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ›´æ–°å†™å…¥
åœ¨LSTMçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œembeddingæƒé‡é€šè¿‡åå‘ä¼ æ’­æ›´æ–°ï¼š

```python
# æ¥è‡ªencoder_decoder_lstm.pyç¬¬52è¡Œ
embedded = self.embedding(input_seq)  # å‰å‘ä¼ æ’­ï¼šæŸ¥è¡¨

# è®­ç»ƒå¾ªç¯ï¼ˆç¬¬241-249è¡Œï¼‰
loss.backward()      # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
optimizer.step()     # å‚æ•°æ›´æ–°ï¼šä¿®æ”¹embedding.weight
```

**ç¨€ç–æ›´æ–°çš„æ•°å­¦è¡¨ç¤º**ï¼š
è®¾batchä¸­å‡ºç°çš„è¯ç´¢å¼•é›†åˆä¸º$\mathcal{B}$ï¼Œåˆ™åªæœ‰è¿™äº›ç´¢å¼•å¯¹åº”çš„embeddingè¡Œä¼šè¢«æ›´æ–°ï¼š

$$E[i]_{t+1} = \begin{cases}
E[i]_t - \eta \nabla_{E[i]} L & \text{if } i \in \mathcal{B} \\
E[i]_t & \text{if } i \notin \mathcal{B}
\end{cases}$$

#### ç¬¬ä¸‰é˜¶æ®µï¼šæ‰‹åŠ¨å†™å…¥ï¼ˆå¯é€‰ï¼‰
æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¾ç½®embeddingæƒé‡ï¼Œå¸¸ç”¨äºåŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼š

```python
# åŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼ˆå¦‚Word2Vecã€GloVeï¼‰
pretrained_embeddings = load_pretrained_vectors()
model.embedding.weight.data = pretrained_embeddings

# æˆ–è€…éƒ¨åˆ†æ›¿æ¢
model.embedding.weight.data[word_idx] = custom_vector
```

### 1.6 embeddingå±‚çš„å®Œæ•´å‰–æ

embeddingå±‚ä¸ä»…ä»…æ˜¯weightï¼Œè¿˜åŒ…å«å¤šä¸ªé‡è¦å±æ€§ï¼š

```python
embedding = nn.Embedding(
    num_embeddings=10000,    # è¯è¡¨å¤§å°
    embedding_dim=300,       # å‘é‡ç»´åº¦  
    padding_idx=0,          # paddingç´¢å¼•ï¼Œè¯¥è¡Œä¸å‚ä¸è®­ç»ƒ
    max_norm=None,          # å‘é‡èŒƒæ•°çº¦æŸ
    norm_type=2.0,          # èŒƒæ•°ç±»å‹
    scale_grad_by_freq=False, # æ˜¯å¦æŒ‰è¯é¢‘ç¼©æ”¾æ¢¯åº¦
    sparse=False            # æ˜¯å¦ä½¿ç”¨ç¨€ç–æ›´æ–°
)

# æ ¸å¿ƒå‚æ•°ï¼šåªæœ‰weightæ˜¯å¯è®­ç»ƒçš„
print(list(embedding.parameters()))  # åªæœ‰embedding.weight
```

#### è´Ÿé‡‡æ ·å¯¹embeddingè´¨é‡çš„å½±å“

åœ¨Word2Vecç­‰æ¨¡å‹ä¸­ï¼Œè´Ÿé‡‡æ ·ç­–ç•¥æ˜¾è‘—å½±å“embeddingè´¨é‡ï¼š

```python
# åŸºäºè¯é¢‘çš„è´Ÿé‡‡æ ·æ¦‚ç‡
def negative_sampling_probability(word_freq, total_freq, power=0.75):
    """è®¡ç®—è´Ÿé‡‡æ ·æ¦‚ç‡"""
    return (word_freq / total_freq) ** power

# é«˜é¢‘è¯è¢«é‡‡æ ·ä¸ºè´Ÿæ ·æœ¬çš„æ¦‚ç‡æ›´é«˜ï¼Œæœ‰åŠ©äºå­¦ä¹ æ›´å¥½çš„embedding
```

### 1.7 å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹æ®Šembeddingæœºåˆ¶

#### Token Embeddingã€Position Embeddingã€Segment Embedding
åœ¨ç°ä»£LLMï¼ˆå¦‚BERTã€GPTï¼‰ä¸­ï¼Œembeddingä¸ä»…ä»…æ˜¯è¯åµŒå…¥ï¼š

```python
class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_seq_len, dropout=0.1):
        super().__init__()
        # Token embeddingï¼šå°†è¯æ±‡æ˜ å°„ä¸ºå‘é‡
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # Position embeddingï¼šç¼–ç ä½ç½®ä¿¡æ¯
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Segment embeddingï¼šåŒºåˆ†ä¸åŒæ®µè½ï¼ˆå¦‚BERTä¸­çš„å¥å­A/Bï¼‰
        self.segment_embedding = nn.Embedding(2, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, tokens, segments=None):
        seq_len = tokens.size(1)
        positions = torch.arange(seq_len).unsqueeze(0)
        
        # ä¸‰ç§embeddingç›¸åŠ 
        embeddings = self.token_embedding(tokens)
        embeddings += self.position_embedding(positions)
        
        if segments is not None:
            embeddings += self.segment_embedding(segments)
            
        return self.dropout(embeddings)
```

#### æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šç°ä»£LLMçš„æ–°è¶‹åŠ¿

```python
def apply_rotary_pos_emb(x, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    # x: [batch_size, seq_len, num_heads, head_dim]
    x1, x2 = x[..., ::2], x[..., 1::2]
    
    # æ—‹è½¬æ“ä½œ
    rotated = torch.stack([-x2, x1], dim=-1).flatten(-2)
    return x * cos + rotated * sin

# RoPEé€šè¿‡æ—‹è½¬è€ŒéåŠ æ³•çš„æ–¹å¼ç¼–ç ä½ç½®ï¼Œåœ¨é•¿åºåˆ—ä¸Šè¡¨ç°æ›´å¥½
```

### 1.8 LSTMä¸­çš„embeddingå®é™…å·¥ä½œæµç¨‹

è®©æˆ‘ä»¬è¿½è¸ªLSTMä»£ç ä¸­embeddingçš„å®Œæ•´æ•°æ®æµï¼š

```python
# 1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆç¬¬27-29è¡Œï¼‰
src_sentence = "æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"
src_indices = src_vocab.sentence_to_indices(src_sentence)
# ç»“æœï¼š[4, 5, 6, 7, 8] ï¼ˆå‡è®¾çš„ç´¢å¼•ï¼‰

# 2. è½¬æ¢ä¸ºtensorï¼ˆç¬¬52è¡Œï¼‰
input_seq = torch.tensor([[4, 5, 6, 7, 8]])  # [batch_size=1, seq_len=5]

# 3. embeddingæŸ¥è¡¨ï¼ˆç¬¬52è¡Œï¼‰
embedded = self.embedding(input_seq)
# å½¢çŠ¶ï¼š[1, 5, 64] ï¼ˆbatch_size, seq_len, embed_sizeï¼‰
# æ¯ä¸ªè¯è¢«æ›¿æ¢ä¸º64ç»´å‘é‡

# 4. LSTMå¤„ç†embeddedå‘é‡ï¼Œè€Œä¸æ˜¯åŸå§‹ç´¢å¼•
```

## 2. æ·±åº¦ç†è§£ä¸å¸¸è§è¯¯åŒº

### 2.1 æ ¸å¿ƒæ¦‚å¿µè¾¨æ

#### è¯¯åŒº1ï¼š"one-hotå‘é‡ä¹Ÿæ˜¯embedding"
**æ­£ç¡®ç†è§£**ï¼šone-hotåªæ˜¯ç¼–ç æ–¹å¼ï¼Œä¸æ˜¯embeddingã€‚embeddingç‰¹æŒ‡ï¼š
- ç¨ å¯†ï¼ˆdenseï¼‰å‘é‡
- å¯å­¦ä¹ ï¼ˆlearnableï¼‰å‚æ•°  
- æœ‰è¯­ä¹‰ç»“æ„ï¼ˆsemantic structureï¼‰

#### è¯¯åŒº2ï¼š"è®­ç»ƒæ—¶embeddingå±‚è¢«æ›´æ–°"
**æ­£ç¡®ç†è§£**ï¼šè¢«æ›´æ–°çš„æ˜¯embedding.weightå‚æ•°ï¼Œembeddingå±‚æœ¬èº«åªæ˜¯"å®¹å™¨"ï¼š

```python
# é”™è¯¯è¯´æ³•ï¼š"embeddingå±‚è¢«è®­ç»ƒ"
# æ­£ç¡®è¯´æ³•ï¼š"embedding.weightå‚æ•°è¢«è®­ç»ƒ"

for name, param in model.named_parameters():
    if 'embedding' in name:
        print(f"{name}: {param.shape}")
        # è¾“å‡ºï¼šencoder.embedding.weight: torch.Size([15, 64])
```

#### è¯¯åŒº3ï¼š"è¯å‘é‡"å’Œ"embedding.weight"æ˜¯ä¸åŒçš„ä¸œè¥¿  
**æ­£ç¡®ç†è§£**ï¼šæˆ‘ä»¬å¸¸è¯´çš„"è¯å‘é‡"å°±æ˜¯embedding.weightçš„æ¯ä¸€è¡Œï¼š

```python
# "æˆ‘"çš„è¯å‘é‡ = embedding.weight[word_idx]
word_vector = model.encoder.embedding.weight[4]  # å‡è®¾"æˆ‘"çš„ç´¢å¼•æ˜¯4
print(f"'æˆ‘'çš„è¯å‘é‡ç»´åº¦: {word_vector.shape}")  # [64]
```

### 2.2 æ€§èƒ½ä¸å·¥ç¨‹è€ƒé‡

#### å†…å­˜æ•ˆç‡å¯¹æ¯”
```python
vocab_size = 50000
embed_size = 300

# one-hotæ–¹å¼ï¼šæ¯ä¸ªè¯éœ€è¦50000ç»´
one_hot_memory = vocab_size * 4  # çº¦200KBæ¯ä¸ªè¯

# embeddingæ–¹å¼ï¼šæ¯ä¸ªè¯åªéœ€è¦300ç»´
embedding_memory = embed_size * 4  # çº¦1.2KBæ¯ä¸ªè¯
# å†…å­˜èŠ‚çœï¼š50000/300 â‰ˆ 167å€
```

#### è®¡ç®—æ•ˆç‡å¯¹æ¯”
```python
# one-hot + çº¿æ€§å±‚ï¼šéœ€è¦çŸ©é˜µä¹˜æ³•
# æ—¶é—´å¤æ‚åº¦ï¼šO(vocab_size * embed_size)

# embeddingæŸ¥è¡¨ï¼šç›´æ¥ç´¢å¼•è®¿é—®  
# æ—¶é—´å¤æ‚åº¦ï¼šO(1)
```

### 2.3 å¤§è§„æ¨¡embeddingçš„å·¥ç¨‹æŒ‘æˆ˜

#### åˆ†å¸ƒå¼embeddingä¼˜åŒ–
åœ¨å¤§å‹LLMè®­ç»ƒä¸­ï¼Œembeddingå±‚å¾€å¾€æ˜¯å‚æ•°æœ€å¤šçš„éƒ¨åˆ†ï¼š

```python
# GPT-3çš„embeddingå±‚å‚æ•°é‡ä¼°ç®—
vocab_size = 50257  # GPT-3è¯è¡¨å¤§å°
d_model = 12288     # GPT-3æœ€å¤§æ¨¡å‹ç»´åº¦
embedding_params = vocab_size * d_model  # çº¦6.2äº¿å‚æ•°

# åˆ†å¸ƒå¼ä¼˜åŒ–ç­–ç•¥ï¼š
# 1. å‚æ•°æœåŠ¡å™¨ï¼šå°†embeddingåˆ†ç‰‡å­˜å‚¨åœ¨ä¸åŒèŠ‚ç‚¹
# 2. æ¨¡å‹å¹¶è¡Œï¼šå°†embeddingæŒ‰è¯æ±‡æˆ–ç»´åº¦åˆ‡åˆ†
# 3. æ¢¯åº¦å‹ç¼©ï¼šä½¿ç”¨ä½ç²¾åº¦æˆ–ç¨€ç–æ¢¯åº¦é€šä¿¡
```

#### embeddingé‡åŒ–ä¸å‹ç¼©

```python
# é‡åŒ–embeddingä»¥èŠ‚çœå†…å­˜
def quantize_embedding(embedding_weight, bits=8):
    """å°†embeddingæƒé‡é‡åŒ–åˆ°æŒ‡å®šä½æ•°"""
    min_val, max_val = embedding_weight.min(), embedding_weight.max()
    scale = (max_val - min_val) / (2**bits - 1)
    
    quantized = torch.round((embedding_weight - min_val) / scale)
    return quantized.byte(), scale, min_val

# ä½ç§©åˆ†è§£å‡å°‘å‚æ•°é‡
def low_rank_embedding(vocab_size, embed_size, rank):
    """ä½¿ç”¨ä½ç§©åˆ†è§£å‡å°‘embeddingå‚æ•°"""
    return nn.Sequential(
        nn.Embedding(vocab_size, rank),
        nn.Linear(rank, embed_size, bias=False)
    )
```

#### é«˜çº§ç‰¹æ€§ä¸ä¼˜åŒ–

#### ç¨€ç–æ›´æ–°ä¼˜åŒ–
åœ¨å¤§è§„æ¨¡è¯è¡¨åœºæ™¯ä¸‹ï¼Œä½¿ç”¨ç¨€ç–æ›´æ–°å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ï¼š

```python
# å¯ç”¨ç¨€ç–æ›´æ–°
embedding = nn.Embedding(vocab_size, embed_size, sparse=True)
optimizer = torch.optim.SparseAdam(embedding.parameters())
```

#### å‘é‡èŒƒæ•°çº¦æŸ
é˜²æ­¢embeddingå‘é‡è¿‡å¤§ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ï¼š

```python
# çº¦æŸembeddingå‘é‡çš„L2èŒƒæ•°ä¸è¶…è¿‡1.0
embedding = nn.Embedding(vocab_size, embed_size, max_norm=1.0)
```

## 3. åº”ç”¨å®è·µä¸æœ€ä½³å®è·µ

### 3.1 LSTM Seq2Seqä¸­çš„embeddingåº”ç”¨

åœ¨æˆ‘ä»¬çš„æœºå™¨ç¿»è¯‘ç¤ºä¾‹ä¸­ï¼Œembeddingæ‰®æ¼”å…³é”®è§’è‰²ï¼š

```python
# ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ä½¿ç”¨embedding
class LSTMEncoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        
class LSTMDecoder(nn.Module):  
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
```

**è®¾è®¡è€ƒè™‘**ï¼š
- æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä½¿ç”¨ç‹¬ç«‹çš„embeddingï¼ˆæ”¯æŒä¸åŒè¯è¡¨ï¼‰
- ç›¸åŒçš„embed_sizeç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
- padding_idx=0å¤„ç†å˜é•¿åºåˆ—

### 3.2 embeddingç»´åº¦é€‰æ‹©æŒ‡å—

| æ•°æ®è§„æ¨¡ | æ¨èç»´åº¦ | è¯´æ˜ |
|---------|---------|------|
| å°å‹ï¼ˆ<1ä¸‡è¯æ±‡ï¼‰ | 32-128 | é¿å…è¿‡æ‹Ÿåˆ |
| ä¸­å‹ï¼ˆ1-10ä¸‡è¯æ±‡ï¼‰ | 128-512 | å¹³è¡¡è¡¨è¾¾åŠ›ä¸æ•ˆç‡ |
| å¤§å‹ï¼ˆ>10ä¸‡è¯æ±‡ï¼‰ | 300-1024 | å……åˆ†è¡¨è¾¾å¤æ‚è¯­ä¹‰ |
| LLMçº§åˆ«ï¼ˆ>5ä¸‡è¯æ±‡ï¼‰ | 1024-12288 | å¤§æ¨¡å‹æ ‡å‡†é…ç½® |

ä»£ç ç¤ºä¾‹ä¸­ä½¿ç”¨64ç»´ï¼Œé€‚åˆå°è§„æ¨¡æ¼”ç¤ºä»»åŠ¡ã€‚

### 3.3 å†·å¯åŠ¨ä¸OOVå¤„ç†

#### æœªç™»å½•è¯ï¼ˆOOVï¼‰å¤„ç†ç­–ç•¥
```python
def sentence_to_indices(self, sentence):
    return [self.word2idx.get(word, self.word2idx['<UNK>']) 
            for word in sentence.split()]
```

#### é¢„è®­ç»ƒembeddingåˆå§‹åŒ–
```python
def load_pretrained_embeddings(vocab, embedding_dim):
    """åŠ è½½é¢„è®­ç»ƒè¯å‘é‡åˆå§‹åŒ–embedding"""
    pretrained = {}  # ä»Word2Vec/GloVeæ–‡ä»¶åŠ è½½
    
    embedding_matrix = torch.randn(len(vocab), embedding_dim)
    for word, idx in vocab.word2idx.items():
        if word in pretrained:
            embedding_matrix[idx] = torch.tensor(pretrained[word])
    
    return embedding_matrix
```

### 3.4 embeddingåœ¨RAGä¸prompt engineeringä¸­çš„åº”ç”¨

#### å‘é‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰
```python
class RAGEmbedding(nn.Module):
    """ç”¨äºRAGçš„embeddingæ¨¡å—"""
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        super().__init__()
        from sentence_transformers import SentenceTransformer
        self.encoder = SentenceTransformer(model_name)
        
    def encode_documents(self, documents):
        """å°†æ–‡æ¡£ç¼–ç ä¸ºå‘é‡ç”¨äºæ£€ç´¢"""
        return self.encoder.encode(documents)
    
    def encode_query(self, query):
        """å°†æŸ¥è¯¢ç¼–ç ä¸ºå‘é‡"""
        return self.encoder.encode([query])

# åœ¨RAGä¸­ï¼Œembeddingç”¨äºï¼š
# 1. å°†çŸ¥è¯†åº“æ–‡æ¡£ç¼–ç ä¸ºå‘é‡å­˜å‚¨
# 2. å°†ç”¨æˆ·æŸ¥è¯¢ç¼–ç ä¸ºå‘é‡è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…
# 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£ä½œä¸ºLLMçš„ä¸Šä¸‹æ–‡
```

#### Promptå‘é‡åŒ–ä¸æ£€ç´¢
```python
def prompt_embedding_search(query_embedding, prompt_database):
    """åŸºäºembeddingçš„promptæ£€ç´¢"""
    similarities = []
    for prompt_embed in prompt_database:
        sim = torch.cosine_similarity(query_embedding, prompt_embed, dim=0)
        similarities.append(sim)
    
    # è¿”å›æœ€ç›¸ä¼¼çš„prompt
    best_idx = torch.argmax(torch.tensor(similarities))
    return best_idx, similarities[best_idx]
```

### 3.5 å¤šè¯­è¨€ä¸è·¨é¢†åŸŸåº”ç”¨

#### å…±äº«embeddingç­–ç•¥
å¯¹äºç›¸ä¼¼ä»»åŠ¡ï¼Œå¯ä»¥å…±äº«embeddingå‡å°‘å‚æ•°ï¼š

```python
# å…±äº«ç¼–ç å™¨å’Œè§£ç å™¨çš„embedding
shared_embedding = nn.Embedding(vocab_size, embed_size)

class SharedEmbeddingSeq2Seq(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        self.shared_embedding = nn.Embedding(vocab_size, embed_size)
        self.encoder = LSTMEncoder(vocab_size, embed_size, hidden_size)
        self.decoder = LSTMDecoder(vocab_size, embed_size, hidden_size)
        
        # å…±äº«embeddingæƒé‡
        self.encoder.embedding = self.shared_embedding
        self.decoder.embedding = self.shared_embedding
```

## 4. å‰æ²¿å‘å±•ä¸æŠ€æœ¯è¶‹åŠ¿

### 4.1 ä¸Šä¸‹æ–‡ç›¸å…³embedding

ä¼ ç»Ÿembeddingï¼ˆå¦‚Word2Vecï¼‰ç»™æ¯ä¸ªè¯å›ºå®šå‘é‡ï¼Œè€Œç°ä»£æ–¹æ³•ï¼ˆå¦‚BERTï¼‰ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„åŠ¨æ€å‘é‡ï¼š

```python
# é™æ€embeddingï¼šä¸€è¯ä¸€å‘é‡
word_vector = embedding(word_idx)  # å›ºå®šå‘é‡

# åŠ¨æ€embeddingï¼šä¸Šä¸‹æ–‡ç›¸å…³
contextualized_vector = bert(sentence)[word_position]  # éšä¸Šä¸‹æ–‡å˜åŒ–
```

### 4.2 å­è¯çº§embedding

è§£å†³OOVé—®é¢˜çš„åˆ©å™¨ï¼š

```python
# å­—ç¬¦çº§embedding
char_embedding = nn.Embedding(char_vocab_size, char_embed_size)

# å­è¯embeddingï¼ˆBPE/SentencePieceï¼‰
subword_embedding = nn.Embedding(subword_vocab_size, embed_size)
```

### 4.3 å¤šæ¨¡æ€embeddingï¼šCLIPæ¡ˆä¾‹æ·±åº¦è§£æ

CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰æ˜¯å¤šæ¨¡æ€embeddingçš„ç»å…¸æ¡ˆä¾‹ï¼š

```python
class CLIPModel(nn.Module):
    """CLIPæ¨¡å‹çš„ç®€åŒ–å®ç°"""
    def __init__(self, vocab_size, embed_dim, image_embed_dim):
        super().__init__()
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_embedding = nn.Embedding(vocab_size, embed_dim)
        self.text_transformer = nn.TransformerEncoder(...)
        
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = nn.Sequential(...)  # å¦‚ResNet/ViT
        
        # æŠ•å½±å±‚ï¼šå°†æ–‡æœ¬å’Œå›¾åƒæ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´
        self.text_projection = nn.Linear(embed_dim, 512)
        self.image_projection = nn.Linear(image_embed_dim, 512)
        
    def forward(self, text_tokens, images):
        # æ–‡æœ¬embedding
        text_features = self.text_embedding(text_tokens)
        text_features = self.text_transformer(text_features)
        text_embed = self.text_projection(text_features.mean(dim=1))
        
        # å›¾åƒembedding
        image_features = self.image_encoder(images)
        image_embed = self.image_projection(image_features)
        
        # å¯¹æ¯”å­¦ä¹ ï¼šè®©åŒ¹é…çš„æ–‡æœ¬-å›¾åƒå¯¹åœ¨embeddingç©ºé—´ä¸­æ›´æ¥è¿‘
        return text_embed, image_embed

# CLIPçš„æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ è®©æ–‡æœ¬å’Œå›¾åƒembeddingåœ¨åŒä¸€ç©ºé—´ä¸­è¯­ä¹‰å¯¹é½
```

#### å¯¹æ¯”å­¦ä¹ çš„embeddingè®­ç»ƒ

```python
def contrastive_loss(text_embeds, image_embeds, temperature=0.07):
    """CLIPçš„å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°"""
    # è®¡ç®—æ‰€æœ‰æ–‡æœ¬-å›¾åƒå¯¹çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = torch.matmul(text_embeds, image_embeds.T) / temperature
    
    # å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼Œå…¶ä½™ä¸ºè´Ÿæ ·æœ¬
    batch_size = text_embeds.shape[0]
    labels = torch.arange(batch_size)
    
    # åŒå‘å¯¹æ¯”æŸå¤±
    loss_text = nn.CrossEntropyLoss()(logits, labels)
    loss_image = nn.CrossEntropyLoss()(logits.T, labels)
    
    return (loss_text + loss_image) / 2
```

### 4.4 åˆ†å±‚embeddingä¸ä¸“å®¶æ··åˆï¼ˆMoEï¼‰

```python
class LayerwiseEmbedding(nn.Module):
    """åˆ†å±‚embeddingï¼šä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„embedding"""
    def __init__(self, vocab_size, embed_dims):
        super().__init__()
        self.embeddings = nn.ModuleList([
            nn.Embedding(vocab_size, dim) for dim in embed_dims
        ])
        
    def forward(self, tokens, layer_idx):
        return self.embeddings[layer_idx](tokens)

# åœ¨æŸäº›å¤§æ¨¡å‹ä¸­ï¼Œä¸åŒå±‚å¯èƒ½ä½¿ç”¨ä¸åŒç»´åº¦çš„embedding
```

## 5. è°ƒè¯•ä¸å¯è§†åŒ–

### 5.1 embeddingè´¨é‡æ£€æŸ¥

```python
def analyze_embedding_quality(embedding, vocab):
    """åˆ†æembeddingè´¨é‡"""
    # æ£€æŸ¥ç›¸ä¼¼è¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
    def cosine_similarity(vec1, vec2):
        return torch.cosine_similarity(vec1, vec2, dim=0)
    
    # ç¤ºä¾‹ï¼šæ£€æŸ¥"çˆ±"å’Œ"å–œæ¬¢"çš„ç›¸ä¼¼åº¦
    love_idx = vocab.word2idx["çˆ±"] 
    like_idx = vocab.word2idx["å–œæ¬¢"]
    
    love_vec = embedding.weight[love_idx]
    like_vec = embedding.weight[like_idx]
    
    similarity = cosine_similarity(love_vec, like_vec)
    print(f"'çˆ±'å’Œ'å–œæ¬¢'çš„ç›¸ä¼¼åº¦: {similarity:.3f}")
    
    # æ£€æŸ¥embeddingçš„ç»Ÿè®¡ç‰¹æ€§
    print(f"Embeddingå‡å€¼: {embedding.weight.mean():.3f}")
    print(f"Embeddingæ ‡å‡†å·®: {embedding.weight.std():.3f}")
    print(f"EmbeddingèŒƒæ•°åˆ†å¸ƒ: {embedding.weight.norm(dim=1).mean():.3f}")
```

### 5.2 embeddingå¯è§†åŒ–

```python
def visualize_embeddings(embedding_matrix, vocab, method='tsne'):
    """ä½¿ç”¨t-SNEæˆ–PCAå¯è§†åŒ–embedding"""
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt
    
    # é™ç»´åˆ°2D
    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42)
    else:
        from sklearn.decomposition import PCA
        reducer = PCA(n_components=2)
    
    embeddings_2d = reducer.fit_transform(embedding_matrix.detach().numpy())
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.figure(figsize=(12, 8))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)
    
    # æ·»åŠ è¯æ±‡æ ‡ç­¾
    for i, word in enumerate(vocab.idx2word.values()):
        if i < 20:  # åªæ˜¾ç¤ºå‰20ä¸ªè¯
            plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))
    
    plt.title(f'Embedding Visualization ({method.upper()})')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True, alpha=0.3)
    plt.show()

# æç¤ºï¼šè¿è¡Œæ­¤ä»£ç åï¼Œè§‚å¯Ÿè¯­ä¹‰ç›¸ä¼¼çš„è¯æ˜¯å¦åœ¨ç©ºé—´ä¸­èšé›†
# å¦‚æœembeddingè®­ç»ƒå¾—å¥½ï¼Œ"çˆ±"ã€"å–œæ¬¢"ç­‰æƒ…æ„Ÿè¯åº”è¯¥å½¼æ­¤æ¥è¿‘
```

### 5.3 embeddingå¼‚å¸¸æ£€æµ‹

```python
def detect_embedding_anomalies(embedding, threshold=3.0):
    """æ£€æµ‹å¼‚å¸¸çš„embeddingå‘é‡"""
    norms = embedding.weight.norm(dim=1)
    mean_norm = norms.mean()
    std_norm = norms.std()
    
    # æ£€æµ‹å¼‚å¸¸å¤§æˆ–å¼‚å¸¸å°çš„å‘é‡
    outliers = torch.where(torch.abs(norms - mean_norm) > threshold * std_norm)[0]
    
    print(f"æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸embedding:")
    for idx in outliers[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"ç´¢å¼• {idx}: èŒƒæ•° {norms[idx]:.3f}")
```

## å®Œæ•´ä»£ç å®ç°

æƒ³è¦æŸ¥çœ‹å®Œæ•´çš„LSTMç¼–ç å™¨-è§£ç å™¨å®ç°ä»£ç å—ï¼Ÿå®Œæ•´çš„276è¡ŒPyTorchä»£ç å·²ç»åœ¨**æ–‡ç« å¼€å¤´**çš„"å®Œæ•´ä»£ç å®ç°"éƒ¨åˆ†æä¾›ï¼ŒåŒ…å«è¯æ±‡è¡¨æ„å»ºã€LSTMç¼–ç å™¨ã€è§£ç å™¨å’Œè®­ç»ƒæµç¨‹ã€‚

### ğŸ“ **ä»£ç æ–‡ä»¶ä¸‹è½½**

å¦‚æœæ‚¨å¸Œæœ›ä¸‹è½½ä»£ç æ–‡ä»¶åˆ°æœ¬åœ°è¿è¡Œï¼Œå¯ä»¥è®¿é—®ï¼š
**[ä¸‹è½½ä»£ç æ–‡ä»¶](/demos/lstm_encoder_decoder.html)**

### ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **å­¦ä¹ è·¯å¾„**ï¼šç»“åˆæ–‡ç« å¼€å¤´çš„å®Œæ•´ä»£ç ä¸æœ¬æ–‡çš„ç†è®ºåˆ†æ
2. **å®è·µæ“ä½œ**ï¼šå°†ä»£ç å¤åˆ¶åˆ°æœ¬åœ°ï¼Œå°è¯•è¿è¡Œå¹¶ä¿®æ”¹å‚æ•°  
3. **æ·±å…¥ç ”ç©¶**ï¼šåŸºäºè¿™ä¸ªåŸºç¡€å®ç°ï¼Œæ¢ç´¢æ›´é«˜çº§çš„seq2seqå˜ä½“

## å»¶ä¼¸é˜…è¯»

### ç»å…¸è®ºæ–‡
- [Efficient Estimation of Word Representations in Vector Space (Word2Vec)](https://arxiv.org/abs/1301.3781)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- [Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

### æŠ€æœ¯æ–‡æ¡£
- [PyTorch Embeddingå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
- [HuggingFace Tokenizersåº“](https://huggingface.co/docs/tokenizers/)
- [Gensim Word2Vecå®ç°](https://radimrehurek.com/gensim/models/word2vec.html)
- [OpenAI CLIPæ¨¡å‹](https://github.com/openai/CLIP)

### å®è·µæ•™ç¨‹
- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)
- [Sebastian Ruder: EmbeddingæŠ€æœ¯ç»¼è¿°](https://ruder.io/word-embeddings-1/)
- [Lilian Weng: Attentionæœºåˆ¶è¯¦è§£](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Jay Alammar: The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

### å·¥å…·ä¸æ¡†æ¶
- [Sentence Transformers](https://www.sbert.net/)ï¼šé«˜è´¨é‡å¥å­embedding
- [Faiss](https://github.com/facebookresearch/faiss)ï¼šé«˜æ•ˆå‘é‡æ£€ç´¢
- [Annoy](https://github.com/spotify/annoy)ï¼šè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
- [Weights & Biases](https://wandb.ai/)ï¼šembeddingå¯è§†åŒ–ä¸å®éªŒè·Ÿè¸ª

---

embeddingæŠ€æœ¯ä»ç®€å•çš„æŸ¥æ‰¾è¡¨å‘å±•åˆ°ä»Šå¤©çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œå†åˆ°å¤šæ¨¡æ€ç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œè§è¯äº†æ·±åº¦å­¦ä¹ åœ¨è¯­ä¹‰ç†è§£æ–¹é¢çš„å·¨å¤§è¿›æ­¥ã€‚ç†è§£embeddingçš„æœ¬è´¨â€”â€”å°†ç¦»æ•£ç¬¦å·æ˜ å°„ä¸ºè¿ç»­è¯­ä¹‰ç©ºé—´â€”â€”æ˜¯æŒæ¡ç°ä»£NLPã€æ¨èç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹çš„å…³é”®ã€‚

åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œembeddingä¸ä»…æ˜¯è¾“å…¥å±‚çš„æŠ€æœ¯ç»†èŠ‚ï¼Œæ›´æ˜¯è¿æ¥ä¸åŒæ¨¡æ€ã€å®ç°é›¶æ ·æœ¬å­¦ä¹ ã€æ”¯æ’‘RAGåº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„ä¸æ–­å¢å¤§ï¼Œembeddingçš„ä¼˜åŒ–ï¼ˆå¦‚é‡åŒ–ã€åˆ†å¸ƒå¼å­˜å‚¨ã€ç¨€ç–æ›´æ–°ï¼‰ä¹Ÿæˆä¸ºå·¥ç¨‹å®è·µçš„é‡è¦è€ƒé‡ã€‚

ä½ æ˜¯å¦æ€è€ƒè¿‡ï¼Œåœ¨AGIï¼ˆé€šç”¨äººå·¥æ™ºèƒ½ï¼‰æ—¶ä»£ï¼Œembeddingå¦‚ä½•ç»Ÿä¸€æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰æ‰€æœ‰æ¨¡æ€çš„è¯­ä¹‰è¡¨ç¤ºï¼Ÿåœ¨ä½ çš„å…·ä½“ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œå¦‚ä½•è®¾è®¡æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„embeddingç­–ç•¥ï¼Ÿè¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è®¸å°±æ˜¯ä¸‹ä¸€ä¸ªæŠ€æœ¯çªç ´çš„èµ·ç‚¹ã€‚

---

*å¸Œæœ›è¿™ä¸ªå®Œæ•´çš„embeddingç†è®ºä¸LSTMå®è·µç›¸ç»“åˆçš„æŒ‡å—èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼*