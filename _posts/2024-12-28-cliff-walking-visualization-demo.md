---
title: 悬崖漫步环境：策略迭代与价值迭代可视化演示
date: 2024-12-28 18:00:00 +0800
categories: [强化学习, 可视化]
tags: [强化学习, 马尔可夫决策过程, 策略迭代, 价值迭代, 演示, 机器学习, AI算法]
description: "交互式悬崖漫步环境可视化工具，直观展示策略迭代与价值迭代算法的收敛过程。包含完整的强化学习算法实现、单步调试功能和性能对比分析，帮助理解马尔可夫决策过程的核心概念。"
image: /assets/img/posts/cliff-walking-ocean-edge.png
author: "Zorro Zuo"
keywords: ["悬崖漫步", "策略迭代", "价值迭代", "强化学习可视化", "马尔可夫决策过程", "动态规划算法"]
---

## 悬崖漫步问题简介

悬崖漫步（Cliff Walking）是强化学习中的一个经典问题，用来演示马尔可夫决策过程（MDP）和动态规划算法的基本概念。

在这个问题中：
- **智能体**需要从起点（左下角）到达终点（右下角）
- **环境**包含一片悬崖区域，掉入悬崖会受到惩罚并重新开始
- **目标**是找到最优策略，最大化累积奖励

![悬崖漫步环境示意图](/assets/images/cliff_walking_diagram.png)
*悬崖漫步环境示意图：S为起点，G为终点，灰色区域为悬崖*

## 算法对比

### 策略迭代 (Policy Iteration)
1. **策略评估**：计算当前策略下的状态价值函数
2. **策略改进**：根据价值函数更新策略
3. **重复**直到策略收敛

#### 策略迭代伪代码

```
算法：策略迭代 (Policy Iteration)

输入：
    - MDP环境：状态集合S，动作集合A，转移概率P，奖励函数R
    - 折扣因子γ
    - 收敛阈值θ

初始化：
    - 随机初始化策略π(s)，对所有状态s ∈ S
    - 初始化状态价值函数V(s) = 0，对所有状态s ∈ S

重复：
    # 策略评估阶段
    重复：
        Δ ← 0
        对每个状态s ∈ S：
            v ← V(s)
            V(s) ← Σ_s' P(s'|s,π(s)) × [R(s,π(s),s') + γ × V(s')]
            Δ ← max(Δ, |v - V(s)|)
    直到 Δ < θ（价值函数收敛）
    
    # 策略改进阶段
    policy_stable ← true
    对每个状态s ∈ S：
        old_action ← π(s)
        π(s) ← argmax_a Σ_s' P(s'|s,a) × [R(s,a,s') + γ × V(s')]
        如果 old_action ≠ π(s)：
            policy_stable ← false
            
直到 policy_stable = true（策略收敛）

输出：最优策略π*和最优价值函数V*
```

### 价值迭代 (Value Iteration)
1. **价值更新**：直接更新状态价值函数
2. **策略提取**：从最终价值函数中提取最优策略
3. **更高效**，通常需要更少的迭代次数

#### 价值迭代伪代码

```
算法：价值迭代 (Value Iteration)

输入：
    - MDP环境：状态集合S，动作集合A，转移概率P，奖励函数R
    - 折扣因子γ
    - 收敛阈值θ

初始化：
    - 状态价值函数V(s) = 0，对所有状态s ∈ S

重复：
    Δ ← 0
    对每个状态s ∈ S：
        v ← V(s)
        V(s) ← max_a Σ_s' P(s'|s,a) × [R(s,a,s') + γ × V(s')]
        Δ ← max(Δ, |v - V(s)|)
直到 Δ < θ（价值函数收敛）

# 策略提取阶段
对每个状态s ∈ S：
    π*(s) ← argmax_a Σ_s' P(s'|s,a) × [R(s,a,s') + γ × V(s')]

输出：最优策略π*和最优价值函数V*
```

### 算法对比分析

| 特性 | 策略迭代 | 价值迭代 |
|------|---------|---------|
| **收敛速度** | 较慢 | 较快 |
| **迭代步骤** | 策略评估 + 策略改进 | 直接价值更新 |
| **计算复杂度** | 每次迭代需完全解线性方程组 | 每次迭代只需一步价值更新 |
| **内存需求** | 需存储策略和价值函数 | 主要存储价值函数 |
| **收敛保证** | 有限步收敛到最优解 | 渐近收敛到最优解 |

## 算法详细说明

如果你想了解这两种算法的详细数学推导、证明过程和完整代码实现，强烈推荐参考《动手学强化学习》第4章的内容：

📚 [**动态规划算法 - 动手学强化学习**](https://hrl.boyuai.com/chapter/1/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95)

该教程提供了：
- **理论基础**：策略迭代和价值迭代的数学原理
- **收敛性证明**：详细的算法收敛性分析
- **完整代码**：悬崖漫步环境和算法的Python实现
- **实验结果**：算法性能对比和分析

> 本可视化工具是基于该教程的理论基础制作的，通过交互式界面帮助更好地理解这些经典算法。
{: .prompt-info }

## 在线演示

我制作了一个交互式的可视化工具，你可以：

✨ **实时观察**算法的收敛过程  
🔄 **单步执行**每个迭代步骤  
📊 **对比分析**两种算法的性能差异  
🎮 **交互体验**强化学习的核心概念  

## [🚀 立即体验可视化演示](/demos/cliff-walking/)

这个可视化工具完全在浏览器中运行，无需安装任何软件。通过直观的网格界面和实时更新的数值，你可以深入理解这两种经典的动态规划算法。

## 学习价值

通过这个演示，你将能够：

1. **理解MDP的基本组成**：状态、动作、奖励、转移概率
2. **观察算法收敛过程**：看到价值函数和策略如何逐步优化
3. **比较算法性能**：了解不同算法的优缺点和适用场景
4. **建立直觉认知**：通过可视化加深对抽象概念的理解

希望这个工具能够帮助你更好地理解强化学习的基础算法！

> **提示**：建议先尝试单步执行模式，仔细观察每一步的变化，再使用完整运行模式查看最终结果。 
